import json
import logging

from list_lm.data import LinkType, SuggestedApplicationData, UnparsedUrl, UnsupportedUrl, UrlType
from list_lm.ollama_client import OllamaClient
from list_lm.parse_html import get_github_readme
from list_lm.parse_url import parse_url
from list_lm.prompt_template import get_links_prompt

LOGGER = logging.getLogger(__name__)


class ParserLinks:
    def __init__(self, max_retries: int = 5) -> None:
        self.ollama_client = OllamaClient()
        self.enable_caching = True
        self.link_type_names = [link_type.name for link_type in [*LinkType]]
        self.max_retries = max_retries

    def parse_url(self, url: str, ollama_model_name: str) -> SuggestedApplicationData | UnparsedUrl | UnsupportedUrl:
        url_type = parse_url(url)

        parsed_data = None
        for retry_number in range(self.max_retries):
            if url_type == UrlType.GITHUB:
                for _ in range(self.max_retries):
                    parsed_data = self.auto_parse_github(url, ollama_model_name)
                    if isinstance(parsed_data, UnparsedUrl):
                        LOGGER.warning(f"Retrying {retry_number + 1}/{self.max_retries} parsing URL: {url}")
                        continue
                    else:
                        return parsed_data

                if parsed_data:
                    return parsed_data
            else:
                break

        LOGGER.error(f"Unsupported URL: {url}")
        return UnsupportedUrl(url=url)

    def auto_parse_github(self, url: str, ollama_model_name: str) -> SuggestedApplicationData | UnparsedUrl:
        readme_text = get_github_readme(url)
        if isinstance(readme_text, UnparsedUrl):
            return readme_text

        prompt = get_links_prompt(readme_text, ", ".join(self.link_type_names))
        generated_text = self.ollama_client.generate(ollama_model_name, prompt)
        start_json = generated_text.find("{")
        end_json = generated_text.find("}")
        if start_json < 0 or end_json < 0:
            msg = "Cannot find JSON object in generated response from LM"
            LOGGER.error(msg)
            return UnparsedUrl(url=url, message=msg)

        try:
            parsed_json = json.loads(generated_text[start_json : end_json + 1])
        except json.decoder.JSONDecodeError as error:
            LOGGER.error(f"Cannot parse JSON: {generated_text!r}, got error: {error}")
            return UnparsedUrl(url=url, message="Cannot parse JSON object generated by LM")

        expected_keys = {"name", "description", "category"}
        if not set(parsed_json.keys()).issuperset(expected_keys):
            LOGGER.error(f"Cannot find all expected keys: {expected_keys} in JSON: {parsed_json}")
            return UnparsedUrl(url=url, message="Cannot process not completed JSON object generated by LM")

        try:
            name = parsed_json["name"]
            description = parsed_json["description"]
            link_type = LinkType[parsed_json["category"]]
        except KeyError as error:
            LOGGER.error(f"Cannot extract data JSON: {generated_text!r}, got error: {error}")
            return UnparsedUrl(url=url, message="Cannot extract data from JSON object generated by LM")

        return SuggestedApplicationData(
            name=name, description=description, url=url, link_type=link_type, readme_text=readme_text
        )

    @staticmethod
    def remove_duplicates(names: list[str]) -> list[str]:
        names_set = set()
        names_uniq = []
        for name in names:
            if name not in names_set:
                names_set.add(name)
                names_uniq.append(name)
        return names_uniq
