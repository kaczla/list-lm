# Language Models

This repository contains language models base on Transformer architecture.

# Model lists

- Transformer
  - Year: 2017
  - Publication: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  - Code: -
  - Model weights: -

- MoE
  - Year: 2017
  - Publication: [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

- GPT
  - Year: 2018
  - Publication: [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
  - Blog post: [Improving Language Understanding with Unsupervised Learning](https://openai.com/blog/language-unsupervised)

- BERT
  - Year: 2018
  - Publication: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

- flair
  - Year: 2018
  - Publication: [Contextual String Embeddings for Sequence Labeling](https://aclanthology.org/C18-1139)
  - Code: [GitHub](https://github.com/flairNLP/flair)

- RR-Transformer
  - Year: 2018
  - Publication: [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)

- LightConv/DynamicConv
  - Year: 2019
  - Publication: [Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/abs/1901.10430)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/pay_less_attention_paper)

- GPT-2
  - Year: 2019
  - Publication: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - Code: [GitHub](https://github.com/openai/gpt-2)

- Transformer-XL
  - Year: 2019
  - Publication: [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
  - Code: [GitHub](https://github.com/kimiyoung/transformer-xl)

- Evolved Transformer
  - Year: 2019
  - Publication: [The Evolved Transformer](https://arxiv.org/abs/1901.11117)

- XLNet
  - Year: 2019
  - Publication: [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)

- RoBERTa
  - Year: 2019
  - Publication: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/roberta)

- ALBERT
  - Year: 2019
  - Publication: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
  - Code: [GitHub](https://github.com/google-research/ALBERT)

- CTRL
  - Year: 2019
  - Publication: [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858)
  - Code: [GitHub](https://github.com/salesforce/ctrl)

- StructBERT
  - Year: 2019
  - Publication: [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)

- Adaptive Span
  - Year: 2019
  - Publication: [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)
  - Code: [GitHub](https://github.com/facebookresearch/adaptive-span)

- All-attention network
  - Year: 2019
  - Publication: [Augmenting Self-attention with Persistent Memory](https://arxiv.org/abs/1907.01470)

- Sparse Transformer
  - Year: 2019
  - Publication: [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)

- Adaptively Sparse Transformers
  - Year: 2019
  - Publication: [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015)

- Megatron-LM
  - Year: 2019
  - Publication: [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- kNN-LM
  - Year: 2019
  - Publication: [Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172)
  - Code: [GitHub](https://github.com/urvashik/knnlm)

- TENER
  - Year: 2019
  - Publication: [TENER: Adapting Transformer Encoder for Named Entity Recognition](https://arxiv.org/abs/1911.04474)

- MobileBERT
  - Year: 2020
  - Publication: [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/mobilebert)

- Poor Man's BERT
  - Year: 2020
  - Publication: [On the Effect of Dropping Layers of Pre-trained Transformer Models](https://arxiv.org/abs/2004.03844)
  - Code: [GitHub](https://github.com/hsajjad/transformers)

- Longformer
  - Year: 2020
  - Publication: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
  - Code: [GitHub](https://github.com/allenai/longformer)

- Linformer
  - Year: 2020
  - Publication: [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
  - Code: [GitHub](https://github.com/Kyan820815/Linformer)

- Routing Transformer
  - Year: 2020
  - Publication: [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/abs/2003.05997)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/routing_transformer)

- Sandwich Transformers
  - Year: 2020
  - Publication: [https://arxiv.org/abs/1911.03864](https://arxiv.org/abs/1911.03864)
  - Code: [GitHub](https://github.com/ofirpress/sandwich_transformer)
  - Video: [YouTube](https://www.youtube.com/watch?v=rFuuGEj3AhU)

- MPNet
  - Year: 2020
  - Publication: [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)
  - Code: [GitHub](https://github.com/microsoft/MPNet)

- DeeBERT
  - Year: 2020
  - Publication: [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993)
  - Code: [GitHub](https://github.com/castorini/DeeBERT)

- BiT (Big Transfer)
  - Year: 2020
  - Publication: [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)

- Adaptive Transformers
  - Year: 2020
  - Publication: [Adaptive Transformers for Learning Multimodal Representations](https://arxiv.org/abs/2005.07486)
  - Code: [GitHub](https://github.com/prajjwal1/adaptive_transformer)

- Synthesizer
  - Year: 2020
  - Publication: [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
  - Code: [GitHub](https://github.com/leaderj1001/Synthesizer-Rethinking-Self-Attention-Transformer-Models) / [GitHub](https://github.com/10-zin/Synthesizer)

- BRC
  - Year: 2020
  - Publication: [A bio-inspired bistable recurrent cell allows for long-lasting memory](https://arxiv.org/abs/2006.05252)
  - Code: [GitHub](https://github.com/nvecoven/BRC) / [GitHub](https://github.com/742617000027/nBRC)

- Feedback Transformer
  - Year: 2020
  - Publication: [Addressing Some Limitations of Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402)

- Shortformer
  - Year: 2020
  - Publication: [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)
  - Code: [GitHub](https://github.com/ofirpress/shortformer)

- Floater
  - Year: 2020
  - Publication: [Learning to Encode Position for Transformer with Continuous Dynamical Model](https://arxiv.org/abs/2003.09229)
  - Code: [GitHub](https://github.com/xuanqing94/FLOATER)

- Informer
  - Year: 2020
  - Publication: [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436)

- TUPE
  - Year: 2020
  - Publication: [Rethinking Positional Encoding in Language Pre-training](https://arxiv.org/abs/2006.15595)
  - Code: [GitHub](https://github.com/guolinke/TUPE)

- Key-Value Memory Feed-Forward
  - Year: 2020
  - Publication: [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)

- DA-Transformer
  - Year: 2020
  - Publication: [DA-Transformer: Distance-aware Transformer](https://arxiv.org/abs/2010.06925)

- MiniLM
  - Year: 2021
  - Publication: [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957)

- MiniLM v2
  - Year: 2021
  - Publication: [MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://arxiv.org/abs/2012.15828)

- GLM
  - Year: 2021
  - Publication: [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)
  - Code: [GitHub](https://github.com/THUDM/GLM)
  - Model weights: [HuggingFace models](https://huggingface.co/BAAI/glm-10b)

- ByT5
  - Year: 2021
  - Publication: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
  - Code: [GitHub](https://github.com/google-research/byt5)

- Luna
  - Year: 2021
  - Publication: [Luna: Linear Unified Nested Attention](https://arxiv.org/abs/2106.01540)
  - Code: [GitHub](https://github.com/XuezheMax/fairseq-apollo)

- UniT
  - Year: 2021
  - Publication: [UniT: Multimodal Multitask Learning with a Unified Transformer](https://arxiv.org/abs/2102.10772)
  - Code: [GitHub](https://github.com/facebookresearch/mmf)

- Charformer
  - Year: 2021
  - Publication: [Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](https://arxiv.org/abs/2106.12672)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/charformer)

- Primer
  - Year: 2021
  - Publication: [Primer: Searching for Efficient Transformers for Language Modeling](https://arxiv.org/abs/2109.08668)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/primer)

- FNet
  - Year: 2021
  - Publication: [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/f_net)
  - Video: [YouTube](https://www.youtube.com/watch?v=JJR3pBl78zw)

- Megatron-LM v2
  - Year: 2021
  - Publication: [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- Switch Transformer
  - Year: 2021
  - Publication: [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
  - Code: [GitHub](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)

- PanGu
  - Year: 2021
  - Publication: [PanGu: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation](https://arxiv.org/abs/2104.12369)

- 24hBERT - Academic Budget BERT
  - Year: 2021
  - Publication: [How to Train BERT with an Academic Budget](https://arxiv.org/abs/2104.07705)
  - Code: [GitHub](https://github.com/IntelLabs/academic-budget-bert)

- T0
  - Year: 2021
  - Publication: [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)
  - Code: [GitHub](https://github.com/bigscience-workshop/t-zero)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/T0)

- XGLM
  - Year: 2021
  - Publication: [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)
  - Code: [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/xglm)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/xglm-7.5B)

- BLOOM
  - Year: 2022
  - Publication: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloom)

- BLOOMZ & mT0
  - Year: 2022
  - Publication: [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloomz)

- Gopher
  - Year: 2022
  - Publication: [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)

- Megatron-Turing NLG
  - Year: 2022
  - Publication: [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)

- Chinchilla
  - Year: 2022
  - Publication: [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)

- OPT
  - Year: 2022
  - Publication: [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)
  - Code: [GitHub](https://github.com/facebookresearch/metaseq)

- RWKV-LM
  - Year: 2022
  - Publication: -
  - Code: [GitHub](https://github.com/BlinkDL/RWKV-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)

- UL2
  - Year: 2022
  - Publication: [UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/ul2)

- FlanT5
  - Year: 2022
  - Publication: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
  - Code: [GitHub](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)
  - Model weights: [HuggingFace models](https://huggingface.co/google/flan-t5-base)

- FlashAttention
  - Year: 2022
  - Publication: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
  - Code: [GitHub](https://github.com/HazyResearch/flash-attention)

- Galactica
  - Year: 2022
  - Publication: [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
  - Code: [GitHub](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/galactica-6.7b)

- NPM
  - Year: 2022
  - Publication: [Nonparametric Masked Language Modeling](https://arxiv.org/abs/2212.01349)
  - Code: [GitHub](https://github.com/facebookresearch/NPM)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/npm)

- Cramming
  - Year: 2022
  - Publication: [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034)
  - Code: [GitHub](https://github.com/JonasGeiping/cramming)

- GLM-130B
  - Year: 2022
  - Publication: [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)
  - Code: [GitHub](https://github.com/THUDM/GLM-130B)
  - Model weights: [HuggingFace models](https://huggingface.co/spaces/THUDM/GLM-130B)

- E-SPA (Exponential Signal Preserving Attention)
  - Year: 2023
  - Publication: [Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation](https://arxiv.org/abs/2302.10322)

- Hyena
  - Year: 2023
  - Publication: [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866)
  - Code: [GitHub](https://github.com/HazyResearch/safari)

- SpikeGPT
  - Year: 2023
  - Publication: [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks](https://arxiv.org/abs/2302.13939)
  - Code: [GitHub](https://github.com/ridgerchu/SpikeGPT)

- LLama
  - Year: 2023
  - Publication: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
  - Code: [GitHub](https://github.com/facebookresearch/llama)

- KOSMOS-1
  - Year: 2023
  - Publication: [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045)

- ParaFormer
  - Year: 2023
  - Publication: [ParaFormer: Parallel Attention Transformer for Efficient Feature Matching](https://arxiv.org/abs/2303.00941)

- PaLM-E
  - Year: 2023
  - Publication: [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)

- Alpaca
  - Year: 2023
  - Publication: [Alpaca: A Strong Open-Source Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  - Code: [GitHub](https://github.com/tatsu-lab/stanford_alpaca)

- TWM
  - Year: 2023
  - Publication: [Transformer-based World Models Are Happy With 100k Interactions](https://arxiv.org/abs/2303.07109)
  - Code: [GitHub](https://github.com/jrobine/twm)

- MosaicBERT
  - Year: 2023
  - Publication: [MosaicBERT: Pretraining BERT from Scratch for $20](https://www.mosaicml.com/blog/mosaicbert)
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mosaic-bert-base)

- GPT-4
  - Year: 2023
  - Publication: [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)

# Model links

- [ColossalAI](https://github.com/hpcaitech/ColossalAI) - Colossal-AI: A Unified Deep Learning System for Big Model Era. Making large AI models cheaper, faster and more accessible.
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) - MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.
- [ExtremeBERT](https://github.com/extreme-bert/extreme-bert) - ExtremeBERT is a toolkit that accelerates the pretraining of customized language models on customized datasets.
- [FairScale](https://github.com/facebookresearch/fairscale) - FairScale is a PyTorch extension library for high performance and large scale training. This library extends basic PyTorch capabilities while adding new SOTA scaling techniques.
- [FlagAI](https://github.com/FlagAI-Open/FlagAI) - FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.
- [GENIUS](https://github.com/beyondguo/geniushttps://github.com/beyondguo/genius) - GENIUS generating text using sketches! A strong and general textual data augmentation tool.
- [Kernl](https://github.com/ELS-RD/kernl) - Kernl lets you run Pytorch transformer models several times faster on GPU with a single line of code, and is designed to be easily hackable. Kernl is the first OSS inference engine written in OpenAI Triton, a new language designed by OpenAI to make it easier to write GPU kernels.
- [LASSL](https://github.com/lassl/lassl) - LASSL is a LAnguage framework for Self-Supervised Learning. LASSL aims to provide an easy-to-use framework for pretraining language model by only using Huggingface's Transformers and Datasets.
- [Latent Diffusion Models](https://github.com/CompVis/latent-diffusion) - High-Resolution Image Synthesis with Latent Diffusion Models, contains: Text-to-Image, Inpainting
- [LiBai](https://github.com/Oneflow-Inc/libai) - LiBai is a large-scale open-source model training toolbox based on OneFlow
- [metaseq](https://github.com/facebookresearch/metaseq) - A codebase for working with Open Pre-trained Transformers.
- [MMF](https://github.com/facebookresearch/mmf) - MMF is a modular framework for vision and language multimodal research
- [multimodal](https://github.com/facebookresearch/multimodal) - TorchMultimodal is a PyTorch library for training state-of-the-art multimodal multi-task models at scale.
- [NVIDIA Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - Megatron is an efficient, model-parallel (tensor, sequence, and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.
- [NVIDIA Merlin](https://github.com/NVIDIA-Merlin/Merlin) - NVIDIA Merlin is an open source library providing end-to-end GPU-accelerated recommender systems, from feature engineering and preprocessing to training deep learning models and running inference in production.
- [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) - NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP).
- [RWKV](https://github.com/BlinkDL/RWKV-LM) - RWKV is a RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer: great performance, fast inference, saves VRAM, fast training, "infinite" ctx_len, and free sentence embedding.
- [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers) - This library is based on the Transformers library by HuggingFace. Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model.
- [small-text](https://github.com/webis-de/small-text) - Small-Text provides state-of-the-art Active Learning for Text Classification.
- [Sockeye](https://github.com/awslabs/sockeye) - Sockeye is an open-source sequence-to-sequence framework for Neural Machine Translation built on PyTorch.
- [SparseZoo](https://github.com/neuralmagic/sparsezoo) - Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes.
- [SpikeGPT](https://github.com/ridgerchu/SpikeGPT) - SpikeGPT is a lightweight generative language model with pure binary, event-driven spiking activation units.
- [tasknet](https://github.com/sileod/tasknet) - Integration of HuggingFace Datasets with HuggingFace Trainer, and multitasking.
- [TorchScale](https://github.com/microsoft/torchscale) - TorchScale is a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively. It has the implementation of fundamental research to improve modeling generality and capability, as well as training stability and efficiency of scaling Transformers.
- [Trankit](https://github.com/nlp-uoregon/trankit) - A Light-Weight Transformer-based Python Toolkit for Multilingual Natural Language Processing
- [transformers](https://github.com/huggingface/transformers) - State-of-the-art Natural Language Processing for PyTorch and TensorFlow 2.0
- [txtai](https://github.com/neuml/txtai) - txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.
- [unilm](https://github.com/microsoft/unilm) - Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities.
- [voltaML](https://github.com/VoltaML/voltaML) - VoltaML is a lightweight library to convert and run your ML/DL deep learning models in high performance inference runtimes like TensorRT, TorchScript, ONNX and TVM.
- [x-transformers](https://github.com/lucidrains/x-transformers) - A simple but complete full-attention transformer with a set of promising experimental features from various papers.
- [xFormers](https://github.com/facebookresearch/xformers) - xFormers is a modular and field agnostic library to flexibly generate transformer architectures by interoperable and optimized building blocks.

# Utils links

- [bagua](https://github.com/BaguaSys/bagua) - Bagua is a deep learning training acceleration framework for PyTorch
- [bricks](https://github.com/code-kern-ai/bricks) - Open-source natural language enrichments at your fingertips.
- [cleanlab](https://github.com/cleanlab/cleanlab) - cleanlab automatically finds and fixes errors in any ML dataset
- [Cramming Language Model (Pretraining)](https://github.com/JonasGeiping/cramming) - Cramming the training of a (BERT-type) language model into limited compute. Cramming: Training a Language Model on a Single GPU in One Day.
- [CTranslate2](https://github.com/OpenNMT/CTranslate2) - CTranslate2 is a C++ and Python library for efficient inference with Transformer models.
- [cyclemoid-pytorch](https://github.com/rasbt/cyclemoid-pytorch) - This is an implementation of the cyclemoid activation function for PyTorch.
- [datasketch](https://github.com/ekzhu/datasketch) - datasketch gives you probabilistic data structures that can process and search very large amount of data super fast, with little loss of accuracy. MinHash, LSH, LSH Forest, Weighted MinHash, HyperLogLog, HyperLogLog++, LSH Ensemble.
- [DeepSparse](https://github.com/neuralmagic/deepsparse) - Inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application.
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.
- [DGL](https://github.com/dmlc/dgl) - DGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs. DGL is framework agnostic, meaning if a deep graph model is a component of an end-to-end application, the rest of the logics can be implemented in any major frameworks, such as PyTorch, Apache MXNet or TensorFlow.
- [Explainpaper](https://www.explainpaper.com) - Upload a paper, highlight confusing text, get an explanation.
- [FastDeploy](https://github.com/PaddlePaddle/FastDeploy) - Easy-to-use and Fast Deep Learning Model Deployment Toolkit for Cloud Mobile and Edge. Including Image, Video, Text and Audio 20+ main stream scenarios and 150+ SOTA models with end-to-end optimization, multi-platform and multi-framework support.
- [FLAML](https://github.com/microsoft/FLAML) - A Fast Library for Automated Machine Learning & Tuning.
- [FlashAttention](https://github.com/HazyResearch/flash-attention) - Fast and memory-efficient exact attention.
- [FlexGen](https://github.com/FMInference/FlexGen) - FlexGen is a high-throughput generation engine for running large language models with limited GPU memory.
- [FlexGen](https://github.com/FMInference/FlexGen) - FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. Running large language models like OPT-175B/GPT-3 on a single GPU. Up to 100x faster than other offloading systems.
- [Gibberish Detector](https://github.com/domanchi/gibberish-detector) - Train a model, and detect gibberish strings with it. Example gibberish: "ertrjiloifdfyyoiu".
- [Guardrails](https://github.com/ShreyaR/guardrails) - Guardrails is an open-source Python package for specifying structure and type, validating and correcting the outputs of large language models (LLMs).
- [hlb-GPT](https://github.com/tysam-code/hlb-gpt) - Minimalistic, fast, and experimentation-friendly researcher's toolbench for GPT-like models in <350 lines of code. Reaches <3.8 validation loss on wikitext-103 on a single A100 in just over 3 minutes.
- [Horovod](https://github.com/horovod/horovod) - Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet
- [incdbscan](https://github.com/DataOmbudsman/incdbscan) - the incremental version of the DBSCAN clustering algorithm
- [LangChain](https://github.com/hwchase17/langchain) - Building applications with LLMs through composability.
- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) - A framework for few-shot evaluation of autoregressive language models.
- [Lovely Tensors](https://github.com/xl0/lovely-tensors) - Tensors, ready for human consumption.
- [Merlin Dataloader](https://github.com/NVIDIA-Merlin/dataloader) - The merlin dataloader lets you rapidly load tabular data for training deep leaning models with TensorFlow, PyTorch or JAX
- [Mini-Chain](https://github.com/srush/minichain) - A tiny library for large language models.
- [MosaicML Examples](https://github.com/mosaicml/examples) - This repo contains reference examples for training ML models quickly and to high accuracy. MosaicBERT
- [mup](https://github.com/microsoft/mup) - Maximal Update Parametrization (μP) and Hyperparameter Transfer (μTransfer): Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer
- [nanoGPT](https://github.com/karpathy/nanoGPT) - The simplest, fastest repository for training/finetuning medium-sized GPTs.
- [Opacus](https://github.com/pytorch/opacus) - Opacus is a library that enables training PyTorch models with differential privacy. It supports training with minimal code changes required on the client, has little impact on training performance, and allows the client to online track the privacy budget expended at any given moment.
- [PEFT](https://github.com/huggingface/peft) - Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters.
- [pybaselines](https://github.com/derb12/pybaselines) - A Python library of algorithms for the baseline correction of experimental data.
- [safari](https://github.com/HazyResearch/safari) - Convolutions for Sequence Modeling, contains: Hyena, Long Convs, Hungry Hungry Hippos (H3).
- [skorch](https://github.com/skorch-dev/skorch) - A scikit-learn compatible neural network library that wraps PyTorch.
- [SparseML](https://github.com/neuralmagic/sparseml) - Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models.
- [state-spaces](https://github.com/HazyResearch/state-spaces) - Sequence Modeling with Structured State Spaces, contains: S4D, HTTYH, SaShiMi, S4, LSSL, HiPPO.
- [surgeon-pytorch](https://github.com/archinetai/surgeon-pytorch) - A library to inspect and extract intermediate layers of PyTorch models.
- [tbparse](https://github.com/j3soon/tbparse) - A simple yet powerful tensorboard event log parser/reader.
- [TextBox](https://github.com/RUCAIBox/TextBox) - TextBox 2.0 is a text generation library with pre-trained language models.
- [TextReducer](https://github.com/helliun/targetedSummarization) - TextReducer is a tool for summarization and information extraction powered by the SentenceTransformer library.
- [tinygrad](https://github.com/geohot/tinygrad) - tinygrad is an extremely simple deep-learning framework. It supports both inference and training.
- [TorchBench](https://github.com/pytorch/benchmark) - TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.
- [torchdistX](https://github.com/pytorch/torchdistx) - Torch Distributed Experimental
- [torchegranate](https://github.com/jmschrei/torchegranate) - Fast, flexible and easy to use probabilistic modelling with PyTorch.
- [Training BERT with Compute/Time (Academic) Budget](https://github.com/IntelLabs/academic-budget-bert) - Repository contains scripts for pre-training and finetuning BERT-like models with limited time and compute budget.
- [transformer-deploy](https://github.com/ELS-RD/transformer-deploy) - Efficient, scalable and enterprise-grade CPU/GPU inference server for Hugging Face transformer models.
- [tsai](https://github.com/timeseriesAI/tsai) - tsai is an open-source deep learning package built on top of Pytorch & fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation.
- [UpTrain](https://github.com/uptrain-ai/uptrain) - An open-source framework to observe ML applications - ML monitoring and refinement toolkit.
- [WeightWatcher](https://github.com/CalculatedContent/WeightWatcher) - WeightWatcher (WW): is an open-source, diagnostic tool for analyzing Deep Neural Networks (DNN), without needing access to training or even test data.

# GPU profiling links

- [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) - NVIDIA Nsight™ Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest system on a chip (SoC).
- [NVIDIA Visual Profiler](https://developer.nvidia.com/nvidia-visual-profiler) - The NVIDIA Visual Profiler is a cross-platform performance profiling tool that delivers developers vital feedback for optimizing CUDA C/C++ applications.
- [NVIDIA/NVTX](https://github.com/NVIDIA/NVTX) - The NVIDIA® Tools Extension SDK (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications.
- [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) - This recipe explains how to use PyTorch profiler and measure the time and memory consumption of the model’s operators.
- [torchnnprofiler](https://github.com/kshitij12345/torchnnprofiler) - Context Manager to profile the forward and backward times of PyTorch's nn.Module

# Visualizations links

- [BertViz](https://github.com/jessevig/bertviz) - BertViz is a tool for visualizing attention in the Transformer model, supporting most models from the transformers library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, BART, etc.)
- [Ecoo](https://github.com/jalammar/ecco) - Ecco is a python library for exploring and explaining Natural Language Processing models using interactive visualizations.

# Vocabulary links

- [SentencePiece](https://github.com/google/sentencepiece) - SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) and unigram language model ) with the extension of direct training from raw sentences.
- [tiktoken](https://github.com/openai/tiktoken) - tiktoken is a fast BPE tokeniser for use with OpenAI's models.
- [tokenizers](https://github.com/huggingface/tokenizers) - Provides an implementation of today's most used tokenizers, with a focus on performance and versatility.

# Dataset links

- [BIG-bench](https://github.com/google/BIG-bench) - The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. The more than 200 tasks included in BIG-bench.
- [CCMatrix](https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix) - CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB.
- [CCNet](https://github.com/facebookresearch/cc_net) - Tools to download and clean Common Crawl as introduced in our paper CCNet: High Quality Monolingual Datasets from Web Crawl Data.
- [Deduplicated CommonCrawl Text](http://statmt.org/ngrams/deduped) - Processed Common Crawl snapshots.
- [DS-1000](https://github.com/HKUNLP/DS-1000) - Official data and code release for the paper DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.
- [FewGLUE](https://github.com/timoschick/fewglue) - FewGLUE dataset, consisting of a random selection of 32 training examples from the SuperGLUE training sets and up to 20,000 unlabeled examples for each SuperGLUE task.
- [Flan v2](https://github.com/google-research/FLAN/tree/main/flan/v2) - Flan 2022 collection of datasets and templates.
- [KILT](https://github.com/facebookresearch/KILT) - A Benchmark for Knowledge Intensive Language Tasks.
- [MASSIVE](https://github.com/alexa/massive) - MASSIVE is a parallel dataset of > 1M utterances across 52 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation.
- [OSCAR (Open Super-large Crawled ALMAnaCH coRpus)](https://oscar-corpus.com) - OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the Ungoliant architecture.
- [P3](https://huggingface.co/datasets/bigscience/P3) - P3 (Public Pool of Prompts) is a collection of prompted English datasets covering a diverse set of NLP tasks.
- [Self-Instruct](https://huggingface.co/datasets/yizhongw/self_instruct) - Self-Instruct is a framework that helps language models improve their ability to follow natural language instructions.
- [WikiMatrix](https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix) - WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia.
- [xP3](https://huggingface.co/datasets/bigscience/xP3) - xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks.

# Documentation links

- [Graphic of learning rate schedulers](https://raw.githubusercontent.com/rasbt/machine-learning-notes/7abac1b3dfe47b84887fcee80e5cca0e7ebf5061/learning-rates/scheduler-comparison/overview.png) - Simple visualizations of learning rate schedulers.
- [ML Papers Explained](https://github.com/dair-ai/ML-Papers-Explained) - List of LM papers explained.
- [Transformer](https://nn.labml.ai/transformers/index.html) - Explanation of Transformer architecture from the code.
- [Transformer models: an introduction and catalog, 2023 Edition](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376) - An introduction and catalog of Transformer models.
- [Transformers from Scratch](https://e2eml.school/transformers.html) - Explanation of Transformer architecture.
- [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) - Understanding how self-attention works from scratch.
