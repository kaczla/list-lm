# Language Models

This repository contains language models base on Transformer architecture.

# Model lists

| Year of publication | Name                           | Links (ðŸ—Ž - PAPER, âš™ - CODE, ðŸŽž - Video)                                                                                                                                         |
|---------------------|--------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 2017                | Transformer                    | [ðŸ—Ž](https://arxiv.org/abs/1706.03762)                                                                                                                                           |
| 2017                | MoE                            | [ðŸ—Ž](https://arxiv.org/abs/1701.06538)                                                                                                                                           |
| 2018                | GPT                            | [ðŸ—Ž](https://openai.com/blog/language-unsupervised/)                                                                                                                             |
| 2018                | BERT                           | [ðŸ—Ž](https://arxiv.org/abs/1810.04805)                                                                                                                                           |
| 2018                | flair                          | [ðŸ—Ž](https://aclanthology.org/C18-1139/) [âš™](https://github.com/flairNLP/flair)                                                                                                  |
| 2018                | RR-Transformer                 | [ðŸ—Ž](https://arxiv.org/abs/1803.02155)                                                                                                                                           |
| 2019                | LightConv/DynamicConv          | [ðŸ—Ž](https://arxiv.org/abs/1901.10430) [âš™](https://github.com/pytorch/fairseq/tree/master/examples/pay_less_attention_paper)                                                     |
| 2019                | GPT-2                          | [ðŸ—Ž](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) [âš™](https://github.com/openai/gpt-2)                                 |
| 2019                | Transformer-XL                 | [ðŸ—Ž](https://arxiv.org/abs/1901.02860) [âš™](https://github.com/kimiyoung/transformer-xl)                                                                                          |
| 2019                | Evolved Transformer            | [ðŸ—Ž](https://arxiv.org/abs/1901.11117)                                                                                                                                           |
| 2019                | XLNet                          | [ðŸ—Ž](https://arxiv.org/abs/1906.08237)                                                                                                                                           |
| 2019                | RoBERTa                        | [ðŸ—Ž](https://arxiv.org/abs/1907.11692) [âš™](https://github.com/pytorch/fairseq/tree/master/examples/roberta)                                                                      |
| 2019                | ALBERT                         | [ðŸ—Ž](https://arxiv.org/abs/1909.11942) [âš™](https://github.com/google-research/ALBERT)                                                                                            |
| 2019                | CTRL                           | [ðŸ—Ž](https://arxiv.org/abs/1909.05858) [âš™](https://github.com/salesforce/ctrl)                                                                                                   |
| 2019                | StructBERT                     | [ðŸ—Ž](https://arxiv.org/abs/1908.04577)                                                                                                                                           |                                                                                                                                          |
| 2019                | Adaptive Span                  | [ðŸ—Ž](https://arxiv.org/abs/1905.07799) [âš™](https://github.com/facebookresearch/adaptive-span)                                                                                    |
| 2019                | All-attention network          | [ðŸ—Ž](https://arxiv.org/abs/1907.01470)                                                                                                                                           |
| 2019                | Sparse Transformer             | [ðŸ—Ž](https://arxiv.org/abs/1904.10509)                                                                                                                                           |
| 2019                | Adaptively Sparse Transformers | [ðŸ—Ž](https://arxiv.org/abs/1909.00015)                                                                                                                                           |
| 2019                | Megatron-LM                    | [ðŸ—Ž](https://arxiv.org/abs/1909.08053) [âš™](https://github.com/NVIDIA/Megatron-LM)                                                                                                |
| 2019                | kNN-LM                         | [ðŸ—Ž](https://arxiv.org/abs/1911.00172) [âš™](https://github.com/urvashik/knnlm)                                                                                                    |
| 2019                | TENER                          | [ðŸ—Ž](https://arxiv.org/abs/1911.04474)                                                                                                                                           |
| 2020                | MobileBERT                     | [ðŸ—Ž](https://arxiv.org/abs/2004.02984) [âš™](https://github.com/google-research/google-research/tree/master/mobilebert)                                                            |
| 2020                | Poor Man's BERT                | [ðŸ—Ž](https://arxiv.org/abs/2004.03844) [âš™](https://github.com/hsajjad/transformers)                                                                                              |
| 2020                | Longformer                     | [ðŸ—Ž](https://arxiv.org/abs/2004.05150) [âš™](https://github.com/allenai/longformer)                                                                                                |
| 2020                | Linformer                      | [ðŸ—Ž](https://arxiv.org/abs/2006.04768) [âš™](https://github.com/Kyan820815/Linformer)                                                                                              |
| 2020                | Routing Transformer            | [ðŸ—Ž](https://arxiv.org/abs/2003.05997) [âš™](https://github.com/google-research/google-research/tree/master/routing_transformer)                                                   |
| 2020                | Sandwich Transformers          | [ðŸ—Ž](https://arxiv.org/abs/1911.03864) [âš™](https://github.com/ofirpress/sandwich_transformer) [ðŸŽž](https://www.youtube.com/watch?v=rFuuGEj3AhU)                                  |
| 2020                | MPNet                          | [ðŸ—Ž](https://arxiv.org/abs/2004.09297) [âš™](https://github.com/microsoft/MPNet)                                                                                                   |
| 2020                | DeeBERT                        | [ðŸ—Ž](https://arxiv.org/abs/2004.12993) [âš™](https://github.com/castorini/DeeBERT)                                                                                                 |
| 2020                | BiT (Big Transfer)             | [ðŸ—Ž](https://arxiv.org/abs/1912.11370)                                                                                                                                           |
| 2020                | Adaptive Transformers          | [ðŸ—Ž](https://arxiv.org/abs/2005.07486) [âš™](https://github.com/prajjwal1/adaptive_transformer)                                                                                    |
| 2020                | Synthesizer                    | [ðŸ—Ž](https://arxiv.org/abs/2005.00743) [âš™](https://github.com/leaderj1001/Synthesizer-Rethinking-Self-Attention-Transformer-Models) / [âš™](https://github.com/10-zin/Synthesizer) |
| 2020                | BRC                            | [ðŸ—Ž](https://arxiv.org/abs/2006.05252) [âš™](https://github.com/nvecoven/BRC) / [âš™](https://github.com/742617000027/nBRC)                                                          |
| 2020                | Feedback Transformer           | [ðŸ—Ž](https://arxiv.org/abs/2002.09402)                                                                                                                                           |
| 2020                | Shortformer                    | [ðŸ—Ž](https://arxiv.org/abs/2012.15832) [âš™](https://github.com/ofirpress/shortformer)                                                                                             |
| 2020                | Floater                        | [ðŸ—Ž](https://arxiv.org/abs/2003.09229) [âš™](https://github.com/xuanqing94/FLOATER)                                                                                                |
| 2020                | Informer                       | [ðŸ—Ž](https://arxiv.org/abs/2012.07436)                                                                                                                                           |
| 2020                | TUPE                           | [ðŸ—Ž](https://arxiv.org/abs/2006.15595) [âš™](https://github.com/guolinke/TUPE)                                                                                                     |
| 2020                | Key-Value Memory Feed-Forward  | [ðŸ—Ž](https://arxiv.org/abs/2012.14913)                                                                                                                                           |
| 2022                | DA-Transformer                 | [ðŸ—Ž](https://arxiv.org/abs/2010.06925)                                                                                                                                           |
| 2021                | MiniLM                         | [ðŸ—Ž](https://arxiv.org/abs/2002.10957)                                                                                                                                           |
| 2021                | MiniLM v2                      | [ðŸ—Ž](https://arxiv.org/abs/2012.15828)                                                                                                                                           |
| 2021                | GLM                            | [ðŸ—Ž](https://arxiv.org/abs/2103.10360) [âš™](https://github.com/THUDM/GLM)                                                                                                         |
| 2021                | ByT5                           | [ðŸ—Ž](https://arxiv.org/abs/2105.13626) [âš™](https://github.com/google-research/byt5)                                                                                              |
| 2021                | Luna                           | [ðŸ—Ž](https://arxiv.org/abs/2106.01540) [âš™](https://github.com/XuezheMax/fairseq-apollo)                                                                                          |
| 2021                | UniT                           | [ðŸ—Ž](https://arxiv.org/abs/2102.10772) [âš™](https://github.com/facebookresearch/mmf)                                                                                              |
| 2021                | Charformer                     | [ðŸ—Ž](https://arxiv.org/abs/2106.12672) [âš™](https://github.com/google-research/google-research/tree/master/charformer)                                                            |
| 2021                | Primer                         | [ðŸ—Ž](https://arxiv.org/abs/2109.08668) [âš™](https://github.com/google-research/google-research/tree/master/primer)                                                                |
| 2021                | FNet                           | [ðŸ—Ž](https://arxiv.org/abs/2105.03824) [âš™](https://github.com/google-research/google-research/tree/master/f_net) [ðŸŽž](https://www.youtube.com/watch?v=JJR3pBl78zw)               |
| 2021                | Megatron-LM v2                 | [ðŸ—Ž](https://arxiv.org/abs/2104.04473) [âš™](https://github.com/NVIDIA/Megatron-LM)                                                                                                |
| 2021                | Switch Transformer             | [ðŸ—Ž](https://arxiv.org/abs/2101.03961) [âš™](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)                                                    |
| 2021                | PanGu                          | [ðŸ—Ž](https://arxiv.org/abs/2104.12369)                                                                                                                                           |
| 2021                | T0                             | [ðŸ—Ž](https://arxiv.org/abs/2110.08207) [âš™](https://github.com/bigscience-workshop/t-zero) [ðŸ¤—](https://huggingface.co/bigscience/T0)                                             |
| 2022                | Gopher                         | [ðŸ—Ž](https://arxiv.org/abs/2112.11446)                                                                                                                                           |
| 2022                | Megatron-Turing NLG            | [ðŸ—Ž](https://arxiv.org/abs/2201.11990)                                                                                                                                           |
| 2022                | Chinchilla                     | [ðŸ—Ž](https://arxiv.org/abs/2203.15556)                                                                                                                                           |
| 2022                | OPT                            | [ðŸ—Ž](https://arxiv.org/abs/2205.01068) [âš™](https://github.com/facebookresearch/metaseq)                                                                                          |
| 2022                | RWKV-LM                        | [âš™](https://github.com/BlinkDL/RWKV-LM) [ðŸ¤—](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)                                                                                      |
| 2022                | UL2                            | [ðŸ—Ž](https://arxiv.org/abs/2205.05131) [âš™](https://github.com/google-research/google-research/tree/master/ul2)                                                                   |
| 2022                | FlanT5                         | [âš™](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints) [ðŸ—Ž](https://arxiv.org/abs/2210.11416) [ðŸ¤—](https://huggingface.co/google/flan-t5-base) |

# Model links

- [ColossalAI](https://github.com/hpcaitech/ColossalAI) - Colossal-AI: A Unified Deep Learning System for Big Model Era
- [ExtremeBERT](https://github.com/extreme-bert/extreme-bert) - ExtremeBERT is a toolkit that accelerates the pretraining of customized language models on customized datasets.
- [FlagAI](https://github.com/FlagAI-Open/FlagAI) - FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.
- [Kernl](https://github.com/ELS-RD/kernl) - Kernl lets you run Pytorch transformer models several times faster on GPU with a single line of code, and is designed to be easily hackable.
- [LASSL](https://github.com/lassl/lassl) - LASSL is a LAnguage framework for Self-Supervised Learning. LASSL aims to provide an easy-to-use framework for pretraining language model by only using Huggingface's Transformers and Datasets.
- [Latent Diffusion Models](https://github.com/CompVis/latent-diffusion) - High-Resolution Image Synthesis with Latent Diffusion Models, contains: Text-to-Image, Inpainting
- [LiBai](https://github.com/Oneflow-Inc/libai) - LiBai is a large-scale open-source model training toolbox based on OneFlow
- [metaseq](https://github.com/facebookresearch/metaseq) - A codebase for working with Open Pre-trained Transformers.
- [MMF](https://github.com/facebookresearch/mmf) - MMF is a modular framework for vision and language multimodal research
- [multimodal](https://github.com/facebookresearch/multimodal) - TorchMultimodal is a PyTorch library for training state-of-the-art multimodal multi-task models at scale.
- [NVIDIA Merlin](https://github.com/NVIDIA-Merlin/Merlin) - NVIDIA Merlin is an open source library providing end-to-end GPU-accelerated recommender systems, from feature engineering and preprocessing to training deep learning models and running inference in production.
- [RWKV](https://github.com/BlinkDL/RWKV-LM) - RWKV is a RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer: great performance, fast inference, saves VRAM, fast training, "infinite" ctx_len, and free sentence embedding.
- [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers) - This library is based on the Transformers library by HuggingFace. Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model.
- [small-text](https://github.com/webis-de/small-text) - Small-Text provides state-of-the-art Active Learning for Text Classification
- [Sockeye](https://github.com/awslabs/sockeye) - Sockeye is an open-source sequence-to-sequence framework for Neural Machine Translation built on PyTorch.
- [SparseZoo](https://github.com/neuralmagic/sparsezoo) - Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes.
- [tasknet](https://github.com/sileod/tasknet) - Integration of HuggingFace Datasets with HuggingFace Trainer, and multitasking.
- [TorchScale](https://github.com/microsoft/torchscale) - TorchScale is a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively. It has the implementation of fundamental research to improve modeling generality and capability, as well as training stability and efficiency of scaling Transformers.
- [Trankit](https://github.com/nlp-uoregon/trankit) - A Light-Weight Transformer-based Python Toolkit for Multilingual Natural Language Processing
- [transformers](https://github.com/huggingface/transformers) - State-of-the-art Natural Language Processing for PyTorch and TensorFlow 2.0
- [txtai](https://github.com/neuml/txtai) - txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.
- [voltaML](https://github.com/VoltaML/voltaML) - VoltaML is a lightweight library to convert and run your ML/DL deep learning models in high performance inference runtimes like TensorRT, TorchScript, ONNX and TVM.
- [x-transformers](https://github.com/lucidrains/x-transformers) - A simple but complete full-attention transformer with a set of promising experimental features from various papers
- [xFormers](https://github.com/facebookresearch/xformers) - xFormers is a modular and field agnostic library to flexibly generate transformer architectures by interoperable and optimized building blocks

# Utils links

- [bagua](https://github.com/BaguaSys/bagua) - Bagua is a deep learning training acceleration framework for PyTorch
- [bricks](https://github.com/code-kern-ai/bricks) - Open-source natural language enrichments at your fingertips.
- [cleanlab](https://github.com/cleanlab/cleanlab) - cleanlab automatically finds and fixes errors in any ML dataset
- [cyclemoid-pytorch](https://github.com/rasbt/cyclemoid-pytorch) - This is an implementation of the cyclemoid activation function for PyTorch.
- [DeepSparse](https://github.com/neuralmagic/deepsparse) - Inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application.
- [Explainpaper](https://www.explainpaper.com) - Upload a paper, highlight confusing text, get an explanation.
- [FastDeploy](https://github.com/PaddlePaddle/FastDeploy) - Easy-to-use and Fast Deep Learning Model Deployment Toolkit for Cloud Mobile and Edge. Including Image, Video, Text and Audio 20+ main stream scenarios and 150+ SOTA models with end-to-end optimization, multi-platform and multi-framework support.
- [Horovod](https://github.com/horovod/horovod) - Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet
- [incdbscan](https://github.com/DataOmbudsman/incdbscan) - the incremental version of the DBSCAN clustering algorithm
- [langchain](https://github.com/hwchase17/langchain) - Building applications with LLMs through composability.
- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) - A framework for few-shot evaluation of autoregressive language models.
- [Lovely Tensors](https://github.com/xl0/lovely-tensors) - Tensors, ready for human consumption.
- [Merlin Dataloader](https://github.com/NVIDIA-Merlin/dataloader) - The merlin dataloader lets you rapidly load tabular data for training deep leaning models with TensorFlow, PyTorch or JAX
- [mup](https://github.com/microsoft/mup) - Maximal Update Parametrization (Î¼P) and Hyperparameter Transfer (Î¼Transfer): Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer
- [Opacus](https://github.com/pytorch/opacus) - Opacus is a library that enables training PyTorch models with differential privacy. It supports training with minimal code changes required on the client, has little impact on training performance, and allows the client to online track the privacy budget expended at any given moment.
- [skorch](https://github.com/skorch-dev/skorch) - A scikit-learn compatible neural network library that wraps PyTorch.
- [SparseML](https://github.com/neuralmagic/sparseml) - Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models.
- [surgeon-pytorch](https://github.com/archinetai/surgeon-pytorch) - A library to inspect and extract intermediate layers of PyTorch models.
- [tbparse](https://github.com/j3soon/tbparse) - A simple yet powerful tensorboard event log parser/reader.
- [tinygrad](https://github.com/geohot/tinygrad) - tinygrad is an extremely simple deep-learning framework. It supports both inference and training.
- [TorchBench](https://github.com/pytorch/benchmark) - TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.
- [torchdistX](https://github.com/pytorch/torchdistx) - Torch Distributed Experimental
- [torchegranate](https://github.com/jmschrei/torchegranate) - Fast, flexible and easy to use probabilistic modelling with PyTorch.
- [WeightWatcher](https://github.com/CalculatedContent/WeightWatcher) - WeightWatcher (WW): is an open-source, diagnostic tool for analyzing Deep Neural Networks (DNN), without needing access to training or even test data.

# GPU profiling links

- [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) - NVIDIA Nsightâ„¢ Systems is a system-wide performance analysis tool designed to visualize an applicationâ€™s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest system on a chip (SoC).
- [NVIDIA Visual Profiler](https://developer.nvidia.com/nvidia-visual-profiler) - The NVIDIA Visual Profiler is a cross-platform performance profiling tool that delivers developers vital feedback for optimizing CUDA C/C++ applications.
- [NVIDIA/NVTX](https://github.com/NVIDIA/NVTX) - The NVIDIAÂ® Tools Extension SDK (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications.
- [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) - This recipe explains how to use PyTorch profiler and measure the time and memory consumption of the modelâ€™s operators.
- [torchnnprofiler](https://github.com/kshitij12345/torchnnprofiler) - Context Manager to profile the forward and backward times of PyTorch's nn.Module

# Visualizations links

- [BertViz](https://github.com/jessevig/bertviz) - BertViz is a tool for visualizing attention in the Transformer model, supporting most models from the transformers library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, BART, etc.)
- [Ecoo](https://github.com/jalammar/ecco) - Ecco is a python library for exploring and explaining Natural Language Processing models using interactive visualizations.

# Vocabulary links

- [SentencePiece](https://github.com/google/sentencepiece) - SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) and unigram language model ) with the extension of direct training from raw sentences.
- [tiktoken](https://github.com/openai/tiktoken) - tiktoken is a fast BPE tokeniser for use with OpenAI's models.
- [tokenizers](https://github.com/huggingface/tokenizers) - Provides an implementation of today's most used tokenizers, with a focus on performance and versatility.

# Dataset links

- [BIG-bench](https://github.com/google/BIG-bench) - The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. The more than 200 tasks included in BIG-bench.
- [CCMatrix](https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix) - CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB.
- [CCNet](https://github.com/facebookresearch/cc_net) - Tools to download and clean Common Crawl as introduced in our paper CCNet: High Quality Monolingual Datasets from Web Crawl Data.
- [Deduplicated CommonCrawl Text](http://statmt.org/ngrams/deduped) - Processed Common Crawl snapshots.
- [DS-1000](https://github.com/HKUNLP/DS-1000) - Official data and code release for the paper DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.
- [FewGLUE](https://github.com/timoschick/fewglue) - FewGLUE dataset, consisting of a random selection of 32 training examples from the SuperGLUE training sets and up to 20,000 unlabeled examples for each SuperGLUE task.
- [MASSIVE](https://github.com/alexa/massive) - MASSIVE is a parallel dataset of > 1M utterances across 52 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation.
- [OSCAR (Open Super-large Crawled ALMAnaCH coRpus)](https://oscar-corpus.com) - OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the Ungoliant architecture.
- [WikiMatrix](https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix) - WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia.

# Documentation links

- [Graphic of learning rate schedulers](https://raw.githubusercontent.com/rasbt/machine-learning-notes/7abac1b3dfe47b84887fcee80e5cca0e7ebf5061/learning-rates/scheduler-comparison/overview.png) - Simple visualizations of learning rate schedulers.
- [Transformer](https://nn.labml.ai/transformers/index.html) - Explanation of Transformer architectures from the code.
