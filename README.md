# Language Models

This repository contains language models base on Transformer architecture.

[TOC]

# Model lists

- Transformer
  - Year: 2017
  - Publication: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- MoE
  - Year: 2017
  - Publication: [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

- GPT
  - Year: 2018
  - Publication: [Blog - Improving Language Understanding with Unsupervised Learning](https://openai.com/blog/language-unsupervised)

- BERT
  - Year: 2018
  - Publication: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

- flair
  - Year: 2018
  - Publication: [Contextual String Embeddings for Sequence Labeling](https://aclanthology.org/C18-1139)
  - Code: [GitHub](https://github.com/flairNLP/flair)

- RR-Transformer
  - Year: 2018
  - Publication: [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)

- LightConv/DynamicConv
  - Year: 2019
  - Publication: [Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/abs/1901.10430)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/pay_less_attention_paper)

- GPT-2
  - Year: 2019
  - Publication: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - Code: [GitHub](https://github.com/openai/gpt-2)

- Transformer-XL
  - Year: 2019
  - Publication: [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
  - Code: [GitHub](https://github.com/kimiyoung/transformer-xl)

- Evolved Transformer
  - Year: 2019
  - Publication: [The Evolved Transformer](https://arxiv.org/abs/1901.11117)

- XLNet
  - Year: 2019
  - Publication: [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)

- RoBERTa
  - Year: 2019
  - Publication: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/roberta)

- ALBERT
  - Year: 2019
  - Publication: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
  - Code: [GitHub](https://github.com/google-research/ALBERT)

- CTRL
  - Year: 2019
  - Publication: [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858)
  - Code: [GitHub](https://github.com/salesforce/ctrl)

- StructBERT
  - Year: 2019
  - Publication: [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)

- Adaptive Span
  - Year: 2019
  - Publication: [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)
  - Code: [GitHub](https://github.com/facebookresearch/adaptive-span)

- All-attention network
  - Year: 2019
  - Publication: [Augmenting Self-attention with Persistent Memory](https://arxiv.org/abs/1907.01470)

- Sparse Transformer
  - Year: 2019
  - Publication: [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)

- Adaptively Sparse Transformers
  - Year: 2019
  - Publication: [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015)

- Megatron-LM
  - Year: 2019
  - Publication: [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- kNN-LM
  - Year: 2019
  - Publication: [Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172)
  - Code: [GitHub](https://github.com/urvashik/knnlm)

- TENER
  - Year: 2019
  - Publication: [TENER: Adapting Transformer Encoder for Named Entity Recognition](https://arxiv.org/abs/1911.04474)

- ERNIE
  - Year: 2019
  - Publication: [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)
  - Model weights: [HuggingFace models](https://huggingface.co/PaddlePaddle/ernie-1.0-base-zh)

- ERNIE 2.0
  - Year: 2019
  - Publication: [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/abs/1907.12412)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)
  - Model weights: [HuggingFace models](https://huggingface.co/PaddlePaddle/ernie-2.0-base-en)

- MobileBERT
  - Year: 2020
  - Publication: [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/mobilebert)

- Poor Man's BERT
  - Year: 2020
  - Publication: [On the Effect of Dropping Layers of Pre-trained Transformer Models](https://arxiv.org/abs/2004.03844)
  - Code: [GitHub](https://github.com/hsajjad/transformers)

- Longformer
  - Year: 2020
  - Publication: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
  - Code: [GitHub](https://github.com/allenai/longformer)

- Linformer
  - Year: 2020
  - Publication: [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
  - Code: [GitHub](https://github.com/Kyan820815/Linformer)

- Routing Transformer
  - Year: 2020
  - Publication: [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/abs/2003.05997)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/routing_transformer)

- Sandwich Transformers
  - Year: 2020
  - Publication: [https://arxiv.org/abs/1911.03864](https://arxiv.org/abs/1911.03864)
  - Video: [YouTube](https://www.youtube.com/watch?v=rFuuGEj3AhU)
  - Code: [GitHub](https://github.com/ofirpress/sandwich_transformer)

- MPNet
  - Year: 2020
  - Publication: [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)
  - Code: [GitHub](https://github.com/microsoft/MPNet)

- DeeBERT
  - Year: 2020
  - Publication: [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993)
  - Code: [GitHub](https://github.com/castorini/DeeBERT)

- BiT (Big Transfer)
  - Year: 2020
  - Publication: [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)

- Adaptive Transformers
  - Year: 2020
  - Publication: [Adaptive Transformers for Learning Multimodal Representations](https://arxiv.org/abs/2005.07486)
  - Code: [GitHub](https://github.com/prajjwal1/adaptive_transformer)

- Synthesizer
  - Year: 2020
  - Publication: [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
  - Code: [GitHub](https://github.com/leaderj1001/Synthesizer-Rethinking-Self-Attention-Transformer-Models) / [GitHub](https://github.com/10-zin/Synthesizer)

- BRC
  - Year: 2020
  - Publication: [A bio-inspired bistable recurrent cell allows for long-lasting memory](https://arxiv.org/abs/2006.05252)
  - Code: [GitHub](https://github.com/nvecoven/BRC) / [GitHub](https://github.com/742617000027/nBRC)

- Feedback Transformer
  - Year: 2020
  - Publication: [Addressing Some Limitations of Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402)

- Shortformer
  - Year: 2020
  - Publication: [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)
  - Code: [GitHub](https://github.com/ofirpress/shortformer)

- Floater
  - Year: 2020
  - Publication: [Learning to Encode Position for Transformer with Continuous Dynamical Model](https://arxiv.org/abs/2003.09229)
  - Code: [GitHub](https://github.com/xuanqing94/FLOATER)

- Informer
  - Year: 2020
  - Publication: [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436)

- TUPE
  - Year: 2020
  - Publication: [Rethinking Positional Encoding in Language Pre-training](https://arxiv.org/abs/2006.15595)
  - Code: [GitHub](https://github.com/guolinke/TUPE)

- Key-Value Memory Feed-Forward
  - Year: 2020
  - Publication: [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)

- DA-Transformer
  - Year: 2020
  - Publication: [DA-Transformer: Distance-aware Transformer](https://arxiv.org/abs/2010.06925)

- DeBERTa
  - Year: 2020
  - Publication: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-base)

- MiniLM
  - Year: 2021
  - Publication: [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957)

- MiniLM v2
  - Year: 2021
  - Publication: [MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://arxiv.org/abs/2012.15828)

- GLM
  - Year: 2021
  - Publication: [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)
  - Code: [GitHub](https://github.com/THUDM/GLM)
  - Model weights: [HuggingFace models](https://huggingface.co/BAAI/glm-10b)

- ByT5
  - Year: 2021
  - Publication: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
  - Code: [GitHub](https://github.com/google-research/byt5)

- Luna
  - Year: 2021
  - Publication: [Luna: Linear Unified Nested Attention](https://arxiv.org/abs/2106.01540)
  - Code: [GitHub](https://github.com/XuezheMax/fairseq-apollo)

- UniT
  - Year: 2021
  - Publication: [UniT: Multimodal Multitask Learning with a Unified Transformer](https://arxiv.org/abs/2102.10772)
  - Code: [GitHub](https://github.com/facebookresearch/mmf)

- Charformer
  - Year: 2021
  - Publication: [Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](https://arxiv.org/abs/2106.12672)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/charformer)

- Primer
  - Year: 2021
  - Publication: [Primer: Searching for Efficient Transformers for Language Modeling](https://arxiv.org/abs/2109.08668)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/primer)

- FNet
  - Year: 2021
  - Publication: [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824)
  - Video: [YouTube](https://www.youtube.com/watch?v=JJR3pBl78zw)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/f_net)

- Megatron-LM v2
  - Year: 2021
  - Publication: [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- Switch Transformer
  - Year: 2021
  - Publication: [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
  - Code: [GitHub](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)

- PanGu
  - Year: 2021
  - Publication: [PanGu: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation](https://arxiv.org/abs/2104.12369)

- 24hBERT - Academic Budget BERT
  - Year: 2021
  - Publication: [How to Train BERT with an Academic Budget](https://arxiv.org/abs/2104.07705)
  - Code: [GitHub](https://github.com/IntelLabs/academic-budget-bert)

- T0
  - Year: 2021
  - Publication: [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)
  - Code: [GitHub](https://github.com/bigscience-workshop/t-zero)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/T0)

- XGLM
  - Year: 2021
  - Publication: [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)
  - Code: [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/xglm)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/xglm-7.5B)

- Jurassic-1
  - Year: 2021
  - Publication: [Jurassic-1: Technical Details And Evaluation](https://assets.website-files.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)
  - Code: [GitHub](https://github.com/ai21labs/lm-evaluation)

- WuDao
  - Year: 2021
  - Publication: [Article: Five Key Facts Wu Dao 2.0: The Largest Transformer Model Ever Built](https://medium.com/dataseries/five-key-facts-wu-dao-2-0-the-largest-transformer-model-ever-built-19316159796b)

- GPT-J
  - Year: 2021
  - Publication: [Blog - GPT-J](https://www.eleuther.ai/artifacts/gpt-j)
  - Code: [GitHub](https://github.com/kingoflolz/mesh-transformer-jax/)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/gpt-j-6b)

- ERNIE 3.0
  - Year: 2021
  - Publication: [ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137)

- HyperCLOVA
  - Year: 2021
  - Publication: [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://arxiv.org/abs/2109.04650)

- ALiBi
  - Year: 2021
  - Publication: [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)
  - Code: [GitHub](https://github.com/ofirpress/attention_with_linear_biases)
  - Model weights: [Private page](https://github.com/ofirpress/attention_with_linear_biases/tree/master/examples/language_model)

- RoFormer
  - Year: 2021
  - Publication: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
  - Code: [GitHub](https://github.com/ZhuiyiTechnology/roformer)
  - Model weights: [HuggingFace models](https://huggingface.co/junnyu/roformer_chinese_base)

- DeBERTaV2
  - Year: 2021
  - Publication: [Blog - Microsoft DeBERTa surpasses human performance on the SuperGLUE benchmark](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-v2-xlarge)

- DeBERTaV3
  - Year: 2021
  - Publication: [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing](https://arxiv.org/abs/2111.09543)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-v3-base)

- NormFormer
  - Year: 2021
  - Publication: [NormFormer: Improved Transformer Pretraining with Extra Normalization](https://arxiv.org/abs/2110.09456)

- CodeT5
  - Year: 2021
  - Publication: [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/abs/2109.00859)
  - Code: [GitHub](https://github.com/salesforce/CodeT5)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codet5-base)

- Memory efficient attention
  - Year: 2021
  - Publication: [Self-attention Does Not Need O(n2) Memory](https://arxiv.org/abs/2112.05682)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/memory_efficient_attention)

- EL-Attention
  - Year: 2021
  - Publication: [EL-Attention: Memory Efficient Lossless Attention for Generation](https://arxiv.org/abs/2105.04779)
  - Code: [GitHub](https://github.com/microsoft/fastseq)

- Minerva
  - Year: 2022
  - Publication: [Solving Quantitative Reasoning Problems with Language Models](https://arxiv.org/abs/2206.14858)

- BLOOM
  - Year: 2022
  - Publication: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloom)

- BLOOMZ & mT0
  - Year: 2022
  - Publication: [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloomz)

- Gopher
  - Year: 2022
  - Publication: [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)

- Megatron-Turing NLG
  - Year: 2022
  - Publication: [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)

- Chinchilla
  - Year: 2022
  - Publication: [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)

- OPT
  - Year: 2022
  - Publication: [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)
  - Code: [GitHub](https://github.com/facebookresearch/metaseq)

- RWKV-LM
  - Year: 2022
  - Code: [GitHub](https://github.com/BlinkDL/RWKV-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)

- UL2
  - Year: 2022
  - Publication: [UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/ul2)

- FlanT5
  - Year: 2022
  - Publication: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
  - Code: [GitHub](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)
  - Model weights: [HuggingFace models](https://huggingface.co/google/flan-t5-base)

- FlashAttention
  - Year: 2022
  - Publication: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
  - Code: [GitHub](https://github.com/HazyResearch/flash-attention)

- Galactica
  - Year: 2022
  - Publication: [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
  - Code: [GitHub](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/galactica-6.7b)

- NPM
  - Year: 2022
  - Publication: [Nonparametric Masked Language Modeling](https://arxiv.org/abs/2212.01349)
  - Code: [GitHub](https://github.com/facebookresearch/NPM)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/npm)

- Cramming
  - Year: 2022
  - Publication: [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034)
  - Code: [GitHub](https://github.com/JonasGeiping/cramming)

- GLM-130B
  - Year: 2022
  - Publication: [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)
  - Code: [GitHub](https://github.com/THUDM/GLM-130B)
  - Model weights: [HuggingFace models](https://huggingface.co/spaces/THUDM/GLM-130B)

- OPT-IML
  - Year: 2022
  - Publication: [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/abs/2212.12017)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/opt-iml-30b)

- GPT-NeoX
  - Year: 2022
  - Publication: [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)
  - Code: [GitHub](https://github.com/EleutherAI/gpt-neox)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/gpt-neox-20b)

- LaMDA
  - Year: 2022
  - Publication: [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239)

- PaLM
  - Year: 2022
  - Publication: [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)

- Megatron-Turing NLG
  - Year: 2022
  - Publication: [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)

- CodeRL
  - Year: 2022
  - Publication: [CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning](https://arxiv.org/abs/2207.01780)
  - Code: [GitHub](https://github.com/salesforce/CodeRL)

- E-SPA (Exponential Signal Preserving Attention)
  - Year: 2023
  - Publication: [Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation](https://arxiv.org/abs/2302.10322)

- Hyena
  - Year: 2023
  - Publication: [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866)
  - Code: [GitHub](https://github.com/HazyResearch/safari)

- SpikeGPT
  - Year: 2023
  - Publication: [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks](https://arxiv.org/abs/2302.13939)
  - Code: [GitHub](https://github.com/ridgerchu/SpikeGPT)

- LLama
  - Year: 2023
  - Publication: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
  - Code: [GitHub](https://github.com/facebookresearch/llama)
  - Model weights: [Private page - request required](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)

- KOSMOS-1
  - Year: 2023
  - Publication: [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045)

- ParaFormer
  - Year: 2023
  - Publication: [ParaFormer: Parallel Attention Transformer for Efficient Feature Matching](https://arxiv.org/abs/2303.00941)

- PaLM-E
  - Year: 2023
  - Publication: [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)

- Alpaca
  - Year: 2023
  - Publication: [Alpaca: A Strong Open-Source Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  - Code: [GitHub](https://github.com/tatsu-lab/stanford_alpaca)

- TWM
  - Year: 2023
  - Publication: [Transformer-based World Models Are Happy With 100k Interactions](https://arxiv.org/abs/2303.07109)
  - Code: [GitHub](https://github.com/jrobine/twm)

- MosaicBERT
  - Year: 2023
  - Publication: [MosaicBERT: Pretraining BERT from Scratch for $20](https://www.mosaicml.com/blog/mosaicbert)
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mosaic-bert-base)

- GPT-4
  - Year: 2023
  - Publication: [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf)

- ChatGLM-6B
  - Year: 2023
  - Code: [GitHub](https://github.com/THUDM/ChatGLM-6B)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/chatglm-6b)

- Cerebras-GPT
  - Year: 2023
  - Publication: [Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster](https://arxiv.org/abs/2304.03208)
  - Code: [GitHub](https://github.com/Cerebras/modelzoo)
  - Model weights: [HuggingFace models](https://huggingface.co/cerebras/Cerebras-GPT-13B)

- GeoV
  - Year: 2023
  - Code: [GitHub](https://github.com/geov-ai/geov)
  - Model weights: [HuggingFace models](https://huggingface.co/GeoV/GeoV-9b)

- GPT4All
  - Year: 2023
  - Code: [GitHub](https://github.com/nomic-ai/gpt4all)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/gpt4all-lora)

- CodeGeeX
  - Year: 2023
  - Publication: [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X](https://arxiv.org/abs/2303.17568)
  - Code: [GitHub](https://github.com/THUDM/CodeGeeX)
  - Model weights: [Private page - request required](https://models.aminer.cn/codegeex/download/request)

- GPTrillion
  - Year: 2023
  - Publication: [Introducing GPTrillion - the world’s first open-source 1.5T parameter model](https://docs.google.com/document/d/1i9PivZcF9q2kQNBL-SurK_Hs5nFw24zGEWNcFrONCdo)
  - Model weights: [HuggingFace models](https://huggingface.co/banana-dev/GPTrillion)

- Pythia
  - Year: 2023
  - Publication: [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)
  - Code: [GitHub](https://github.com/EleutherAI/pythia)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/pythia-70m)

- alpaca-opt
  - Year: 2023
  - Code: [GitHub](https://github.com/Manuel030/alpaca-opt)
  - Model weights: [HuggingFace models](https://huggingface.co/Manuel030/alpaca-opt-6.7b)

- GALPACA
  - Year: 2023
  - Model weights: [HuggingFace models](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b)

- NarrowBERT
  - Year: 2023
  - Publication: [NarrowBERT: Accelerating Masked Language Model Pretraining and Inference](https://arxiv.org/abs/2301.04761)

- FastChat-T5
  - Year: 2023
  - Publication: [Twitter message](https://twitter.com/lmsysorg/status/1652037026705985537)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)
  - Code: [GitHub](https://github.com/lm-sys/FastChat)

- Vicuna
  - Year: 2023
  - Publication: [Blog - Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality](https://vicuna.lmsys.org/)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/vicuna-13b-delta-v1.1)

- GPT4All-J
  - Year: 2023
  - Code: [GitHub](https://github.com/nomic-ai/gpt4all)
  - Model weights: [Direct link](https://gpt4all.io/ggml-gpt4all-j.bin)

- MPT-1b-RedPajama-200b
  - Year: 2023
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)

- StableLM-Alpha
  - Year: 2023
  - Publication: [Blog - Stability AI Launches the First of its StableLM Suite of Language Models](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)
  - Code: [GitHub](https://github.com/stability-AI/stableLM/)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)

- Dolly
  - Year: 2023
  - Publication: [Blog - Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)
  - Code: [GitHub](https://github.com/databrickslabs/dolly)
  - Model weights: [HuggingFace models](https://huggingface.co/databricks/dolly-v2-12b)

- OpenLLaMA
  - Year: 2023
  - Code: [GitHub](https://github.com/openlm-research/open_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/openlm-research/open_llama_7b)

- ReplitLM
  - Year: 2023
  - Code: [GitHub](https://github.com/replit/ReplitLM)
  - Model weights: [HuggingFace models](https://huggingface.co/replit/replit-code-v1-3b)

- Lamini
  - Year: 2023
  - Publication: [Blog - Introducing Lamini, the LLM Engine for Rapidly Customizing Models](https://lamini.ai/blog/introducing-lamini)
  - Code: [GitHub](https://github.com/lamini-ai/lamini)
  - Model weights: [HuggingFace models](https://huggingface.co/lamini/instruct-tuned-12b)

- h2ogpt
  - Year: 2023
  - Publication: [h2oGPT: Democratizing Large Language Models](https://arxiv.org/abs/2306.08161)
  - Code: [GitHub](https://github.com/h2oai/h2ogpt)
  - Model weights: [HuggingFace models](https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b)

- MPT-7B
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b)

- MPT-7B-Instruct
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-instruct)

- MPT-7B-Chat
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-chat)

- StarCoder
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoder)

- StarChat Alpha
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceH4/starchat-alpha)

- DLite
  - Year: 2023
  - Publication: [Blog - Announcing DLite V2: Lightweight, Open LLMs That Can Run Anywhere](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e)
  - Model weights: [HuggingFace models](https://huggingface.co/aisquared/dlite-v2-1_5b)

- PaLM 2
  - Year: 2023
  - Publication: [Blog - Introducing PaLM 2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/)

- SantaCoder
  - Year: 2023
  - Publication: [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/santacoder)
  - Code: [GitHub](https://github.com/bigcode-project/Megatron-LM)

- CodeGen2
  - Year: 2023
  - Publication: [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309)
  - Code: [GitHub](https://github.com/salesforce/CodeGen2)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen2-16B)

- OpenFlamingo
  - Year: 2023
  - Publication: [Blog - ANNOUNCING OPENFLAMINGO: AN OPEN-SOURCE FRAMEWORK FOR TRAINING VISION-LANGUAGE MODELS WITH IN-CONTEXT LEARNING](https://laion.ai/blog/open-flamingo/)
  - Code: [GitHub](https://github.com/mlfoundations/open_flamingo)
  - Model weights: [HuggingFace models](https://huggingface.co/openflamingo/OpenFlamingo-9B)

- Multimodal-GPT
  - Year: 2023
  - Publication: [MultiModal-GPT: A Vision and Language Model for Dialogue with Humans](https://arxiv.org/abs/2305.04790)
  - Code: [GitHub](https://github.com/open-mmlab/Multimodal-GPT)
  - Model weights: [Direct link](https://download.openmmlab.com/mmgpt/v0/mmgpt-lora-v0-release.pt)

- OpenLLaMA
  - Year: 2023
  - Code: [GitHub](https://github.com/openlm-research/open_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/openlm-research/open_llama_13b)

- Open-source PaLM
  - Year: 2023
  - Code: [GitHub](https://github.com/conceptofmind/PaLM)
  - Model weights: [HuggingFace models](https://huggingface.co/conceptofmind/palm-1b)

- Jurassic-X
  - Year: 2023
  - Publication: [Blog - Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system)

- Jurassic-2
  - Year: 2023
  - Publication: [Blog - Announcing Jurassic-2 and Task-Specific APIs](https://www.ai21.com/blog/introducing-j2)

- Otter
  - Year: 2023
  - Publication: [Otter: A Multi-Modal Model with In-Context Instruction Tuning](https://arxiv.org/abs/2305.03726)
  - Code: [GitHub](https://github.com/Luodian/otter)
  - Model weights: [HuggingFace models](https://huggingface.co/luodian/otter-9b-hf)

- ImageBind
  - Year: 2023
  - Publication: [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665)
  - Code: [GitHub](https://github.com/facebookresearch/ImageBind)
  - Model weights: [Direct link](https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth)

- Koala
  - Year: 2023
  - Publication: [Blog - Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/)
  - Code: [GitHub](https://github.com/young-geng/EasyLM)
  - Model weights: [HuggingFace models](https://huggingface.co/young-geng/koala)

- umT5
  - Year: 2023
  - Publication: [UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining](https://arxiv.org/abs/2304.09151)
  - Code: [GitHub](https://github.com/google-research/t5x)
  - Model weights: [Direct link](https://github.com/google-research/t5x/blob/main/docs/models.md#umt5-checkpoints)

- mLongT5
  - Year: 2023
  - Publication: [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://arxiv.org/abs/2305.11129)
  - Code: [GitHub](https://github.com/google-research/longt5)

- DarkBERT
  - Year: 2023
  - Publication: [DarkBERT: A Language Model for the Dark Side of the Internet](https://arxiv.org/abs/2305.08596)

- Aurora genAI
  - Year: 2023
  - Publication: [Blog - Intel Announces Aurora genAI, Generative AI Model With 1 Trillion Parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters)

- LIMA
  - Year: 2023
  - Publication: [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)

- BLOOMChat
  - Year: 2023
  - Publication: [Blog - BLOOMChat: a New Open Multilingual Chat LLM](https://sambanova.ai/blog/introducing-bloomchat-176b-the-multilingual-chat-based-llm/)
  - Model weights: [HuggingFace models](https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1)

- Falcon
  - Year: 2023
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/falcon-40b)

- Goat
  - Year: 2023
  - Publication: [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks](https://arxiv.org/abs/2305.14201)
  - Code: [GitHub](https://github.com/liutiedong/goat)
  - Model weights: [HuggingFace models](https://huggingface.co/tiedong/goat-lora-7b)

- Macaw-LLM
  - Year: 2023
  - Code: [GitHub](https://github.com/lyuchenyang/Macaw-LLM)

- MeZO
  - Year: 2023
  - Publication: [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333)
  - Code: [GitHub](https://github.com/princeton-nlp/MeZO)

- LLaMA-Adapter
  - Year: 2023
  - Publication: [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- LLaMA-Adapter V2
  - Year: 2023
  - Publication: [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://arxiv.org/abs/2304.15010)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- OpenAssistant
  - Year: 2023
  - Publication: [OpenAssistant Conversations -- Democratizing Large Language Model Alignment](https://arxiv.org/abs/2304.07327)
  - Code: [GitHub](https://github.com/LAION-AI/Open-Assistant)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- RedPajama-INCITE
  - Year: 2023
  - Publication: [Blog - RedPajama 7B now available, instruct model outperforms all open 7B models on HELM benchmarks](https://www.together.xyz/blog/redpajama-7b)
  - Code: [GitHub](https://github.com/togethercomputer/redpajama.cpp)
  - Model weights: [HuggingFace models](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base)

- Gorilla
  - Year: 2023
  - Publication: [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)
  - Code: [GitHub](https://github.com/ShishirPatil/gorilla)

- MEGABYTE
  - Year: 2023
  - Publication: [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185)

- AlpacOOM
  - Year: 2023
  - Model weights: [HuggingFace models](https://huggingface.co/mrm8488/Alpacoom)

- CodeT5+
  - Year: 2023
  - Publication: [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922)
  - Code: [GitHub](https://github.com/salesforce/CodeT5)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codet5p-16b)

- BiomedGPT
  - Year: 2023
  - Publication: [BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks](https://arxiv.org/abs/2305.17100)
  - Code: [GitHub](https://github.com/taokz/BiomedGPT)

- NoPE
  - Year: 2023
  - Publication: [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)

- StarCoderBase
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoderbase)

- StarCoderPlus
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoderplus)

- Tulu
  - Year: 2023
  - Publication: [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751)
  - Code: [GitHub](https://github.com/allenai/open-instruct)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/tulu-65b)

- InternLM
  - Year: 2023
  - Publication: [InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities](https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf)

- MEFT
  - Year: 2023
  - Publication: [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning](https://arxiv.org/abs/2306.00477)
  - Code: [GitHub](https://github.com/baohaoLiao/mefts)

- LongMem
  - Year: 2023
  - Publication: [Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.07174)
  - Code: [GitHub](https://github.com/Victorwz/LongMem)

- SpQR
  - Year: 2023
  - Publication: [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/abs/2306.03078)
  - Code: [GitHub](https://github.com/Vahe1994/SpQR)

- BioMedLM
  - Year: 2023
  - Publication: [Blog - BioMedLM: a Domain-Specific Large Language Model for Biomedical Text](https://www.mosaicml.com/blog/introducing-pubmed-gpt)
  - Code: [GitHub](https://github.com/stanford-crfm/BioMedLM)
  - Model weights: [HuggingFace models](https://huggingface.co/stanford-crfm/BioMedLM)

- BloombergGPT
  - Year: 2023
  - Publication: [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)

- FinGPT
  - Year: 2023
  - Publication: [FinGPT: Open-Source Financial Large Language Models](https://arxiv.org/abs/2306.06031)
  - Code: [GitHub](https://github.com/ai4finance-foundation/fingpt)

- SqueezeLLM
  - Year: 2023
  - Publication: [SqueezeLLM: Dense-and-Sparse Quantization](https://export.arxiv.org/abs/2306.07629)
  - Code: [GitHub](https://github.com/SqueezeAILab/SqueezeLLM)

- WizardLM
  - Year: 2023
  - Publication: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)
  - Code: [GitHub](https://github.com/nlpxucan/WizardLM)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardLM-30B-V1.0)

- Orca
  - Year: 2023
  - Publication: [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.07174)

- LongMem
  - Year: 2023
  - Publication: [Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.02707)

- Inflection-1
  - Year: 2023
  - Publication: [Blog - Inflection-1: Pi’s Best-in-Class LLM](https://inflection.ai/inflection-1)

- Kosmos-2
  - Year: 2023
  - Publication: [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/kosmos-2)

# Model links

- [C Transformers](https://github.com/marella/ctransformers) - Python bindings for the Transformer models implemented in C/C++ using GGML library.
- [CodeGeeX](https://github.com/THUDM/CodeGeeX) - We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages.
- [CodeT5 and CodeT5+](https://github.com/salesforce/CodeT5) - Official research release for CodeT5 and CodeT5+ models for Code Understanding and Generation from Salesforce Research/
- [CodeTF](https://github.com/salesforce/CodeTF) - CodeTF is a one-stop Python transformer-based library for code large language models (Code LLMs) and code intelligence, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on.
- [ColossalAI](https://github.com/hpcaitech/ColossalAI) - Colossal-AI: A Unified Deep Learning System for Big Model Era. Making large AI models cheaper, faster and more accessible.
- [DeepSpeed-MII](https://github.com/microsoft/DeepSpeed-MII) - MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.
- [EasyLM](https://github.com/young-geng/EasyLM) - Large language models (LLMs) made easy, EasyLM is a one stop solution for pre-training, finetuning, evaluating and serving LLMs in JAX/Flax. EasyLM can scale up LLM training to hundreds of TPU/GPU accelerators by leveraging JAX's pjit functionality.
- [ExtremeBERT](https://github.com/extreme-bert/extreme-bert) - ExtremeBERT is a toolkit that accelerates the pretraining of customized language models on customized datasets.
- [FairScale](https://github.com/facebookresearch/fairscale) - FairScale is a PyTorch extension library for high performance and large scale training. This library extends basic PyTorch capabilities while adding new SOTA scaling techniques.
- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) - This repository provides a script and recipe to run the highly optimized transformer-based encoder and decoder component, and it is tested and maintained by NVIDIA.
- [FastSeq](https://github.com/microsoft/fastseq) - FastSeq provides efficient implementation of popular sequence models (e.g. Bart, ProphetNet) for text generation, summarization, translation tasks etc.
- [FlagAI](https://github.com/FlagAI-Open/FlagAI) - FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.
- [GENIUS](https://github.com/beyondguo/geniushttps://github.com/beyondguo/genius) - GENIUS generating text using sketches! A strong and general textual data augmentation tool.
- [GeoV](https://github.com/geov-ai/geov) - The GeoV model is a large langauge model designed by Georges Harik and uses Rotary Positional Embeddings with Relative distances (RoPER). We have shared a pre-trained 9B parameter model.
- [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio) - H2O LLM Studio, a framework and no-code GUI designed for fine-tuning state-of-the-art large language models (LLMs).
- [h2oGPT](https://github.com/h2oai/h2ogpt)- h2oGPT - The world's best open source GPT: open-source repository with fully permissive, commercially usable code, data and models and code for fine-tuning large language models.
- [JARVIS](https://github.com/microsoft/JARVIS) - JARVIS, a system to connect LLMs with ML community.
- [Kernl](https://github.com/ELS-RD/kernl) - Kernl lets you run Pytorch transformer models several times faster on GPU with a single line of code, and is designed to be easily hackable. Kernl is the first OSS inference engine written in OpenAI Triton, a new language designed by OpenAI to make it easier to write GPU kernels.
- [LASSL](https://github.com/lassl/lassl) - LASSL is a LAnguage framework for Self-Supervised Learning. LASSL aims to provide an easy-to-use framework for pretraining language model by only using Huggingface's Transformers and Datasets.
- [Latent Diffusion Models](https://github.com/CompVis/latent-diffusion) - High-Resolution Image Synthesis with Latent Diffusion Models, contains: Text-to-Image, Inpainting
- [LiBai](https://github.com/Oneflow-Inc/libai) - LiBai is a large-scale open-source model training toolbox based on OneFlow
- [LLM Foundry](https://github.com/mosaicml/llm-foundry) - This repository contains code for training, finetuning, evaluating, and deploying LLMs for inference with Composer and the MosaicML platform.
- [LMFlow](https://github.com/OptimalScale/LMFlow) - An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.
- [LongT5](https://github.com/google-research/longt5) - LongT5 is an extension of the T5 model that handles long sequence inputs more efficiently.
- [MaxText](https://github.com/google/maxtext) - MaxText is a high performance, arbitrarily scalable, open-source, simple, easily forkable, well-tested, batteries included LLM written in pure Python/Jax and targeting Google Cloud TPUs.
- [metaseq](https://github.com/facebookresearch/metaseq) - A codebase for working with Open Pre-trained Transformers.
- [MMF](https://github.com/facebookresearch/mmf) - MMF is a modular framework for vision and language multimodal research
- [Mosaic composer](https://github.com/mosaicml/composer) - Composer is a PyTorch library that enables you to train neural networks faster, at lower cost, and to higher accuracy. Train neural networks up to 7x faster.
- [multimodal](https://github.com/facebookresearch/multimodal) - TorchMultimodal is a PyTorch library for training state-of-the-art multimodal multi-task models at scale.
- [nanoGPT](https://github.com/karpathy/nanoGPT) - The simplest, fastest repository for training/finetuning medium-sized GPTs.
- [nanoPALM](https://github.com/RobertRiachi/nanoPALM) - nanoPALM is the simplest, fastest repository for training/finetuning small to medium-sized PALM models.
- [nanoT5](https://github.com/PiotrNawrot/nanoT5) - Fast & Simple repository for pre-training and fine-tuning T5-style models.
- [NVIDIA Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - Megatron is an efficient, model-parallel (tensor, sequence, and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.
- [NVIDIA Merlin](https://github.com/NVIDIA-Merlin/Merlin) - NVIDIA Merlin is an open source library providing end-to-end GPU-accelerated recommender systems, from feature engineering and preprocessing to training deep learning models and running inference in production.
- [NVIDIA NeMo](https://github.com/NVIDIA/NeMo) - NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP).
- [OpenLLM](https://github.com/bentoml/OpenLLM) - An open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.
- [OSLO](https://github.com/EleutherAI/oslo) - OSLO is a framework that provides various GPU based optimization technologies for large-scale modeling. Features like 3D parallelism and kernel fusion which could be useful when training a large model are the key features.
- [RWKV](https://github.com/BlinkDL/RWKV-LM) - RWKV is a RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer: great performance, fast inference, saves VRAM, fast training, "infinite" ctx_len, and free sentence embedding.
- [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers) - This library is based on the Transformers library by HuggingFace. Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model.
- [small-text](https://github.com/webis-de/small-text) - Small-Text provides state-of-the-art Active Learning for Text Classification.
- [Sockeye](https://github.com/awslabs/sockeye) - Sockeye is an open-source sequence-to-sequence framework for Neural Machine Translation built on PyTorch.
- [SparseZoo](https://github.com/neuralmagic/sparsezoo) - Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes.
- [SpikeGPT](https://github.com/ridgerchu/SpikeGPT) - SpikeGPT is a lightweight generative language model with pure binary, event-driven spiking activation units.
- [StableLM](https://github.com/stability-AI/stableLM) - StableLM: Stability AI Language Models
- [t5x](https://github.com/google-research/t5x) - T5X is a modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models (starting with language) at many scales.
- [tasknet](https://github.com/sileod/tasknet) - Integration of HuggingFace Datasets with HuggingFace Trainer, and multitasking.
- [TorchScale](https://github.com/microsoft/torchscale) - TorchScale is a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively. It has the implementation of fundamental research to improve modeling generality and capability, as well as training stability and efficiency of scaling Transformers.
- [Trankit](https://github.com/nlp-uoregon/trankit) - A Light-Weight Transformer-based Python Toolkit for Multilingual Natural Language Processing
- [transformers](https://github.com/huggingface/transformers) - State-of-the-art Natural Language Processing for PyTorch and TensorFlow 2.0
- [txtai](https://github.com/neuml/txtai) - txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.
- [unilm](https://github.com/microsoft/unilm) - Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities.
- [voltaML](https://github.com/VoltaML/voltaML) - VoltaML is a lightweight library to convert and run your ML/DL deep learning models in high performance inference runtimes like TensorRT, TorchScript, ONNX and TVM.
- [x-transformers](https://github.com/lucidrains/x-transformers) - A simple but complete full-attention transformer with a set of promising experimental features from various papers.
- [xFormers](https://github.com/facebookresearch/xformers) - xFormers is a modular and field agnostic library to flexibly generate transformer architectures by interoperable and optimized building blocks.

# Utils links

- [AITemplate](https://github.com/facebookincubator/AITemplate) - AITemplate (AIT) is a Python framework that transforms deep neural networks into CUDA (NVIDIA GPU) / HIP (AMD GPU) C++ code for lightning-fast inference serving.
- [Alpaca-LoRA](https://github.com/tloen/alpaca-lora) - This repository contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA).
- [Ambrosia](https://github.com/reactorsh/ambrosia) - Ambrosia is a cross-platform command line tool for improving the text datasets you use for machine learning.
- [Aviary](https://github.com/ray-project/aviary) - Aviary is an app that lets you interact with a variety of large language models (LLMs) in a single place. You can compare the outputs of different models directly, rank them by quality, get a cost and latency estimate, and more.
- [bagua](https://github.com/BaguaSys/bagua) - Bagua is a deep learning training acceleration framework for PyTorch
- [BiGS](https://github.com/jxiw/BiGS) - This repository contains BiGS's jax model definitions, pretrained models weights, training and fine-tuning code for our paper exploring using state space models for pretraining.
- [bricks](https://github.com/code-kern-ai/bricks) - Open-source natural language enrichments at your fingertips.
- [ChainForge](https://github.com/ianarawjo/ChainForge) - ChainForge is a data flow prompt engineering environment for analyzing and evaluating LLM responses. It is geared towards early-stage, quick-and-dirty exploration of prompts and response quality that goes beyond ad-hoc chatting with individual LLMs.
- [cleanlab](https://github.com/cleanlab/cleanlab) - cleanlab automatically finds and fixes errors in any ML dataset
- [Cramming Language Model (Pretraining)](https://github.com/JonasGeiping/cramming) - Cramming the training of a (BERT-type) language model into limited compute. Cramming: Training a Language Model on a Single GPU in One Day.
- [CRITIC](https://github.com/microsoft/ProphetNet/tree/master/CRITIC) - CRITIC empowers LLMs to validate and rectify themselves through interaction with external tools.
- [CTranslate2](https://github.com/OpenNMT/CTranslate2) - CTranslate2 is a C++ and Python library for efficient inference with Transformer models.
- [cyclemoid-pytorch](https://github.com/rasbt/cyclemoid-pytorch) - This is an implementation of the cyclemoid activation function for PyTorch.
- [dalai](https://github.com/cocktailpeanut/dalai) - Run LLaMA and Alpaca on your computer.
- [datasketch](https://github.com/ekzhu/datasketch) - datasketch gives you probabilistic data structures that can process and search very large amount of data super fast, with little loss of accuracy. MinHash, LSH, LSH Forest, Weighted MinHash, HyperLogLog, HyperLogLog++, LSH Ensemble.
- [DeepSparse](https://github.com/neuralmagic/deepsparse) - Inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application.
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.
- [DGL](https://github.com/dmlc/dgl) - DGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs. DGL is framework agnostic, meaning if a deep graph model is a component of an end-to-end application, the rest of the logics can be implemented in any major frameworks, such as PyTorch, Apache MXNet or TensorFlow.
- [Dolly](https://github.com/databrickslabs/dolly) - Databricks’ Dolly is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use.
- [embedchain](https://github.com/embedchain/embedchain) - embedchain is a framework to easily create LLM powered bots over any dataset.
- [evals](https://github.com/openai/evals) - Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.
- [ExLlama](https://github.com/turboderp/exllama) - A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.
- [Explainpaper](https://www.explainpaper.com) - Upload a paper, highlight confusing text, get an explanation.
- [FastChat](https://github.com/lm-sys/FastChat) - An open platform for training, serving, and evaluating large language model based chatbots.
- [FastChat](https://github.com/lm-sys/FastChat) - FastChat is an open platform for training, serving, and evaluating large language model based chatbots.
- [FastDeploy](https://github.com/PaddlePaddle/FastDeploy) - Easy-to-use and Fast Deep Learning Model Deployment Toolkit for Cloud Mobile and Edge. Including Image, Video, Text and Audio 20+ main stream scenarios and 150+ SOTA models with end-to-end optimization, multi-platform and multi-framework support.
- [FLAML](https://github.com/microsoft/FLAML) - A Fast Library for Automated Machine Learning & Tuning.
- [FlashAttention](https://github.com/HazyResearch/flash-attention) - Fast and memory-efficient exact attention.
- [FlexGen](https://github.com/FMInference/FlexGen) - FlexGen is a high-throughput generation engine for running large language models with limited GPU memory.
- [FlexGen](https://github.com/FMInference/FlexGen) - FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. Running large language models like OPT-175B/GPT-3 on a single GPU. Up to 100x faster than other offloading systems.
- [ggml](https://github.com/ggerganov/ggml) - Tensor library for machine learning. With ggml you can efficiently run GPT-2 and GPT-J inference on the CPU.
- [Gibberish Detector](https://github.com/domanchi/gibberish-detector) - Train a model, and detect gibberish strings with it. Example gibberish: "ertrjiloifdfyyoiu".
- [GPT4All](https://github.com/nomic-ai/gpt4all) - gpt4all: a chatbot trained on a massive collection of clean assistant data including code, stories and dialogue.
- [Guardrails](https://github.com/ShreyaR/guardrails) - Guardrails is an open-source Python package for specifying structure and type, validating and correcting the outputs of large language models (LLMs).
- [hlb-GPT](https://github.com/tysam-code/hlb-gpt) - Minimalistic, fast, and experimentation-friendly researcher's toolbench for GPT-like models in <350 lines of code. Reaches <3.8 validation loss on wikitext-103 on a single A100 in just over 3 minutes.
- [Horovod](https://github.com/horovod/horovod) - Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet
- [incdbscan](https://github.com/DataOmbudsman/incdbscan) - the incremental version of the DBSCAN clustering algorithm
- [Lamini](https://github.com/lamini-ai/lamini) - Official repo for Lamini's data generator for generating instructions to train instruction-following LLMs.
- [Lance](https://github.com/lancedb/lance) - Lance is a modern columnar data format that is optimized for ML workflows and datasets. Convert from parquet in 2-lines of code for 100x faster random access, a vector index, data versioning, and more. Compatible with pandas, duckdb, polars, pyarrow, with more integrations on the way.
- [LangChain](https://github.com/hwchase17/langchain) - Building applications with LLMs through composability.
- [Language Models](https://github.com/jncraton/languagemodels) - Python building blocks to explore large language models on any computer with 512MB of RAM.
- [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama) - Independent implementation of LLaMA that is fully open source under the Apache 2.0 license.
- [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter) - LLaMA-Adapter is a lightweight adaption method for fine-tuning instruction-following LLaMA models, using 52K data provided by Stanford Alpaca.
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Inference of LLaMA model in pure C/C++.
- [LlamaIndex](https://github.com/jerryjliu/llama_index) - LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.
- [llm](https://github.com/rustformers/llm) - llm is a Rust ecosystem of libraries for running inference on large language models, inspired by llama.cpp.
- [LLMParser](https://github.com/kyang6/llmparser) - LLMParser is a simple and flexible tool to classify and extract structured data from text with large language models.
- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) - A framework for few-shot evaluation of autoregressive language models.
- [Lovely Tensors](https://github.com/xl0/lovely-tensors) - Tensors, ready for human consumption.
- [Macaw-LLM](https://github.com/lyuchenyang/Macaw-LLM) - Macaw-LLM is an exploratory endeavor that pioneers multi-modal language modeling by seamlessly combining image, video, audio, and text data, built upon the foundations of CLIP, Whisper, and LLaMA.
- [Marvin](https://github.com/PrefectHQ/marvin) - Meet Marvin: a batteries-included library for building AI-powered software. Marvin's job is to integrate AI directly into your codebase by making it look and feel like any other function.
- [Merlin Dataloader](https://github.com/NVIDIA-Merlin/dataloader) - The merlin dataloader lets you rapidly load tabular data for training deep leaning models with TensorFlow, PyTorch or JAX
- [Mini-Chain](https://github.com/srush/minichain) - A tiny library for large language models.
- [MiniLLM](https://github.com/kuleshov/minillm) - MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUs.
- [MLC LLM](https://github.com/mlc-ai/mlc-llm) - Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.
- [ModelScope](https://github.com/modelscope/modelscope) - ModelScope is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform model inference, training and evaluation.
- [Mosaic streaming](https://github.com/mosaicml/streaming) - Fast, accurate streaming of training data from cloud storage.
- [MosaicML Examples](https://github.com/mosaicml/examples) - This repo contains reference examples for training ML models quickly and to high accuracy. MosaicBERT
- [Multimodal-GPT](https://github.com/open-mmlab/Multimodal-GPT) - Train a multi-modal chatbot with visual and language instructions!
- [mup](https://github.com/microsoft/mup) - Maximal Update Parametrization (μP) and Hyperparameter Transfer (μTransfer): Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer
- [nebullvm](https://github.com/nebuly-ai/nebullvm) - Nebullvm is an ecosystem of plug and play modules to optimize the performances of your AI systems. The optimization modules are stack-agnostic and work with any library. They are designed to be easily integrated into your system, providing a quick and seamless boost to its performance. Simply plug and play to start realizing the benefits of optimized performance right away.
- [Opacus](https://github.com/pytorch/opacus) - Opacus is a library that enables training PyTorch models with differential privacy. It supports training with minimal code changes required on the client, has little impact on training performance, and allows the client to online track the privacy budget expended at any given moment.
- [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) - OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories.
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo) - An open-source framework for training large multimodal models.
- [OpenLM](https://github.com/r2d4/openlm) - Drop-in OpenAI-compatible library that can call LLMs from other providers (e.g., HuggingFace, Cohere, and more).
- [openplayground](https://github.com/nat/openplayground) - An LLM playground you can run on your laptop.
- [Otter](https://github.com/Luodian/otter) - Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning.
- [PaLM](https://github.com/conceptofmind/PaLM) - An open-source implementation of Google's PaLM models.
- [PaLM + RLHF](https://github.com/lucidrains/PaLM-rlhf-pytorch) - Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM.
- [PEFT](https://github.com/huggingface/peft) - Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters.
- [privateGPT](https://github.com/imartinez/privateGPT) - Interact privately with your documents using the power of GPT, 100% privately, no data leaks.
- [pybaselines](https://github.com/derb12/pybaselines) - A Python library of algorithms for the baseline correction of experimental data.
- [PyTorch Geometric](https://github.com/pyg-team/pytorch_geometric) - PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.
- [RasaGPT](https://github.com/paulpierre/RasaGPT) - RasaGPT is the first headless LLM chatbot platform built on top of Rasa and Langchain. Built w/ Rasa, FastAPI, Langchain, LlamaIndex, SQLModel, pgvector, ngrok, telegram.
- [safari](https://github.com/HazyResearch/safari) - Convolutions for Sequence Modeling, contains: Hyena, Long Convs, Hungry Hungry Hippos (H3).
- [Semantic Kernel](https://github.com/microsoft/semantic-kernel) - Semantic Kernel (SK) is a lightweight SDK enabling integration of AI Large Language Models (LLMs) with conventional programming languages.
- [skorch](https://github.com/skorch-dev/skorch) - A scikit-learn compatible neural network library that wraps PyTorch.
- [SparseML](https://github.com/neuralmagic/sparseml) - Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models.
- [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) - This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model.
- [StarCoder](https://github.com/bigcode-project/starcoder) - Home of StarCoder: fine-tuning & inference!
- [state-spaces](https://github.com/HazyResearch/state-spaces) - Sequence Modeling with Structured State Spaces, contains: S4D, HTTYH, SaShiMi, S4, LSSL, HiPPO.
- [string2string](https://github.com/stanfordnlp/string2string) - The string2string library is an open-source tool that offers a comprehensive suite of efficient algorithms for a broad range of string-to-string problems. It includes both traditional algorithmic solutions and recent advanced neural approaches to address various problems in pairwise string alignment, distance measurement, lexical and semantic search, and similarity analysis. Additionally, the library provides several helpful visualization tools and metrics to facilitate the interpretation and analysis of these methods.
- [surgeon-pytorch](https://github.com/archinetai/surgeon-pytorch) - A library to inspect and extract intermediate layers of PyTorch models.
- [tbparse](https://github.com/j3soon/tbparse) - A simple yet powerful tensorboard event log parser/reader.
- [TextBox](https://github.com/RUCAIBox/TextBox) - TextBox 2.0 is a text generation library with pre-trained language models.
- [TextReducer](https://github.com/helliun/targetedSummarization) - TextReducer is a tool for summarization and information extraction powered by the SentenceTransformer library.
- [tinygrad](https://github.com/geohot/tinygrad) - tinygrad is an extremely simple deep-learning framework. It supports both inference and training.
- [TorchBench](https://github.com/pytorch/benchmark) - TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.
- [torchdistX](https://github.com/pytorch/torchdistx) - Torch Distributed Experimental
- [torchegranate](https://github.com/jmschrei/torchegranate) - Fast, flexible and easy to use probabilistic modelling with PyTorch.
- [Training BERT with Compute/Time (Academic) Budget](https://github.com/IntelLabs/academic-budget-bert) - Repository contains scripts for pre-training and finetuning BERT-like models with limited time and compute budget.
- [transformer-deploy](https://github.com/ELS-RD/transformer-deploy) - Efficient, scalable and enterprise-grade CPU/GPU inference server for Hugging Face transformer models.
- [tsai](https://github.com/timeseriesAI/tsai) - tsai is an open-source deep learning package built on top of Pytorch & fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation.
- [UpTrain](https://github.com/uptrain-ai/uptrain) - UpTrain is an open-source, data-secure tool for ML practitioners to observe and refine their ML models by monitoring their performance, checking for (data) distribution shifts, and collecting edge cases to retrain them upon.
- [vLLM](https://github.com/vllm-project/vllm) - vLLM is a fast and easy-to-use library for LLM inference and serving.
- [WeightWatcher](https://github.com/CalculatedContent/WeightWatcher) - WeightWatcher (WW): is an open-source, diagnostic tool for analyzing Deep Neural Networks (DNN), without needing access to training or even test data.
- [WizardLM](https://github.com/nlpxucan/WizardLM) - WizardLM: An Instruction-following LLM Using Evol-Instruct. At present, our core contributors are preparing the 65B version and we expect to empower WizardLM with the ability to perform instruction evolution itself, aiming to evolve your specific data at a low cost.
- [xturing](https://github.com/stochasticai/xturing) - xturing provides fast, efficient and simple fine-tuning of LLMs, such as LLaMA, GPT-J, GPT-2, OPT, Cerebras-GPT, Galactica, and more.

# GPU profiling links

- [NVIDIA Nsight Systems](https://developer.nvidia.com/nsight-systems) - NVIDIA Nsight™ Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest system on a chip (SoC).
- [NVIDIA Visual Profiler](https://developer.nvidia.com/nvidia-visual-profiler) - The NVIDIA Visual Profiler is a cross-platform performance profiling tool that delivers developers vital feedback for optimizing CUDA C/C++ applications.
- [NVIDIA/NVTX](https://github.com/NVIDIA/NVTX) - The NVIDIA® Tools Extension SDK (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications.
- [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) - This recipe explains how to use PyTorch profiler and measure the time and memory consumption of the model’s operators.
- [torchnnprofiler](https://github.com/kshitij12345/torchnnprofiler) - Context Manager to profile the forward and backward times of PyTorch's nn.Module

# Visualizations links

- [BertViz](https://github.com/jessevig/bertviz) - BertViz is a tool for visualizing attention in the Transformer model, supporting most models from the transformers library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, BART, etc.)
- [Ecoo](https://github.com/jalammar/ecco) - Ecco is a python library for exploring and explaining Natural Language Processing models using interactive visualizations.

# Vocabulary links

- [SentencePiece](https://github.com/google/sentencepiece) - SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) and unigram language model ) with the extension of direct training from raw sentences.
- [tiktoken](https://github.com/openai/tiktoken) - tiktoken is a fast BPE tokeniser for use with OpenAI's models.
- [tokenizers](https://github.com/huggingface/tokenizers) - Provides an implementation of today's most used tokenizers, with a focus on performance and versatility.

# Optimizers links

- [LOMO](https://arxiv.org/abs/2306.09782) - LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage.
- [Sophia](https://arxiv.org/abs/2305.14342) - Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training

# Dataset links

- [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval) - AlpacaEval : An Automatic Evaluator for Instruction-following Language Models.
- [BIG-bench](https://github.com/google/BIG-bench) - The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. The more than 200 tasks included in BIG-bench.
- [CCMatrix](https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix) - CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB.
- [CCNet](https://github.com/facebookresearch/cc_net) - Tools to download and clean Common Crawl as introduced in our paper CCNet: High Quality Monolingual Datasets from Web Crawl Data.
- [Deduplicated CommonCrawl Text](http://statmt.org/ngrams/deduped) - Processed Common Crawl snapshots.
- [DS-1000](https://github.com/HKUNLP/DS-1000) - Official data and code release for the paper DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.
- [FewGLUE](https://github.com/timoschick/fewglue) - FewGLUE dataset, consisting of a random selection of 32 training examples from the SuperGLUE training sets and up to 20,000 unlabeled examples for each SuperGLUE task.
- [Flan v2](https://github.com/google-research/FLAN/tree/main/flan/v2) - Flan 2022 collection of datasets and templates.
- [GLGE](https://github.com/microsoft/glge) - This repository contains information about the general langugae generation evaluation benchmark GLGE, which is composed of 8 language generation tasks, including Abstractive Text Summarization (CNN/DailyMail, Gigaword, XSUM, MSNews), Answer-aware Question Generation (SQuAD 1.1, MSQG), Conversational Question Answering (CoQA), and Personalizing Dialogue (Personachat).
- [HumanEval-X](https://huggingface.co/datasets/THUDM/humaneval-x) - HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation.
- [KILT](https://github.com/facebookresearch/KILT) - A Benchmark for Knowledge Intensive Language Tasks.
- [LIMA](https://huggingface.co/datasets/GAIR/lima) - Dataset for LIMA: Less Is More for Alignment.
- [MASSIVE](https://github.com/alexa/massive) - MASSIVE is a parallel dataset of > 1M utterances across 52 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation.
- [MultiLegalPile](https://huggingface.co/datasets/joelito/Multi_Legal_Pile) - The Multi_Legal_Pile is a large-scale multilingual legal dataset suited for pretraining language models. It spans over 24 languages and five legal text types.
- [OSCAR (Open Super-large Crawled ALMAnaCH coRpus)](https://oscar-corpus.com) - OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the Ungoliant architecture.
- [P3](https://huggingface.co/datasets/bigscience/P3) - P3 (Public Pool of Prompts) is a collection of prompted English datasets covering a diverse set of NLP tasks.
- [PG-19 Language Modelling Benchmark](https://github.com/deepmind/pg19) - This repository contains the PG-19 language modeling benchmark. It includes a set of books extracted from the Project Gutenberg books library, that were published before 1919. It also contains metadata of book titles and publication dates.
- [PRESTO](https://github.com/google-research-datasets/presto) - PRESTO is a dataset of over 550K contextual multilingual conversations between humans and virtual assistants. PRESTO contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions.
- [ReCoRD](https://github.com/deepmind/rc-data) - This repository contains a script to generate question/answer pairs using CNN and Daily Mail articles downloaded from the Wayback Machine.
- [RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data) - RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset. This repo contains a reproducible data receipe for the RedPajama data.
- [Seahorse](https://github.com/google-research-datasets/seahorse) - Seahorse is a dataset for multilingual, multifaceted summarization evaluation. It contains 96K summaries with human ratings along 6 quality dimensions: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4 datasets.
- [Self-Instruct](https://huggingface.co/datasets/yizhongw/self_instruct) - Self-Instruct is a framework that helps language models improve their ability to follow natural language instructions.
- [SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B) - The dataset consists of 59166 jsonl files and is ~895GB compressed. It is a cleaned and deduplicated version of Together's RedPajama.
- [WikiMatrix](https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix) - WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia.
- [xP3](https://huggingface.co/datasets/bigscience/xP3) - xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks.

# Documentation links

- [Graphic of learning rate schedulers](https://raw.githubusercontent.com/rasbt/machine-learning-notes/7abac1b3dfe47b84887fcee80e5cca0e7ebf5061/learning-rates/scheduler-comparison/overview.png) - Simple visualizations of learning rate schedulers.
- [ML Papers Explained](https://github.com/dair-ai/ML-Papers-Explained) - List of LM papers explained.
- [Open LLMs](https://github.com/eugeneyan/open-llms) - A list of open LLMs available for commercial use.
- [The Practical Guides for Large Language Models](https://github.com/Mooler0410/LLMsPracticalGuide) - A curated (still actively updated) list of practical guide resources of LLMs.
- [Transformer](https://nn.labml.ai/transformers/index.html) - Explanation of Transformer architecture from the code.
- [Transformer models: an introduction and catalog, 2023 Edition](https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376) - An introduction and catalog of Transformer models.
- [Transformers from Scratch](https://e2eml.school/transformers.html) - Explanation of Transformer architecture.
- [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html) - Understanding how self-attention works from scratch.
