[
    {
        "name": "AgentTuning",
        "description": "AgentTuning represents the very first attempt to instruction-tune LLMs using interaction trajectories across multiple agent tasks.",
        "url": "https://github.com/THUDM/AgentTuning",
        "type_name": "utils_links"
    },
    {
        "name": "AITemplate",
        "description": "AITemplate (AIT) is a Python framework that transforms deep neural networks into CUDA (NVIDIA GPU) / HIP (AMD GPU) C++ code for lightning-fast inference serving.",
        "url": "https://github.com/facebookincubator/AITemplate",
        "type_name": "utils_links"
    },
    {
        "name": "Alpaca-LoRA",
        "description": "This repository contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA).",
        "url": "https://github.com/tloen/alpaca-lora",
        "type_name": "utils_links"
    },
    {
        "name": "AlpacaEval",
        "description": "AlpacaEval : An Automatic Evaluator for Instruction-following Language Models.",
        "url": "https://github.com/tatsu-lab/alpaca_eval",
        "type_name": "dataset_links"
    },
    {
        "name": "AlpacaFarm",
        "description": "AlpacaFarm is a simulator that enables research and development on learning from feedback at a fraction of the usual cost, promoting accessible research on instruction following and alignment.",
        "url": "https://github.com/tatsu-lab/alpaca_farm",
        "type_name": "utils_links"
    },
    {
        "name": "Ambrosia",
        "description": "Ambrosia is a cross-platform command line tool for improving the text datasets you use for machine learning.",
        "url": "https://github.com/reactorsh/ambrosia",
        "type_name": "utils_links"
    },
    {
        "name": "AutoChain",
        "description": "AutoChain: Build lightweight, extensible, and testable LLM Agents",
        "url": "https://github.com/Forethought-Technologies/AutoChain",
        "type_name": "utils_links"
    },
    {
        "name": "AutoGen",
        "description": "AutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation.",
        "url": "https://github.com/microsoft/autogen",
        "type_name": "utils_links"
    },
    {
        "name": "Aviary",
        "description": "Aviary is an app that lets you interact with a variety of large language models (LLMs) in a single place. You can compare the outputs of different models directly, rank them by quality, get a cost and latency estimate, and more.",
        "url": "https://github.com/ray-project/aviary",
        "type_name": "utils_links"
    },
    {
        "name": "bagua",
        "description": "Bagua is a deep learning training acceleration framework for PyTorch",
        "url": "https://github.com/BaguaSys/bagua",
        "type_name": "utils_links"
    },
    {
        "name": "Batched LoRAs",
        "description": "Maximize GPU util by routing inference through multiple LoRAs in the same batch.",
        "url": "https://github.com/sabetAI/BLoRA",
        "type_name": "utils_links"
    },
    {
        "name": "BayLing",
        "description": "BayLing is an instruction-following large language model equipped with advanced language alignment, showing superior capability in English/Chinese generation, instruction following and multi-turn interaction.",
        "url": "https://github.com/ictnlp/BayLing",
        "type_name": "utils_links"
    },
    {
        "name": "BertViz",
        "description": "BertViz is a tool for visualizing attention in the Transformer model, supporting most models from the transformers library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, BART, etc.)",
        "url": "https://github.com/jessevig/bertviz",
        "type_name": "visualization_links"
    },
    {
        "name": "BIG-bench",
        "description": "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. The more than 200 tasks included in BIG-bench.",
        "url": "https://github.com/google/BIG-bench",
        "type_name": "dataset_links"
    },
    {
        "name": "BiGS",
        "description": "This repository contains BiGS's jax model definitions, pretrained models weights, training and fine-tuning code for our paper exploring using state space models for pretraining.",
        "url": "https://github.com/jxiw/BiGS",
        "type_name": "utils_links"
    },
    {
        "name": "bricks",
        "description": "Open-source natural language enrichments at your fingertips.",
        "url": "https://github.com/code-kern-ai/bricks",
        "type_name": "utils_links"
    },
    {
        "name": "Burn",
        "description": "Burn - A Flexible and Comprehensive Deep Learning Framework in Rust",
        "url": "https://github.com/burn-rs/burn",
        "type_name": "utils_links"
    },
    {
        "name": "BYOD",
        "description": "A framework for self-supervised model evaluation. In this framework, metrics are defined as invariances and sensitivities that can be checked in a self-supervised fashion using interventions based only on the model in question rather than external labels.",
        "url": "https://github.com/neelsjain/BYOD",
        "type_name": "utils_links"
    },
    {
        "name": "C Transformers",
        "description": "Python bindings for the Transformer models implemented in C/C++ using GGML library.",
        "url": "https://github.com/marella/ctransformers",
        "type_name": "model_links"
    },
    {
        "name": "candle",
        "description": "Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use.",
        "url": "https://github.com/huggingface/candle",
        "type_name": "model_links"
    },
    {
        "name": "CCMatrix",
        "description": "CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB.",
        "url": "https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix",
        "type_name": "dataset_links"
    },
    {
        "name": "CCNet",
        "description": "Tools to download and clean Common Crawl as introduced in our paper CCNet: High Quality Monolingual Datasets from Web Crawl Data.",
        "url": "https://github.com/facebookresearch/cc_net",
        "type_name": "dataset_links"
    },
    {
        "name": "ChainForge",
        "description": "ChainForge is a data flow prompt engineering environment for analyzing and evaluating LLM responses. It is geared towards early-stage, quick-and-dirty exploration of prompts and response quality that goes beyond ad-hoc chatting with individual LLMs.",
        "url": "https://github.com/ianarawjo/ChainForge",
        "type_name": "utils_links"
    },
    {
        "name": "cleanlab",
        "description": "cleanlab automatically finds and fixes errors in any ML dataset",
        "url": "https://github.com/cleanlab/cleanlab",
        "type_name": "utils_links"
    },
    {
        "name": "CodeGeeX",
        "description": "We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages.",
        "url": "https://github.com/THUDM/CodeGeeX",
        "type_name": "model_links"
    },
    {
        "name": "CodeT5 and CodeT5+",
        "description": "Official research release for CodeT5 and CodeT5+ models for Code Understanding and Generation from Salesforce Research/",
        "url": "https://github.com/salesforce/CodeT5",
        "type_name": "model_links"
    },
    {
        "name": "CodeTF",
        "description": "CodeTF is a one-stop Python transformer-based library for code large language models (Code LLMs) and code intelligence, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on.",
        "url": "https://github.com/salesforce/CodeTF",
        "type_name": "model_links"
    },
    {
        "name": "COLLIE",
        "description": "COLLIE framework for easy constraint structure specification, example extraction, instruction rendering, and model evaluation.",
        "url": "https://github.com/princeton-nlp/Collie",
        "type_name": "utils_links"
    },
    {
        "name": "ColossalAI",
        "description": "Colossal-AI: A Unified Deep Learning System for Big Model Era. Making large AI models cheaper, faster and more accessible.",
        "url": "https://github.com/hpcaitech/ColossalAI",
        "type_name": "model_links"
    },
    {
        "name": "comgra",
        "description": "Comgra stands for \"computation graph analysis\" and it is a library for use with pytorch that makes it easier to inspect the internals of your neural networks.",
        "url": "https://github.com/FlorianDietz/comgra",
        "type_name": "utils_links"
    },
    {
        "name": "Cramming Language Model (Pretraining)",
        "description": "Cramming the training of a (BERT-type) language model into limited compute. Cramming: Training a Language Model on a Single GPU in One Day.",
        "url": "https://github.com/JonasGeiping/cramming",
        "type_name": "utils_links"
    },
    {
        "name": "CRITIC",
        "description": "CRITIC empowers LLMs to validate and rectify themselves through interaction with external tools.",
        "url": "https://github.com/microsoft/ProphetNet/tree/master/CRITIC",
        "type_name": "utils_links"
    },
    {
        "name": "CTranslate2",
        "description": "CTranslate2 is a C++ and Python library for efficient inference with Transformer models.",
        "url": "https://github.com/OpenNMT/CTranslate2",
        "type_name": "utils_links"
    },
    {
        "name": "CulturaX",
        "description": "CulturaX is a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for large language model (LLM) development.",
        "url": "https://huggingface.co/datasets/uonlp/CulturaX",
        "type_name": "dataset_links"
    },
    {
        "name": "cyclemoid-pytorch",
        "description": "This is an implementation of the cyclemoid activation function for PyTorch.",
        "url": "https://github.com/rasbt/cyclemoid-pytorch",
        "type_name": "utils_links"
    },
    {
        "name": "Daft",
        "description": "Daft is a fast, Pythonic and scalable open-source dataframe library built for Python and Machine Learning workloads.",
        "url": "https://github.com/Eventual-Inc/Daft",
        "type_name": "utils_links"
    },
    {
        "name": "dalai",
        "description": "Run LLaMA and Alpaca on your computer.",
        "url": "https://github.com/cocktailpeanut/dalai",
        "type_name": "utils_links"
    },
    {
        "name": "DALMs",
        "description": "Domain Adapted Language Modeling Toolkit - E2E RAG",
        "url": "https://github.com/arcee-ai/DALM",
        "type_name": "utils_links"
    },
    {
        "name": "datasketch",
        "description": "datasketch gives you probabilistic data structures that can process and search very large amount of data super fast, with little loss of accuracy. MinHash, LSH, LSH Forest, Weighted MinHash, HyperLogLog, HyperLogLog++, LSH Ensemble.",
        "url": "https://github.com/ekzhu/datasketch",
        "type_name": "utils_links"
    },
    {
        "name": "Deduplicated CommonCrawl Text",
        "description": "Processed Common Crawl snapshots.",
        "url": "http://statmt.org/ngrams/deduped",
        "type_name": "dataset_links"
    },
    {
        "name": "DeepEval",
        "description": "DeepEval provides metrics on different aspects when evaluating an LLM response to ensure that answers are relevant, consistent, unbiased, non-toxic.",
        "url": "https://github.com/confident-ai/deepeval",
        "type_name": "utils_links"
    },
    {
        "name": "DeepSparse",
        "description": "Inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application.",
        "url": "https://github.com/neuralmagic/deepsparse",
        "type_name": "utils_links"
    },
    {
        "name": "DeepSpeed",
        "description": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.",
        "url": "https://github.com/microsoft/DeepSpeed",
        "type_name": "utils_links"
    },
    {
        "name": "DeepSpeed-MII",
        "description": "MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.",
        "url": "https://github.com/microsoft/DeepSpeed-MII",
        "type_name": "model_links"
    },
    {
        "name": "DGL",
        "description": "DGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs. DGL is framework agnostic, meaning if a deep graph model is a component of an end-to-end application, the rest of the logics can be implemented in any major frameworks, such as PyTorch, Apache MXNet or TensorFlow.",
        "url": "https://github.com/dmlc/dgl",
        "type_name": "utils_links"
    },
    {
        "name": "Dolly",
        "description": "Databricks’ Dolly is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use.",
        "url": "https://github.com/databrickslabs/dolly",
        "type_name": "utils_links"
    },
    {
        "name": "Dolma",
        "description": "Dolma is a dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials.",
        "url": "https://huggingface.co/datasets/allenai/dolma",
        "type_name": "dataset_links"
    },
    {
        "name": "DS-1000",
        "description": "Official data and code release for the paper DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.",
        "url": "https://github.com/HKUNLP/DS-1000",
        "type_name": "dataset_links"
    },
    {
        "name": "DSPy",
        "description": "DSPy is the framework for solving advanced tasks with language models (LMs) and retrieval models (RMs). DSPy unifies techniques for prompting and fine-tuning LMs.",
        "url": "https://github.com/stanfordnlp/dspy",
        "type_name": "utils_links"
    },
    {
        "name": "EasyLLM",
        "description": "EasyLLM is an open source project that provides helpful tools and methods for working with large language models (LLMs), both open source and closed source.",
        "url": "https://github.com/philschmid/easyllm",
        "type_name": "model_links"
    },
    {
        "name": "EasyLM",
        "description": "Large language models (LLMs) made easy, EasyLM is a one stop solution for pre-training, finetuning, evaluating and serving LLMs in JAX/Flax. EasyLM can scale up LLM training to hundreds of TPU/GPU accelerators by leveraging JAX's pjit functionality.",
        "url": "https://github.com/young-geng/EasyLM",
        "type_name": "model_links"
    },
    {
        "name": "Ecoo",
        "description": "Ecco is a python library for exploring and explaining Natural Language Processing models using interactive visualizations.",
        "url": "https://github.com/jalammar/ecco",
        "type_name": "visualization_links"
    },
    {
        "name": "embedchain",
        "description": "embedchain is a framework to easily create LLM powered bots over any dataset.",
        "url": "https://github.com/embedchain/embedchain",
        "type_name": "utils_links"
    },
    {
        "name": "evals",
        "description": "Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.",
        "url": "https://github.com/openai/evals",
        "type_name": "utils_links"
    },
    {
        "name": "ExecuTorch",
        "description": "ExecuTorch is an end-to-end solution for enabling on-device inference capabilities across mobile and edge devices including wearables, embedded devices and microcontrollers. It is part of the PyTorch Edge ecosystem and enables efficient deployment of PyTorch models to edge devices.",
        "url": "https://github.com/pytorch/executorch",
        "type_name": "utils_links"
    },
    {
        "name": "ExLlama",
        "description": "A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.",
        "url": "https://github.com/turboderp/exllama",
        "type_name": "utils_links"
    },
    {
        "name": "ExLlamaV2",
        "description": "ExLlamaV2 is a fast inference library for running LLMs locally on modern consumer-class GPUs",
        "url": "https://github.com/turboderp/exllamav2",
        "type_name": "utils_links"
    },
    {
        "name": "Explainpaper",
        "description": "Upload a paper, highlight confusing text, get an explanation.",
        "url": "https://www.explainpaper.com",
        "type_name": "utils_links"
    },
    {
        "name": "ExtremeBERT",
        "description": "ExtremeBERT is a toolkit that accelerates the pretraining of customized language models on customized datasets.",
        "url": "https://github.com/extreme-bert/extreme-bert",
        "type_name": "model_links"
    },
    {
        "name": "FairScale",
        "description": "FairScale is a PyTorch extension library for high performance and large scale training. This library extends basic PyTorch capabilities while adding new SOTA scaling techniques.",
        "url": "https://github.com/facebookresearch/fairscale",
        "type_name": "model_links"
    },
    {
        "name": "Falcon RefinedWeb",
        "description": "Falcon RefinedWeb is a massive English web dataset.",
        "url": "https://huggingface.co/datasets/tiiuae/falcon-refinedweb",
        "type_name": "dataset_links"
    },
    {
        "name": "FastChat",
        "description": "FastChat is an open platform for training, serving, and evaluating large language model based chatbots.",
        "url": "https://github.com/lm-sys/FastChat",
        "type_name": "utils_links"
    },
    {
        "name": "FastDeploy",
        "description": "Easy-to-use and Fast Deep Learning Model Deployment Toolkit for Cloud Mobile and Edge. Including Image, Video, Text and Audio 20+ main stream scenarios and 150+ SOTA models with end-to-end optimization, multi-platform and multi-framework support.",
        "url": "https://github.com/PaddlePaddle/FastDeploy",
        "type_name": "utils_links"
    },
    {
        "name": "FasterTransformer",
        "description": "This repository provides a script and recipe to run the highly optimized transformer-based encoder and decoder component, and it is tested and maintained by NVIDIA.",
        "url": "https://github.com/NVIDIA/FasterTransformer",
        "type_name": "model_links"
    },
    {
        "name": "FastSeq",
        "description": "FastSeq provides efficient implementation of popular sequence models (e.g. Bart, ProphetNet) for text generation, summarization, translation tasks etc.",
        "url": "https://github.com/microsoft/fastseq",
        "type_name": "model_links"
    },
    {
        "name": "fastText",
        "description": "fastText is a library for efficient learning of word representations and sentence classification.",
        "url": "https://github.com/facebookresearch/fastText/",
        "type_name": "utils_links"
    },
    {
        "name": "FewGLUE",
        "description": "FewGLUE dataset, consisting of a random selection of 32 training examples from the SuperGLUE training sets and up to 20,000 unlabeled examples for each SuperGLUE task.",
        "url": "https://github.com/timoschick/fewglue",
        "type_name": "dataset_links"
    },
    {
        "name": "FlagAI",
        "description": "FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.",
        "url": "https://github.com/FlagAI-Open/FlagAI",
        "type_name": "model_links"
    },
    {
        "name": "FLAML",
        "description": "A Fast Library for Automated Machine Learning & Tuning.",
        "url": "https://github.com/microsoft/FLAML",
        "type_name": "utils_links"
    },
    {
        "name": "Flan v2",
        "description": "Flan 2022 collection of datasets and templates.",
        "url": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "type_name": "dataset_links"
    },
    {
        "name": "FlashAttention",
        "description": "Fast and memory-efficient exact attention.",
        "url": "https://github.com/HazyResearch/flash-attention",
        "type_name": "utils_links"
    },
    {
        "name": "FlexGen",
        "description": "FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows high-throughput generation by IO-efficient offloading, compression, and large effective batch sizes.",
        "url": "https://github.com/FMInference/FlexGen",
        "type_name": "utils_links"
    },
    {
        "name": "GENIUS",
        "description": "GENIUS generating text using sketches! A strong and general textual data augmentation tool.",
        "url": "https://github.com/beyondguo/geniushttps://github.com/beyondguo/genius",
        "type_name": "model_links"
    },
    {
        "name": "GeoV",
        "description": "The GeoV model is a large langauge model designed by Georges Harik and uses Rotary Positional Embeddings with Relative distances (RoPER). We have shared a pre-trained 9B parameter model.",
        "url": "https://github.com/geov-ai/geov",
        "type_name": "model_links"
    },
    {
        "name": "ggml",
        "description": "Tensor library for machine learning. With ggml you can efficiently run GPT-2 and GPT-J inference on the CPU.",
        "url": "https://github.com/ggerganov/ggml",
        "type_name": "utils_links"
    },
    {
        "name": "Gibberish Detector",
        "description": "Train a model, and detect gibberish strings with it. Example gibberish: \"ertrjiloifdfyyoiu\".",
        "url": "https://github.com/domanchi/gibberish-detector",
        "type_name": "utils_links"
    },
    {
        "name": "Glaive-code-assistant",
        "description": "Glaive-code-assistant is a dataset of ~140k code problems and solutions generated using Glaive’s synthetic data generation platform.",
        "url": "https://huggingface.co/datasets/glaiveai/glaive-code-assistant",
        "type_name": "dataset_links"
    },
    {
        "name": "GLGE",
        "description": "This repository contains information about the general langugae generation evaluation benchmark GLGE, which is composed of 8 language generation tasks, including Abstractive Text Summarization (CNN/DailyMail, Gigaword, XSUM, MSNews), Answer-aware Question Generation (SQuAD 1.1, MSQG), Conversational Question Answering (CoQA), and Personalizing Dialogue (Personachat).",
        "url": "https://github.com/microsoft/glge",
        "type_name": "dataset_links"
    },
    {
        "name": "GlotLID",
        "description": "GlotLID is an open-source language identification model with support for more than 1600 languages.",
        "url": "https://github.com/cisnlp/GlotLID",
        "type_name": "utils_links"
    },
    {
        "name": "GPT4All",
        "description": "gpt4all: a chatbot trained on a massive collection of clean assistant data including code, stories and dialogue.",
        "url": "https://github.com/nomic-ai/gpt4all",
        "type_name": "utils_links"
    },
    {
        "name": "Graphic of learning rate schedulers",
        "description": "Simple visualizations of learning rate schedulers.",
        "url": "https://raw.githubusercontent.com/rasbt/machine-learning-notes/7abac1b3dfe47b84887fcee80e5cca0e7ebf5061/learning-rates/scheduler-comparison/overview.png",
        "type_name": "documentation_links"
    },
    {
        "name": "Guardrails",
        "description": "Guardrails is an open-source Python package for specifying structure and type, validating and correcting the outputs of large language models (LLMs).",
        "url": "https://github.com/ShreyaR/guardrails",
        "type_name": "utils_links"
    },
    {
        "name": "H2O LLM Studio",
        "description": "H2O LLM Studio, a framework and no-code GUI designed for fine-tuning state-of-the-art large language models (LLMs).",
        "url": "https://github.com/h2oai/h2o-llmstudio",
        "type_name": "model_links"
    },
    {
        "name": "h2oGPT",
        "description": "h2oGPT - The world's best open source GPT: open-source repository with fully permissive, commercially usable code, data and models and code for fine-tuning large language models.",
        "url": "https://github.com/h2oai/h2ogpt",
        "type_name": "model_links"
    },
    {
        "name": "Haystack",
        "description": "Haystack is an end-to-end NLP framework that enables you to build applications powered by LLMs, Transformer models, vector search and more.",
        "url": "https://github.com/deepset-ai/haystack",
        "type_name": "utils_links"
    },
    {
        "name": "hlb-GPT",
        "description": "Minimalistic, fast, and experimentation-friendly researcher's toolbench for GPT-like models in <350 lines of code. Reaches <3.8 validation loss on wikitext-103 on a single A100 in just over 3 minutes.",
        "url": "https://github.com/tysam-code/hlb-gpt",
        "type_name": "utils_links"
    },
    {
        "name": "Horovod",
        "description": "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet",
        "url": "https://github.com/horovod/horovod",
        "type_name": "utils_links"
    },
    {
        "name": "HumanEval-X",
        "description": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation.",
        "url": "https://huggingface.co/datasets/THUDM/humaneval-x",
        "type_name": "dataset_links"
    },
    {
        "name": "incdbscan",
        "description": "the incremental version of the DBSCAN clustering algorithm",
        "url": "https://github.com/DataOmbudsman/incdbscan",
        "type_name": "utils_links"
    },
    {
        "name": "InstructEvalImpact",
        "description": "The IMPACT dataset contains 50 human created prompts for each category, 200 in total, to test LLMs general writing ability.",
        "url": "https://huggingface.co/datasets/declare-lab/InstructEvalImpact",
        "type_name": "dataset_links"
    },
    {
        "name": "InternLM",
        "description": "InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios.",
        "url": "https://github.com/InternLM/InternLM",
        "type_name": "utils_links"
    },
    {
        "name": "JARVIS",
        "description": "JARVIS, a system to connect LLMs with ML community.",
        "url": "https://github.com/microsoft/JARVIS",
        "type_name": "model_links"
    },
    {
        "name": "Kernl",
        "description": "Kernl lets you run Pytorch transformer models several times faster on GPU with a single line of code, and is designed to be easily hackable. Kernl is the first OSS inference engine written in OpenAI Triton, a new language designed by OpenAI to make it easier to write GPU kernels.",
        "url": "https://github.com/ELS-RD/kernl",
        "type_name": "model_links"
    },
    {
        "name": "KILT",
        "description": "A Benchmark for Knowledge Intensive Language Tasks.",
        "url": "https://github.com/facebookresearch/KILT",
        "type_name": "dataset_links"
    },
    {
        "name": "L-Eval",
        "description": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models.",
        "url": "https://github.com/openlmlab/leval",
        "type_name": "utils_links"
    },
    {
        "name": "labml.ai Deep Learning Paper Implementations",
        "description": "60 Implementations/tutorials of deep learning papers with side-by-side notes.",
        "url": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
        "type_name": "documentation_links"
    },
    {
        "name": "Lamini",
        "description": "Official repo for Lamini's data generator for generating instructions to train instruction-following LLMs.",
        "url": "https://github.com/lamini-ai/lamini",
        "type_name": "utils_links"
    },
    {
        "name": "Lance",
        "description": "Lance is a modern columnar data format that is optimized for ML workflows and datasets. Convert from parquet in 2-lines of code for 100x faster random access, a vector index, data versioning, and more. Compatible with pandas, duckdb, polars, pyarrow, with more integrations on the way.",
        "url": "https://github.com/lancedb/lance",
        "type_name": "utils_links"
    },
    {
        "name": "LangChain",
        "description": "Building applications with LLMs through composability.",
        "url": "https://github.com/hwchase17/langchain",
        "type_name": "utils_links"
    },
    {
        "name": "Language Models",
        "description": "Python building blocks to explore large language models on any computer with 512MB of RAM.",
        "url": "https://github.com/jncraton/languagemodels",
        "type_name": "utils_links"
    },
    {
        "name": "LASSL",
        "description": "LASSL is a LAnguage framework for Self-Supervised Learning. LASSL aims to provide an easy-to-use framework for pretraining language model by only using Huggingface's Transformers and Datasets.",
        "url": "https://github.com/lassl/lassl",
        "type_name": "model_links"
    },
    {
        "name": "Latent Diffusion Models",
        "description": "High-Resolution Image Synthesis with Latent Diffusion Models, contains: Text-to-Image, Inpainting",
        "url": "https://github.com/CompVis/latent-diffusion",
        "type_name": "model_links"
    },
    {
        "name": "Levanter",
        "description": "Levanter is a framework for training large language models (LLMs) and other foundation models that strives for legibility, scalability, and reproducibility.",
        "url": "https://github.com/stanford-crfm/levanter",
        "type_name": "model_links"
    },
    {
        "name": "LiBai",
        "description": "LiBai is a large-scale open-source model training toolbox based on OneFlow",
        "url": "https://github.com/Oneflow-Inc/libai",
        "type_name": "model_links"
    },
    {
        "name": "LightSeq",
        "description": "Official repository for LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. LightSeq achieves up to 2x faster, 2-8x longer sequences vs Megatron-LM on 16 80GB A100s.",
        "url": "https://github.com/RulinShao/LightSeq",
        "type_name": "model_links"
    },
    {
        "name": "LIMA",
        "description": "Dataset for LIMA: Less Is More for Alignment.",
        "url": "https://huggingface.co/datasets/GAIR/lima",
        "type_name": "dataset_links"
    },
    {
        "name": "Lit-GPT",
        "description": "Hackable implementation of state-of-the-art open-source LLMs based on nanoGPT. Supports flash attention, 4-bit and 8-bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training.",
        "url": "https://github.com/Lightning-AI/lit-gpt",
        "type_name": "model_links"
    },
    {
        "name": "Lit-LLaMA",
        "description": "Independent implementation of LLaMA that is fully open source under the Apache 2.0 license.",
        "url": "https://github.com/Lightning-AI/lit-llama",
        "type_name": "utils_links"
    },
    {
        "name": "LiteLLM",
        "description": "lightweight package to simplify LLM API calls - Azure, OpenAI, Cohere, Anthropic, Replicate.",
        "url": "https://github.com/BerriAI/litellm",
        "type_name": "utils_links"
    },
    {
        "name": "Llama 2 Everywhere (Unikraft-centric setup)",
        "description": "Standalone, Binary Portable, Bootable Llama 2. This is a Unikraft-centric setup of Llama 2 Everywhere (L2E).",
        "url": "https://github.com/unikraft/app-llama2-c",
        "type_name": "utils_links"
    },
    {
        "name": "LLaMA-Adapter",
        "description": "LLaMA-Adapter is a lightweight adaption method for fine-tuning instruction-following LLaMA models, using 52K data provided by Stanford Alpaca.",
        "url": "https://github.com/ZrrSkywalker/LLaMA-Adapter",
        "type_name": "utils_links"
    },
    {
        "name": "llama.cpp",
        "description": "Inference of LLaMA model in pure C/C++.",
        "url": "https://github.com/ggerganov/llama.cpp",
        "type_name": "utils_links"
    },
    {
        "name": "LLaMA2-Accessory",
        "description": "LLaMA2-Accessory is an open-source toolkit for pre-training, fine-tuning and deployment of Large Language Models (LLMs) and mutlimodal LLMs.",
        "url": "https://github.com/Alpha-VLLM/LLaMA2-Accessory",
        "type_name": "utils_links"
    },
    {
        "name": "LlamaIndex",
        "description": "LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.",
        "url": "https://github.com/jerryjliu/llama_index",
        "type_name": "utils_links"
    },
    {
        "name": "llm",
        "description": "llm is a Rust ecosystem of libraries for running inference on large language models, inspired by llama.cpp.",
        "url": "https://github.com/rustformers/llm",
        "type_name": "utils_links"
    },
    {
        "name": "LLM Foundry",
        "description": "This repository contains code for training, finetuning, evaluating, and deploying LLMs for inference with Composer and the MosaicML platform.",
        "url": "https://github.com/mosaicml/llm-foundry",
        "type_name": "model_links"
    },
    {
        "name": "LLMParser",
        "description": "LLMParser is a simple and flexible tool to classify and extract structured data from text with large language models.",
        "url": "https://github.com/kyang6/llmparser",
        "type_name": "utils_links"
    },
    {
        "name": "lm-evaluation-harness",
        "description": "A framework for few-shot evaluation of autoregressive language models.",
        "url": "https://github.com/EleutherAI/lm-evaluation-harness",
        "type_name": "utils_links"
    },
    {
        "name": "LMFlow",
        "description": "An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.",
        "url": "https://github.com/OptimalScale/LMFlow",
        "type_name": "model_links"
    },
    {
        "name": "LMSYS-Chat",
        "description": "This dataset contains one million real-world conversations with 25 state-of-the-art LLMs.",
        "url": "https://huggingface.co/datasets/lmsys/lmsys-chat-1m",
        "type_name": "dataset_links"
    },
    {
        "name": "LOMO",
        "description": "LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage.",
        "url": "https://arxiv.org/abs/2306.09782",
        "type_name": "optimizer_links"
    },
    {
        "name": "LongT5",
        "description": "LongT5 is an extension of the T5 model that handles long sequence inputs more efficiently.",
        "url": "https://github.com/google-research/longt5",
        "type_name": "model_links"
    },
    {
        "name": "Lovely Tensors",
        "description": "Tensors, ready for human consumption.",
        "url": "https://github.com/xl0/lovely-tensors",
        "type_name": "utils_links"
    },
    {
        "name": "M3E",
        "description": "Moka Massive Mixed Embedding",
        "url": "https://github.com/wangyingdong/m3e-base",
        "type_name": "utils_links"
    },
    {
        "name": "Macaw-LLM",
        "description": "Macaw-LLM is an exploratory endeavor that pioneers multi-modal language modeling by seamlessly combining image, video, audio, and text data, built upon the foundations of CLIP, Whisper, and LLaMA.",
        "url": "https://github.com/lyuchenyang/Macaw-LLM",
        "type_name": "utils_links"
    },
    {
        "name": "Marvin",
        "description": "Meet Marvin: a batteries-included library for building AI-powered software. Marvin's job is to integrate AI directly into your codebase by making it look and feel like any other function.",
        "url": "https://github.com/PrefectHQ/marvin",
        "type_name": "utils_links"
    },
    {
        "name": "MASSIVE",
        "description": "MASSIVE is a parallel dataset of > 1M utterances across 52 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation.",
        "url": "https://github.com/alexa/massive",
        "type_name": "dataset_links"
    },
    {
        "name": "MathInstruct",
        "description": "MathInstruct is a meticulously curated instruction tuning dataset that is lightweight yet generalizable.",
        "url": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
        "type_name": "dataset_links"
    },
    {
        "name": "MaxText",
        "description": "MaxText is a high performance, arbitrarily scalable, open-source, simple, easily forkable, well-tested, batteries included LLM written in pure Python/Jax and targeting Google Cloud TPUs.",
        "url": "https://github.com/google/maxtext",
        "type_name": "model_links"
    },
    {
        "name": "Merlin Dataloader",
        "description": "The merlin dataloader lets you rapidly load tabular data for training deep leaning models with TensorFlow, PyTorch or JAX",
        "url": "https://github.com/NVIDIA-Merlin/dataloader",
        "type_name": "utils_links"
    },
    {
        "name": "metaseq",
        "description": "A codebase for working with Open Pre-trained Transformers.",
        "url": "https://github.com/facebookresearch/metaseq",
        "type_name": "model_links"
    },
    {
        "name": "MiniChain",
        "description": "MiniChain is a tiny library for coding with large language models.",
        "url": "https://github.com/srush/MiniChain",
        "type_name": "utils_links"
    },
    {
        "name": "MiniLLM",
        "description": "MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUs.",
        "url": "https://github.com/kuleshov/minillm",
        "type_name": "utils_links"
    },
    {
        "name": "ML Papers Explained",
        "description": "List of LM papers explained.",
        "url": "https://github.com/dair-ai/ML-Papers-Explained",
        "type_name": "documentation_links"
    },
    {
        "name": "MLC LLM",
        "description": "Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.",
        "url": "https://github.com/mlc-ai/mlc-llm",
        "type_name": "utils_links"
    },
    {
        "name": "MMF",
        "description": "MMF is a modular framework for vision and language multimodal research",
        "url": "https://github.com/facebookresearch/mmf",
        "type_name": "model_links"
    },
    {
        "name": "ModelScope",
        "description": "ModelScope is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform model inference, training and evaluation.",
        "url": "https://github.com/modelscope/modelscope",
        "type_name": "utils_links"
    },
    {
        "name": "ModuleFormer",
        "description": "ModuleFormer is a MoE-based architecture that includes two different types of experts: stick-breaking attention heads and feedforward experts. We released a collection of ModuleFormer-based Language Models (MoLM) ranging in scale from 4 billion to 8 billion parameters.",
        "url": "https://github.com/IBM/ModuleFormer",
        "type_name": "model_links"
    },
    {
        "name": "Mosaic composer",
        "description": "Composer is a PyTorch library that enables you to train neural networks faster, at lower cost, and to higher accuracy. Train neural networks up to 7x faster.",
        "url": "https://github.com/mosaicml/composer",
        "type_name": "model_links"
    },
    {
        "name": "Mosaic streaming",
        "description": "Fast, accurate streaming of training data from cloud storage.",
        "url": "https://github.com/mosaicml/streaming",
        "type_name": "utils_links"
    },
    {
        "name": "MosaicML Examples",
        "description": "This repo contains reference examples for training ML models quickly and to high accuracy. MosaicBERT",
        "url": "https://github.com/mosaicml/examples",
        "type_name": "utils_links"
    },
    {
        "name": "MS-AMP",
        "description": "MS-AMP is an automatic mixed precision package for deep learning developed by Microsoft.",
        "url": "https://github.com/azure/ms-amp",
        "type_name": "model_links"
    },
    {
        "name": "MultiLegalPile",
        "description": "The Multi_Legal_Pile is a large-scale multilingual legal dataset suited for pretraining language models. It spans over 24 languages and five legal text types.",
        "url": "https://huggingface.co/datasets/joelito/Multi_Legal_Pile",
        "type_name": "dataset_links"
    },
    {
        "name": "multimodal",
        "description": "TorchMultimodal is a PyTorch library for training state-of-the-art multimodal multi-task models at scale.",
        "url": "https://github.com/facebookresearch/multimodal",
        "type_name": "model_links"
    },
    {
        "name": "Multimodal-GPT",
        "description": "Train a multi-modal chatbot with visual and language instructions!",
        "url": "https://github.com/open-mmlab/Multimodal-GPT",
        "type_name": "utils_links"
    },
    {
        "name": "mup",
        "description": "Maximal Update Parametrization (μP) and Hyperparameter Transfer (μTransfer): Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
        "url": "https://github.com/microsoft/mup",
        "type_name": "utils_links"
    },
    {
        "name": "nanoGPT",
        "description": "The simplest, fastest repository for training/finetuning medium-sized GPTs.",
        "url": "https://github.com/karpathy/nanoGPT",
        "type_name": "model_links"
    },
    {
        "name": "nanoPALM",
        "description": "nanoPALM is the simplest, fastest repository for training/finetuning small to medium-sized PALM models.",
        "url": "https://github.com/RobertRiachi/nanoPALM",
        "type_name": "model_links"
    },
    {
        "name": "nanoT5",
        "description": "Fast & Simple repository for pre-training and fine-tuning T5-style models.",
        "url": "https://github.com/PiotrNawrot/nanoT5",
        "type_name": "model_links"
    },
    {
        "name": "nebullvm",
        "description": "Nebullvm is an ecosystem of plug and play modules to optimize the performances of your AI systems. The optimization modules are stack-agnostic and work with any library. They are designed to be easily integrated into your system, providing a quick and seamless boost to its performance. Simply plug and play to start realizing the benefits of optimized performance right away.",
        "url": "https://github.com/nebuly-ai/nebullvm",
        "type_name": "utils_links"
    },
    {
        "name": "NEFTune",
        "description": "Noisy Embedding Instruction Fine Tuning (NEFTune) add random noise to the embedding vectors of the training data during the forward pass of fine-tuning.",
        "url": "https://github.com/neelsjain/NEFTune",
        "type_name": "utils_links"
    },
    {
        "name": "NVIDIA Megatron-LM",
        "description": "Megatron is an efficient, model-parallel (tensor, sequence, and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.",
        "url": "https://github.com/NVIDIA/Megatron-LM",
        "type_name": "model_links"
    },
    {
        "name": "NVIDIA Merlin",
        "description": "NVIDIA Merlin is an open source library providing end-to-end GPU-accelerated recommender systems, from feature engineering and preprocessing to training deep learning models and running inference in production.",
        "url": "https://github.com/NVIDIA-Merlin/Merlin",
        "type_name": "model_links"
    },
    {
        "name": "NVIDIA NeMo",
        "description": "NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP).",
        "url": "https://github.com/NVIDIA/NeMo",
        "type_name": "model_links"
    },
    {
        "name": "NVIDIA Nsight Systems",
        "description": "NVIDIA Nsight™ Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest system on a chip (SoC).",
        "url": "https://developer.nvidia.com/nsight-systems",
        "type_name": "gpu_profiling_links"
    },
    {
        "name": "Nvidia TensorRT",
        "description": "TensorRT is an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications.",
        "url": "https://github.com/NVIDIA/TensorRT",
        "type_name": "model_links"
    },
    {
        "name": "NVIDIA Visual Profiler",
        "description": "The NVIDIA Visual Profiler is a cross-platform performance profiling tool that delivers developers vital feedback for optimizing CUDA C/C++ applications.",
        "url": "https://developer.nvidia.com/nvidia-visual-profiler",
        "type_name": "gpu_profiling_links"
    },
    {
        "name": "NVIDIA/NVTX",
        "description": "The NVIDIA® Tools Extension SDK (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications.",
        "url": "https://github.com/NVIDIA/NVTX",
        "type_name": "gpu_profiling_links"
    },
    {
        "name": "OctoPack",
        "description": "OctoPack: Instruction Tuning Code Large Language Models",
        "url": "https://github.com/bigcode-project/octopack",
        "type_name": "utils_links"
    },
    {
        "name": "Ollama",
        "description": "Run, create, and share large language models (LLMs).",
        "url": "https://github.com/jmorganca/ollama",
        "type_name": "utils_links"
    },
    {
        "name": "Opacus",
        "description": "Opacus is a library that enables training PyTorch models with differential privacy. It supports training with minimal code changes required on the client, has little impact on training performance, and allows the client to online track the privacy budget expended at any given moment.",
        "url": "https://github.com/pytorch/opacus",
        "type_name": "utils_links"
    },
    {
        "name": "Open LLMetry",
        "description": "Open-source observability for your LLM application",
        "url": "https://github.com/traceloop/openllmetry",
        "type_name": "utils_links"
    },
    {
        "name": "Open LLMs",
        "description": "A list of open LLMs available for commercial use.",
        "url": "https://github.com/eugeneyan/open-llms",
        "type_name": "documentation_links"
    },
    {
        "name": "OpenChatKit",
        "description": "OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories.",
        "url": "https://github.com/togethercomputer/OpenChatKit",
        "type_name": "utils_links"
    },
    {
        "name": "OpenELM",
        "description": "OpenELM is an open-source library by CarperAI, designed to enable evolutionary search with language models in both code and natural language.",
        "url": "https://github.com/CarperAI/OpenELM",
        "type_name": "utils_links"
    },
    {
        "name": "OpenFlamingo",
        "description": "An open-source framework for training large multimodal models.",
        "url": "https://github.com/mlfoundations/open_flamingo",
        "type_name": "utils_links"
    },
    {
        "name": "OpenLLM",
        "description": "An open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.",
        "url": "https://github.com/bentoml/OpenLLM",
        "type_name": "model_links"
    },
    {
        "name": "OpenLM",
        "description": "OpenLM is a minimal but performative language modeling (LM) repository, aimed to facilitate research on medium sized LMs.",
        "url": "https://github.com/mlfoundations/open_lm",
        "type_name": "model_links"
    },
    {
        "name": "OpenLM",
        "description": "Drop-in OpenAI-compatible library that can call LLMs from other providers (e.g., HuggingFace, Cohere, and more).",
        "url": "https://github.com/r2d4/openlm",
        "type_name": "utils_links"
    },
    {
        "name": "OpenMoE",
        "description": "OpenMoE is a project aimed at igniting the open-source MoE community! We are releasing a family of open-sourced Mixture-of-Experts (MoE) Large Language Models.",
        "url": "https://github.com/XueFuzhao/OpenMoE",
        "type_name": "model_links"
    },
    {
        "name": "OpenPipe",
        "description": "Test and deploy your LLM prompts in a data-driven way on an open-source and self-hostable platform.",
        "url": "https://github.com/OpenPipe/OpenPipe",
        "type_name": "utils_links"
    },
    {
        "name": "openplayground",
        "description": "An LLM playground you can run on your laptop.",
        "url": "https://github.com/nat/openplayground",
        "type_name": "utils_links"
    },
    {
        "name": "OSCAR (Open Super-large Crawled ALMAnaCH coRpus)",
        "description": "OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the Ungoliant architecture.",
        "url": "https://oscar-corpus.com",
        "type_name": "dataset_links"
    },
    {
        "name": "OSLO",
        "description": "OSLO is a framework that provides various GPU based optimization technologies for large-scale modeling. Features like 3D parallelism and kernel fusion which could be useful when training a large model are the key features.",
        "url": "https://github.com/EleutherAI/oslo",
        "type_name": "model_links"
    },
    {
        "name": "Otter",
        "description": "Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning.",
        "url": "https://github.com/Luodian/otter",
        "type_name": "utils_links"
    },
    {
        "name": "Outlines",
        "description": "Outlines is a library for neural text generation.",
        "url": "https://github.com/normal-computing/outlines",
        "type_name": "utils_links"
    },
    {
        "name": "P3",
        "description": "P3 (Public Pool of Prompts) is a collection of prompted English datasets covering a diverse set of NLP tasks.",
        "url": "https://huggingface.co/datasets/bigscience/P3",
        "type_name": "dataset_links"
    },
    {
        "name": "PaLM",
        "description": "An open-source implementation of Google's PaLM models.",
        "url": "https://github.com/conceptofmind/PaLM",
        "type_name": "utils_links"
    },
    {
        "name": "PaLM + RLHF",
        "description": "Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM.",
        "url": "https://github.com/lucidrains/PaLM-rlhf-pytorch",
        "type_name": "utils_links"
    },
    {
        "name": "PEFT",
        "description": "Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters.",
        "url": "https://github.com/huggingface/peft",
        "type_name": "utils_links"
    },
    {
        "name": "PG-19 Language Modelling Benchmark",
        "description": "This repository contains the PG-19 language modeling benchmark. It includes a set of books extracted from the Project Gutenberg books library, that were published before 1919. It also contains metadata of book titles and publication dates.",
        "url": "https://github.com/deepmind/pg19",
        "type_name": "dataset_links"
    },
    {
        "name": "PRESTO",
        "description": "PRESTO is a dataset of over 550K contextual multilingual conversations between humans and virtual assistants. PRESTO contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions.",
        "url": "https://github.com/google-research-datasets/presto",
        "type_name": "dataset_links"
    },
    {
        "name": "Pretrained Language Model",
        "description": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab.",
        "url": "https://github.com/huawei-noah/Pretrained-Language-Model",
        "type_name": "model_links"
    },
    {
        "name": "privateGPT",
        "description": "Interact privately with your documents using the power of GPT, 100% privately, no data leaks.",
        "url": "https://github.com/imartinez/privateGPT",
        "type_name": "utils_links"
    },
    {
        "name": "PROSE Public Benchmark Suite",
        "description": "PROSE Public Benchmark Suite contains benchmarks drawn from three classes of tasks: string-to-string transformation, text-to-table transformation, substring extraction from semi-structured text, syntax (and some semantic) program repair for code that can be achieved with few edits.",
        "url": "https://github.com/microsoft/prose-benchmarks",
        "type_name": "dataset_links"
    },
    {
        "name": "pybaselines",
        "description": "A Python library of algorithms for the baseline correction of experimental data.",
        "url": "https://github.com/derb12/pybaselines",
        "type_name": "utils_links"
    },
    {
        "name": "pykoi",
        "description": "pykoi is an open-source python library for LLMs providing a unified interface for data & feedback collection, RLHF, and model comparisons.",
        "url": "https://github.com/CambioML/pykoi",
        "type_name": "utils_links"
    },
    {
        "name": "PyTorch Geometric",
        "description": "PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.",
        "url": "https://github.com/pyg-team/pytorch_geometric",
        "type_name": "utils_links"
    },
    {
        "name": "PyTorch Profiler",
        "description": "This recipe explains how to use PyTorch profiler and measure the time and memory consumption of the model’s operators.",
        "url": "https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html",
        "type_name": "gpu_profiling_links"
    },
    {
        "name": "RasaGPT",
        "description": "RasaGPT is the first headless LLM chatbot platform built on top of Rasa and Langchain. Built w/ Rasa, FastAPI, Langchain, LlamaIndex, SQLModel, pgvector, ngrok, telegram.",
        "url": "https://github.com/paulpierre/RasaGPT",
        "type_name": "utils_links"
    },
    {
        "name": "ReCoRD",
        "description": "This repository contains a script to generate question/answer pairs using CNN and Daily Mail articles downloaded from the Wayback Machine.",
        "url": "https://github.com/deepmind/rc-data",
        "type_name": "dataset_links"
    },
    {
        "name": "RedPajama-Data",
        "description": "RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset. This repo contains a reproducible data receipe for the RedPajama data.",
        "url": "https://github.com/togethercomputer/RedPajama-Data",
        "type_name": "dataset_links"
    },
    {
        "name": "RedPajama-V2",
        "description": "RedPajama-V2 is an open dataset for training large language models. The dataset includes over 100B text documents coming from 84 CommonCrawl snapshots and processed using the CCNet pipeline.",
        "url": "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2",
        "type_name": "dataset_links"
    },
    {
        "name": "RWKV",
        "description": "RWKV is a RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer: great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding.",
        "url": "https://github.com/BlinkDL/RWKV-LM",
        "type_name": "model_links"
    },
    {
        "name": "safari",
        "description": "Convolutions for Sequence Modeling, contains: Hyena, Long Convs, Hungry Hungry Hippos (H3).",
        "url": "https://github.com/HazyResearch/safari",
        "type_name": "utils_links"
    },
    {
        "name": "SapientML",
        "description": "SapientML is an AutoML technology that can learn from a corpus of existing datasets and their human-written pipelines, and efficiently generate a high-quality pipeline for a predictive task on a new dataset.",
        "url": "https://github.com/sapientml/sapientml",
        "type_name": "utils_links"
    },
    {
        "name": "Seahorse",
        "description": "Seahorse is a dataset for multilingual, multifaceted summarization evaluation. It contains 96K summaries with human ratings along 6 quality dimensions: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4 datasets.",
        "url": "https://github.com/google-research-datasets/seahorse",
        "type_name": "dataset_links"
    },
    {
        "name": "Self-Instruct",
        "description": "Self-Instruct is a framework that helps language models improve their ability to follow natural language instructions.",
        "url": "https://huggingface.co/datasets/yizhongw/self_instruct",
        "type_name": "dataset_links"
    },
    {
        "name": "Semantic Kernel",
        "description": "Semantic Kernel (SK) is a lightweight SDK enabling integration of AI Large Language Models (LLMs) with conventional programming languages.",
        "url": "https://github.com/microsoft/semantic-kernel",
        "type_name": "utils_links"
    },
    {
        "name": "SentencePiece",
        "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) and unigram language model ) with the extension of direct training from raw sentences.",
        "url": "https://github.com/google/sentencepiece",
        "type_name": "vocabulary_links"
    },
    {
        "name": "SeqIO",
        "description": "Task-based datasets, preprocessing, and evaluation for sequence models.",
        "url": "https://github.com/google/seqio",
        "type_name": "model_links"
    },
    {
        "name": "simpletransformers",
        "description": "This library is based on the Transformers library by HuggingFace. Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model.",
        "url": "https://github.com/ThilinaRajapakse/simpletransformers",
        "type_name": "model_links"
    },
    {
        "name": "skorch",
        "description": "A scikit-learn compatible neural network library that wraps PyTorch.",
        "url": "https://github.com/skorch-dev/skorch",
        "type_name": "utils_links"
    },
    {
        "name": "SlimPajama",
        "description": "The dataset consists of 59166 jsonl files and is ~895GB compressed. It is a cleaned and deduplicated version of Together's RedPajama.",
        "url": "https://huggingface.co/datasets/cerebras/SlimPajama-627B",
        "type_name": "dataset_links"
    },
    {
        "name": "slowllama",
        "description": "Fine-tune Llama2 and CodeLLama models, including 70B/35B on Apple M1/M2 devices (for example, Macbook Air or Mac Mini) or consumer nVidia GPUs.",
        "url": "https://github.com/okuvshynov/slowllama",
        "type_name": "utils_links"
    },
    {
        "name": "small-text",
        "description": "Small-Text provides state-of-the-art Active Learning for Text Classification.",
        "url": "https://github.com/webis-de/small-text",
        "type_name": "model_links"
    },
    {
        "name": "Sockeye",
        "description": "Sockeye is an open-source sequence-to-sequence framework for Neural Machine Translation built on PyTorch.",
        "url": "https://github.com/awslabs/sockeye",
        "type_name": "model_links"
    },
    {
        "name": "Sophia",
        "description": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training",
        "url": "https://arxiv.org/abs/2305.14342",
        "type_name": "optimizer_links"
    },
    {
        "name": "SparseML",
        "description": "Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models.",
        "url": "https://github.com/neuralmagic/sparseml",
        "type_name": "utils_links"
    },
    {
        "name": "SparseZoo",
        "description": "Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes.",
        "url": "https://github.com/neuralmagic/sparsezoo",
        "type_name": "model_links"
    },
    {
        "name": "SpikeGPT",
        "description": "SpikeGPT is a lightweight generative language model with pure binary, event-driven spiking activation units.",
        "url": "https://github.com/ridgerchu/SpikeGPT",
        "type_name": "model_links"
    },
    {
        "name": "srf-attention",
        "description": "Simplex Random Feature attention, in PyTorch",
        "url": "https://github.com/notarussianteenager/srf-attention",
        "type_name": "utils_links"
    },
    {
        "name": "StableLM",
        "description": "StableLM: Stability AI Language Models",
        "url": "https://github.com/stability-AI/stableLM",
        "type_name": "model_links"
    },
    {
        "name": "Stanford Alpaca",
        "description": "This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model.",
        "url": "https://github.com/tatsu-lab/stanford_alpaca",
        "type_name": "utils_links"
    },
    {
        "name": "StarCoder",
        "description": "Home of StarCoder: fine-tuning & inference!",
        "url": "https://github.com/bigcode-project/starcoder",
        "type_name": "utils_links"
    },
    {
        "name": "state-spaces",
        "description": "Sequence Modeling with Structured State Spaces, contains: S4D, HTTYH, SaShiMi, S4, LSSL, HiPPO.",
        "url": "https://github.com/HazyResearch/state-spaces",
        "type_name": "utils_links"
    },
    {
        "name": "StreamingLLM",
        "description": "Efficient Streaming Language Models with Attention Sinks",
        "url": "https://github.com/mit-han-lab/streaming-llm",
        "type_name": "model_links"
    },
    {
        "name": "string2string",
        "description": "The string2string library is an open-source tool that offers a comprehensive suite of efficient algorithms for a broad range of string-to-string problems. It includes both traditional algorithmic solutions and recent advanced neural approaches to address various problems in pairwise string alignment, distance measurement, lexical and semantic search, and similarity analysis. Additionally, the library provides several helpful visualization tools and metrics to facilitate the interpretation and analysis of these methods.",
        "url": "https://github.com/stanfordnlp/string2string",
        "type_name": "utils_links"
    },
    {
        "name": "Superagent",
        "description": "Superagent is a powerful tool that simplifies the configuration and deployment of LLM (Large Language Model) Agents to production.",
        "url": "https://github.com/homanp/superagent",
        "type_name": "utils_links"
    },
    {
        "name": "surgeon-pytorch",
        "description": "A library to inspect and extract intermediate layers of PyTorch models.",
        "url": "https://github.com/archinetai/surgeon-pytorch",
        "type_name": "utils_links"
    },
    {
        "name": "t5-experiments",
        "description": "Tools and scripts for experimenting with Transformers: Bert, T5.",
        "url": "https://github.com/yurakuratov/t5-experiments",
        "type_name": "model_links"
    },
    {
        "name": "t5x",
        "description": "T5X is a modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models (starting with language) at many scales.",
        "url": "https://github.com/google-research/t5x",
        "type_name": "model_links"
    },
    {
        "name": "tasknet",
        "description": "Integration of HuggingFace Datasets with HuggingFace Trainer, and multitasking.",
        "url": "https://github.com/sileod/tasknet",
        "type_name": "model_links"
    },
    {
        "name": "tbparse",
        "description": "A simple yet powerful tensorboard event log parser/reader.",
        "url": "https://github.com/j3soon/tbparse",
        "type_name": "utils_links"
    },
    {
        "name": "Text Generation Inference",
        "description": "Text Generation Inference is a Rust, Python and gRPC server for text generation inference.",
        "url": "https://github.com/huggingface/text-generation-inference",
        "type_name": "utils_links"
    },
    {
        "name": "Text Generation Inference (fork, free)",
        "description": "Text Generation Inference is a Rust, Python and gRPC server for text generation inference.",
        "url": "https://github.com/Preemo-Inc/text-generation-inference",
        "type_name": "utils_links"
    },
    {
        "name": "TextBox",
        "description": "TextBox 2.0 is a text generation library with pre-trained language models.",
        "url": "https://github.com/RUCAIBox/TextBox",
        "type_name": "utils_links"
    },
    {
        "name": "TextReducer",
        "description": "TextReducer is a tool for summarization and information extraction powered by the SentenceTransformer library.",
        "url": "https://github.com/helliun/targetedSummarization",
        "type_name": "utils_links"
    },
    {
        "name": "The Practical Guides for Large Language Models",
        "description": "A curated (still actively updated) list of practical guide resources of LLMs.",
        "url": "https://github.com/Mooler0410/LLMsPracticalGuide",
        "type_name": "documentation_links"
    },
    {
        "name": "tiktoken",
        "description": "tiktoken is a fast BPE tokeniser for use with OpenAI's models.",
        "url": "https://github.com/openai/tiktoken",
        "type_name": "vocabulary_links"
    },
    {
        "name": "tiny-codes",
        "description": "This synthetic dataset is a collection of 1.6 millions short and clear code snippets that can help LLM models learn how to reason with both natural and programming languages.",
        "url": "https://huggingface.co/datasets/nampdn-ai/tiny-codes",
        "type_name": "dataset_links"
    },
    {
        "name": "tinygrad",
        "description": "tinygrad is an extremely simple deep-learning framework. It supports both inference and training.",
        "url": "https://github.com/geohot/tinygrad",
        "type_name": "utils_links"
    },
    {
        "name": "tokenizers",
        "description": "Provides an implementation of today's most used tokenizers, with a focus on performance and versatility.",
        "url": "https://github.com/huggingface/tokenizers",
        "type_name": "vocabulary_links"
    },
    {
        "name": "TokenMonster",
        "description": "TokenMonster is an ungreedy subword tokenizer and vocabulary generator, enabling language models to run faster, cheaper, smarter and generate longer streams of text.",
        "url": "https://github.com/alasdairforsythe/tokenmonster",
        "type_name": "vocabulary_links"
    },
    {
        "name": "TorchBench",
        "description": "TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.",
        "url": "https://github.com/pytorch/benchmark",
        "type_name": "utils_links"
    },
    {
        "name": "torchdistX",
        "description": "Torch Distributed Experimental",
        "url": "https://github.com/pytorch/torchdistx",
        "type_name": "utils_links"
    },
    {
        "name": "torchegranate",
        "description": "Fast, flexible and easy to use probabilistic modelling with PyTorch.",
        "url": "https://github.com/jmschrei/torchegranate",
        "type_name": "utils_links"
    },
    {
        "name": "torchnnprofiler",
        "description": "Context Manager to profile the forward and backward times of PyTorch's nn.Module",
        "url": "https://github.com/kshitij12345/torchnnprofiler",
        "type_name": "gpu_profiling_links"
    },
    {
        "name": "TorchScale",
        "description": "TorchScale is a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively. It has the implementation of fundamental research to improve modeling generality and capability, as well as training stability and efficiency of scaling Transformers.",
        "url": "https://github.com/microsoft/torchscale",
        "type_name": "model_links"
    },
    {
        "name": "Training BERT with Compute/Time (Academic) Budget",
        "description": "Repository contains scripts for pre-training and finetuning BERT-like models with limited time and compute budget.",
        "url": "https://github.com/IntelLabs/academic-budget-bert",
        "type_name": "utils_links"
    },
    {
        "name": "Trankit",
        "description": "A Light-Weight Transformer-based Python Toolkit for Multilingual Natural Language Processing",
        "url": "https://github.com/nlp-uoregon/trankit",
        "type_name": "model_links"
    },
    {
        "name": "Transformer",
        "description": "Explanation of Transformer architecture from the code.",
        "url": "https://nn.labml.ai/transformers/index.html",
        "type_name": "documentation_links"
    },
    {
        "name": "Transformer models: an introduction and catalog, 2023 Edition",
        "description": "An introduction and catalog of Transformer models.",
        "url": "https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376",
        "type_name": "documentation_links"
    },
    {
        "name": "transformer-deploy",
        "description": "Efficient, scalable and enterprise-grade CPU/GPU inference server for Hugging Face transformer models.",
        "url": "https://github.com/ELS-RD/transformer-deploy",
        "type_name": "utils_links"
    },
    {
        "name": "transformers",
        "description": "State-of-the-art Natural Language Processing for PyTorch and TensorFlow 2.0",
        "url": "https://github.com/huggingface/transformers",
        "type_name": "model_links"
    },
    {
        "name": "Transformers from Scratch",
        "description": "Explanation of Transformer architecture.",
        "url": "https://e2eml.school/transformers.html",
        "type_name": "documentation_links"
    },
    {
        "name": "tsai",
        "description": "tsai is an open-source deep learning package built on top of Pytorch & fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation.",
        "url": "https://github.com/timeseriesAI/tsai",
        "type_name": "utils_links"
    },
    {
        "name": "txtai",
        "description": "txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.",
        "url": "https://github.com/neuml/txtai",
        "type_name": "model_links"
    },
    {
        "name": "UltraChat",
        "description": "UltraChat is an open-source, large-scale, and multi-round dialogue data powered by Turbo APIs.",
        "url": "https://huggingface.co/datasets/stingning/ultrachat",
        "type_name": "dataset_links"
    },
    {
        "name": "UltraFeedback",
        "description": "UltraFeedback is a large-scale, fine-grained, diverse preference dataset, used for training powerful reward models and critic models.",
        "url": "https://huggingface.co/datasets/openbmb/UltraFeedback",
        "type_name": "dataset_links"
    },
    {
        "name": "Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch",
        "description": "Understanding how self-attention works from scratch.",
        "url": "https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html",
        "type_name": "documentation_links"
    },
    {
        "name": "unilm",
        "description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities.",
        "url": "https://github.com/microsoft/unilm",
        "type_name": "model_links"
    },
    {
        "name": "UpTrain",
        "description": "UpTrain is an open-source, data-secure tool for ML practitioners to observe and refine their ML models by monitoring their performance, checking for (data) distribution shifts, and collecting edge cases to retrain them upon.",
        "url": "https://github.com/uptrain-ai/uptrain",
        "type_name": "utils_links"
    },
    {
        "name": "Vizro",
        "description": "Vizro is a toolkit for creating modular data visualization applications.",
        "url": "https://github.com/mckinsey/vizro",
        "type_name": "visualization_links"
    },
    {
        "name": "vLLM",
        "description": "vLLM is a fast and easy-to-use library for LLM inference and serving.",
        "url": "https://github.com/vllm-project/vllm",
        "type_name": "utils_links"
    },
    {
        "name": "voltaML",
        "description": "VoltaML is a lightweight library to convert and run your ML/DL deep learning models in high performance inference runtimes like TensorRT, TorchScript, ONNX and TVM.",
        "url": "https://github.com/VoltaML/voltaML",
        "type_name": "model_links"
    },
    {
        "name": "WeightWatcher",
        "description": "WeightWatcher (WW): is an open-source, diagnostic tool for analyzing Deep Neural Networks (DNN), without needing access to training or even test data.",
        "url": "https://github.com/CalculatedContent/WeightWatcher",
        "type_name": "utils_links"
    },
    {
        "name": "WikiMatrix",
        "description": "WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia.",
        "url": "https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix",
        "type_name": "dataset_links"
    },
    {
        "name": "WizardLM",
        "description": "WizardLM: An Instruction-following LLM Using Evol-Instruct. At present, our core contributors are preparing the 65B version and we expect to empower WizardLM with the ability to perform instruction evolution itself, aiming to evolve your specific data at a low cost.",
        "url": "https://github.com/nlpxucan/WizardLM",
        "type_name": "utils_links"
    },
    {
        "name": "word2vec",
        "description": "This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words.",
        "url": "https://github.com/dav/word2vec",
        "type_name": "utils_links"
    },
    {
        "name": "x-transformers",
        "description": "A simple but complete full-attention transformer with a set of promising experimental features from various papers.",
        "url": "https://github.com/lucidrains/x-transformers",
        "type_name": "model_links"
    },
    {
        "name": "xFormers",
        "description": "xFormers is a modular and field agnostic library to flexibly generate transformer architectures by interoperable and optimized building blocks.",
        "url": "https://github.com/facebookresearch/xformers",
        "type_name": "model_links"
    },
    {
        "name": "Xinference",
        "description": "Xorbits Inference(Xinference) is a powerful and versatile library designed to serve language, speech recognition, and multimodal models. With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command.",
        "url": "https://github.com/xorbitsai/inference",
        "type_name": "utils_links"
    },
    {
        "name": "xP3",
        "description": "xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks.",
        "url": "https://huggingface.co/datasets/bigscience/xP3",
        "type_name": "dataset_links"
    },
    {
        "name": "xturing",
        "description": "xturing provides fast, efficient and simple fine-tuning of LLMs, such as LLaMA, GPT-J, GPT-2, OPT, Cerebras-GPT, Galactica, and more.",
        "url": "https://github.com/stochasticai/xturing",
        "type_name": "utils_links"
    }
]