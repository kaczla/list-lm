[
    {
        "name": "AgentTuning",
        "description": "AgentTuning represents the very first attempt to instruction-tune LLMs using interaction trajectories across multiple agent tasks.",
        "url": "https://github.com/THUDM/AgentTuning",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "AITemplate",
        "description": "AITemplate (AIT) is a Python framework that transforms deep neural networks into CUDA (NVIDIA GPU) / HIP (AMD GPU) C++ code for lightning-fast inference serving.",
        "url": "https://github.com/facebookincubator/AITemplate",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Alpaca-LoRA",
        "description": "This repository contains code for reproducing the Stanford Alpaca results using low-rank adaptation (LoRA).",
        "url": "https://github.com/tloen/alpaca-lora",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "AlpacaEval",
        "description": "AlpacaEval : An Automatic Evaluator for Instruction-following Language Models.",
        "url": "https://github.com/tatsu-lab/alpaca_eval",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "AlpacaFarm",
        "description": "AlpacaFarm is a simulator that enables research and development on learning from feedback at a fraction of the usual cost, promoting accessible research on instruction following and alignment.",
        "url": "https://github.com/tatsu-lab/alpaca_farm",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Ambrosia",
        "description": "Ambrosia is a cross-platform command line tool for improving the text datasets you use for machine learning.",
        "url": "https://github.com/reactorsh/ambrosia",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "AQT",
        "description": "AQT is a software library designed for easy tensor opeartion quantization in JAX.",
        "url": "https://github.com/google/aqt",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "AutoChain",
        "description": "AutoChain: Build lightweight, extensible, and testable LLM Agents",
        "url": "https://github.com/Forethought-Technologies/AutoChain",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "AutoGen",
        "description": "AutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation.",
        "url": "https://github.com/microsoft/autogen",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Aviary",
        "description": "Aviary is an app that lets you interact with a variety of large language models (LLMs) in a single place. You can compare the outputs of different models directly, rank them by quality, get a cost and latency estimate, and more.",
        "url": "https://github.com/ray-project/aviary",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Axolotl",
        "description": "Axolotl is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.",
        "url": "https://github.com/OpenAccess-AI-Collective/axolotl",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "bagua",
        "description": "Bagua is a deep learning training acceleration framework for PyTorch",
        "url": "https://github.com/BaguaSys/bagua",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Batched LoRAs",
        "description": "Maximize GPU util by routing inference through multiple LoRAs in the same batch.",
        "url": "https://github.com/sabetAI/BLoRA",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "BayLing",
        "description": "BayLing is an instruction-following large language model equipped with advanced language alignment, showing superior capability in English/Chinese generation, instruction following and multi-turn interaction.",
        "url": "https://github.com/ictnlp/BayLing",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "BertViz",
        "description": "BertViz is a tool for visualizing attention in the Transformer model, supporting most models from the transformers library (BERT, GPT-2, XLNet, RoBERTa, XLM, CTRL, BART, etc.)",
        "url": "https://github.com/jessevig/bertviz",
        "link_type": "Visualization links",
        "manual_validated": false
    },
    {
        "name": "BIG-bench",
        "description": "The Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. The more than 200 tasks included in BIG-bench.",
        "url": "https://github.com/google/BIG-bench",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "BiGS",
        "description": "This repository contains BiGS's jax model definitions, pretrained models weights, training and fine-tuning code for our paper exploring using state space models for pretraining.",
        "url": "https://github.com/jxiw/BiGS",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "BitFit",
        "description": "BitFit is a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified.",
        "url": "https://github.com/benzakenelad/BitFit",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "bitsandbytes",
        "description": "The bitsandbytes is a lightweight wrapper around CUDA custom functions, in particular 8-bit optimizers, matrix multiplication (LLM.int8()), and quantization functions.",
        "url": "https://github.com/TimDettmers/bitsandbytes",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "bricks",
        "description": "Open-source natural language enrichments at your fingertips.",
        "url": "https://github.com/code-kern-ai/bricks",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Burn",
        "description": "Burn - A Flexible and Comprehensive Deep Learning Framework in Rust",
        "url": "https://github.com/burn-rs/burn",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "BYOD",
        "description": "A framework for self-supervised model evaluation. In this framework, metrics are defined as invariances and sensitivities that can be checked in a self-supervised fashion using interventions based only on the model in question rather than external labels.",
        "url": "https://github.com/neelsjain/BYOD",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "C Transformers",
        "description": "Python bindings for the Transformer models implemented in C/C++ using GGML library.",
        "url": "https://github.com/marella/ctransformers",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "candle",
        "description": "Candle is a minimalist ML framework for Rust with a focus on performance (including GPU support) and ease of use.",
        "url": "https://github.com/huggingface/candle",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "CCMatrix",
        "description": "CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB.",
        "url": "https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "CCNet",
        "description": "Tools to download and clean Common Crawl as introduced in our paper CCNet: High Quality Monolingual Datasets from Web Crawl Data.",
        "url": "https://github.com/facebookresearch/cc_net",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "Chain of Explanation",
        "description": "Chain of Explanation: New Prompting Method to Generate Higher Quality Natural Language Explanation for Implicit Hate Speech",
        "url": "https://arxiv.org/abs/2209.04889",
        "link_type": "Topic links",
        "manual_validated": false
    },
    {
        "name": "Chain-of-Knowledge",
        "description": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
        "url": "https://arxiv.org/abs/2305.13269",
        "link_type": "Topic links",
        "manual_validated": false
    },
    {
        "name": "Chain-of-Thought",
        "description": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
        "url": "https://arxiv.org/abs/2201.11903",
        "link_type": "Topic links",
        "manual_validated": false
    },
    {
        "name": "Chain-of-Verification",
        "description": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "url": "https://arxiv.org/abs/2309.11495",
        "link_type": "Topic links",
        "manual_validated": false
    },
    {
        "name": "ChainForge",
        "description": "ChainForge is a data flow prompt engineering environment for analyzing and evaluating LLM responses. It is geared towards early-stage, quick-and-dirty exploration of prompts and response quality that goes beyond ad-hoc chatting with individual LLMs.",
        "url": "https://github.com/ianarawjo/ChainForge",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "cleanlab",
        "description": "cleanlab automatically finds and fixes errors in any ML dataset",
        "url": "https://github.com/cleanlab/cleanlab",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "CodeGeeX",
        "description": "We introduce CodeGeeX, a large-scale multilingual code generation model with 13 billion parameters, pre-trained on a large code corpus of more than 20 programming languages.",
        "url": "https://github.com/THUDM/CodeGeeX",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "CodeT5 and CodeT5+",
        "description": "Official research release for CodeT5 and CodeT5+ models for Code Understanding and Generation from Salesforce Research/",
        "url": "https://github.com/salesforce/CodeT5",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "CodeTF",
        "description": "CodeTF is a one-stop Python transformer-based library for code large language models (Code LLMs) and code intelligence, provides a seamless interface for training and inferencing on code intelligence tasks like code summarization, translation, code generation and so on.",
        "url": "https://github.com/salesforce/CodeTF",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "COLLIE",
        "description": "COLLIE framework for easy constraint structure specification, example extraction, instruction rendering, and model evaluation.",
        "url": "https://github.com/princeton-nlp/Collie",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "ColossalAI",
        "description": "Colossal-AI: A Unified Deep Learning System for Big Model Era. Making large AI models cheaper, faster and more accessible.",
        "url": "https://github.com/hpcaitech/ColossalAI",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "comgra",
        "description": "Comgra stands for \"computation graph analysis\" and it is a library for use with pytorch that makes it easier to inspect the internals of your neural networks.",
        "url": "https://github.com/FlorianDietz/comgra",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Cramming Language Model (Pretraining)",
        "description": "Cramming the training of a (BERT-type) language model into limited compute. Cramming: Training a Language Model on a Single GPU in One Day.",
        "url": "https://github.com/JonasGeiping/cramming",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "CRITIC",
        "description": "CRITIC empowers LLMs to validate and rectify themselves through interaction with external tools.",
        "url": "https://github.com/microsoft/ProphetNet/tree/master/CRITIC",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "CTranslate2",
        "description": "CTranslate2 is a C++ and Python library for efficient inference with Transformer models.",
        "url": "https://github.com/OpenNMT/CTranslate2",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "CulturaX",
        "description": "CulturaX is a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for large language model (LLM) development.",
        "url": "https://huggingface.co/datasets/uonlp/CulturaX",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "cyclemoid-pytorch",
        "description": "This is an implementation of the cyclemoid activation function for PyTorch.",
        "url": "https://github.com/rasbt/cyclemoid-pytorch",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Daft",
        "description": "Daft is a fast, Pythonic and scalable open-source dataframe library built for Python and Machine Learning workloads.",
        "url": "https://github.com/Eventual-Inc/Daft",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "dalai",
        "description": "Run LLaMA and Alpaca on your computer.",
        "url": "https://github.com/cocktailpeanut/dalai",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "DALMs",
        "description": "Domain Adapted Language Modeling Toolkit - E2E RAG",
        "url": "https://github.com/arcee-ai/DALM",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "datasketch",
        "description": "datasketch gives you probabilistic data structures that can process and search very large amount of data super fast, with little loss of accuracy. MinHash, LSH, LSH Forest, Weighted MinHash, HyperLogLog, HyperLogLog++, LSH Ensemble.",
        "url": "https://github.com/ekzhu/datasketch",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "DataTrove",
        "description": "DataTrove is a library to process, filter and deduplicate text data at a very large scale. It provides a set of prebuilt commonly used processing blocks with a framework to easily add custom functionality.",
        "url": "https://github.com/huggingface/datatrove",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Deduplicated CommonCrawl Text",
        "description": "Processed Common Crawl snapshots.",
        "url": "http://statmt.org/ngrams/deduped",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "DeepEval",
        "description": "DeepEval is a simple-to-use, open-source evaluation framework for LLM applications.",
        "url": "https://github.com/confident-ai/deepeval",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "deepseek-LLM",
        "description": "DeepSeek LLM: Let there be answers",
        "url": "https://github.com/deepseek-ai/deepseek-LLM",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "DeepSparse",
        "description": "Inference runtime offering GPU-class performance on CPUs and APIs to integrate ML into your application.",
        "url": "https://github.com/neuralmagic/deepsparse",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "DeepSpeed",
        "description": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.",
        "url": "https://github.com/microsoft/DeepSpeed",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "DeepSpeed Model Implementations for Inference",
        "description": "DeepSpeed Model Implementations for Inference (MII) makes low-latency and high-throughput inference possible, powered by DeepSpeed.",
        "url": "https://github.com/microsoft/DeepSpeed-MII",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "DeepSpeed-MII",
        "description": "MII makes low-latency and high-throughput inference possible, powered by DeepSpeed.",
        "url": "https://github.com/microsoft/DeepSpeed-MII",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "DGL",
        "description": "DGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs. DGL is framework agnostic, meaning if a deep graph model is a component of an end-to-end application, the rest of the logics can be implemented in any major frameworks, such as PyTorch, Apache MXNet or TensorFlow.",
        "url": "https://github.com/dmlc/dgl",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Diff Pruning",
        "description": "Parameter Efficient Transfer Learning with Diff Pruning",
        "url": "https://github.com/dguo98/DiffPruning",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Dolly",
        "description": "Databricks’ Dolly is an instruction-following large language model trained on the Databricks machine learning platform that is licensed for commercial use.",
        "url": "https://github.com/databrickslabs/dolly",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Dolma",
        "description": "Dolma is a dataset of 3 trillion tokens from a diverse mix of web content, academic publications, code, books, and encyclopedic materials.",
        "url": "https://huggingface.co/datasets/allenai/dolma",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "DPO: Direct Preference Optimization",
        "description": "Reference implementation for DPO (Direct Preference Optimization)",
        "url": "https://github.com/eric-mitchell/direct-preference-optimization",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "DS-1000",
        "description": "Official data and code release for the paper DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.",
        "url": "https://github.com/HKUNLP/DS-1000",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "DSPy",
        "description": "DSPy is the framework for solving advanced tasks with language models (LMs) and retrieval models (RMs). DSPy unifies techniques for prompting and fine-tuning LMs.",
        "url": "https://github.com/stanfordnlp/dspy",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "EAGLE",
        "description": "EAGLE (Extrapolation Algorithm for Greater Language-model Efficiency) is a new baseline for fast decoding of Large Language Models (LLMs) with provable performance maintenance.",
        "url": "https://github.com/SafeAILab/EAGLE",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "EasyLLM",
        "description": "EasyLLM is an open source project that provides helpful tools and methods for working with large language models (LLMs), both open source and closed source.",
        "url": "https://github.com/philschmid/easyllm",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "EasyLM",
        "description": "Large language models (LLMs) made easy, EasyLM is a one stop solution for pre-training, finetuning, evaluating and serving LLMs in JAX/Flax. EasyLM can scale up LLM training to hundreds of TPU/GPU accelerators by leveraging JAX's pjit functionality.",
        "url": "https://github.com/young-geng/EasyLM",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Ecoo",
        "description": "Ecco is a python library for exploring and explaining Natural Language Processing models using interactive visualizations.",
        "url": "https://github.com/jalammar/ecco",
        "link_type": "Visualization links",
        "manual_validated": false
    },
    {
        "name": "embedchain",
        "description": "embedchain is a framework to easily create LLM powered bots over any dataset.",
        "url": "https://github.com/embedchain/embedchain",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "evals",
        "description": "Evals is a framework for evaluating OpenAI models and an open-source registry of benchmarks.",
        "url": "https://github.com/openai/evals",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "ExecuTorch",
        "description": "ExecuTorch is an end-to-end solution for enabling on-device inference capabilities across mobile and edge devices including wearables, embedded devices and microcontrollers. It is part of the PyTorch Edge ecosystem and enables efficient deployment of PyTorch models to edge devices.",
        "url": "https://github.com/pytorch/executorch",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "ExLlama",
        "description": "A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.",
        "url": "https://github.com/turboderp/exllama",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "ExLlamaV2",
        "description": "ExLlamaV2 is a fast inference library for running LLMs locally on modern consumer-class GPUs",
        "url": "https://github.com/turboderp/exllamav2",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Expert Sparsity",
        "description": "Official Pytorch implementation of the expert pruning and dynamic skipping methods.",
        "url": "https://github.com/lucky-lance/expert_sparsity",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Explainpaper",
        "description": "Upload a paper, highlight confusing text, get an explanation.",
        "url": "https://www.explainpaper.com",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "ExtremeBERT",
        "description": "ExtremeBERT is a toolkit that accelerates the pretraining of customized language models on customized datasets.",
        "url": "https://github.com/extreme-bert/extreme-bert",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "fabric",
        "description": "fabric is an open-source framework for augmenting humans using AI.",
        "url": "https://github.com/danielmiessler/fabric",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "FairScale",
        "description": "FairScale is a PyTorch extension library for high performance and large scale training. This library extends basic PyTorch capabilities while adding new SOTA scaling techniques.",
        "url": "https://github.com/facebookresearch/fairscale",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Falcon RefinedWeb",
        "description": "Falcon RefinedWeb is a massive English web dataset.",
        "url": "https://huggingface.co/datasets/tiiuae/falcon-refinedweb",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "FastChat",
        "description": "FastChat is an open platform for training, serving, and evaluating large language model based chatbots.",
        "url": "https://github.com/lm-sys/FastChat",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "FastDeploy",
        "description": "Easy-to-use and Fast Deep Learning Model Deployment Toolkit for Cloud Mobile and Edge. Including Image, Video, Text and Audio 20+ main stream scenarios and 150+ SOTA models with end-to-end optimization, multi-platform and multi-framework support.",
        "url": "https://github.com/PaddlePaddle/FastDeploy",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "FasterTransformer",
        "description": "This repository provides a script and recipe to run the highly optimized transformer-based encoder and decoder component, and it is tested and maintained by NVIDIA.",
        "url": "https://github.com/NVIDIA/FasterTransformer",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "FastSeq",
        "description": "FastSeq provides efficient implementation of popular sequence models (e.g. Bart, ProphetNet) for text generation, summarization, translation tasks etc.",
        "url": "https://github.com/microsoft/fastseq",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "fastText",
        "description": "fastText is a library for efficient learning of word representations and sentence classification.",
        "url": "https://github.com/facebookresearch/fastText/",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "FewGLUE",
        "description": "FewGLUE dataset, consisting of a random selection of 32 training examples from the SuperGLUE training sets and up to 20,000 unlabeled examples for each SuperGLUE task.",
        "url": "https://github.com/timoschick/fewglue",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "FlagAI",
        "description": "FlagAI (Fast LArge-scale General AI models) is a fast, easy-to-use and extensible toolkit for large-scale model. Our goal is to support training, fine-tuning, and deployment of large-scale models on various downstream tasks with multi-modality.",
        "url": "https://github.com/FlagAI-Open/FlagAI",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "FLAML",
        "description": "A Fast Library for Automated Machine Learning & Tuning.",
        "url": "https://github.com/microsoft/FLAML",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Flan v2",
        "description": "Flan 2022 collection of datasets and templates.",
        "url": "https://github.com/google-research/FLAN/tree/main/flan/v2",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "FlashAttention",
        "description": "Fast and memory-efficient exact attention.",
        "url": "https://github.com/HazyResearch/flash-attention",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "FlashFFTConv",
        "description": "FlashFFTConv is a fast algorithm for computing long depthwise convolutions using the FFT algorithm.",
        "url": "https://github.com/HazyResearch/flash-fft-conv",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "FlashInfer",
        "description": "FlashInfer: Kernel Library for LLM Serving",
        "url": "https://github.com/flashinfer-ai/flashinfer",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "FlexGen",
        "description": "FlexGen is a high-throughput generation engine for running large language models with limited GPU memory. FlexGen allows high-throughput generation by IO-efficient offloading, compression, and large effective batch sizes.",
        "url": "https://github.com/FMInference/FlexGen",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "GENIUS",
        "description": "GENIUS generating text using sketches! A strong and general textual data augmentation tool.",
        "url": "https://github.com/beyondguo/geniushttps://github.com/beyondguo/genius",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "GeoV",
        "description": "The GeoV model is a large langauge model designed by Georges Harik and uses Rotary Positional Embeddings with Relative distances (RoPER). We have shared a pre-trained 9B parameter model.",
        "url": "https://github.com/geov-ai/geov",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "ggml",
        "description": "Tensor library for machine learning. With ggml you can efficiently run GPT-2 and GPT-J inference on the CPU.",
        "url": "https://github.com/ggerganov/ggml",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Gibberish Detector",
        "description": "Train a model, and detect gibberish strings with it. Example gibberish: \"ertrjiloifdfyyoiu\".",
        "url": "https://github.com/domanchi/gibberish-detector",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "gigaGPT",
        "description": "We present gigaGPT – the simplest implementation for training large language models with tens or hundreds of billions of parameters.",
        "url": "https://github.com/Cerebras/gigaGPT",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Glaive-code-assistant",
        "description": "Glaive-code-assistant is a dataset of ~140k code problems and solutions generated using Glaive’s synthetic data generation platform.",
        "url": "https://huggingface.co/datasets/glaiveai/glaive-code-assistant",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "GLGE",
        "description": "This repository contains information about the general langugae generation evaluation benchmark GLGE, which is composed of 8 language generation tasks, including Abstractive Text Summarization (CNN/DailyMail, Gigaword, XSUM, MSNews), Answer-aware Question Generation (SQuAD 1.1, MSQG), Conversational Question Answering (CoQA), and Personalizing Dialogue (Personachat).",
        "url": "https://github.com/microsoft/glge",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "GlotLID",
        "description": "GlotLID is an open-source language identification model with support for more than 1600 languages.",
        "url": "https://github.com/cisnlp/GlotLID",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "gpt-fast",
        "description": "Simple and efficient pytorch-native transformer text generation.",
        "url": "https://github.com/pytorch-labs/gpt-fast",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "GPT4All",
        "description": "gpt4all: a chatbot trained on a massive collection of clean assistant data including code, stories and dialogue.",
        "url": "https://github.com/nomic-ai/gpt4all",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Graphic of learning rate schedulers",
        "description": "Simple visualizations of learning rate schedulers.",
        "url": "https://raw.githubusercontent.com/rasbt/machine-learning-notes/7abac1b3dfe47b84887fcee80e5cca0e7ebf5061/learning-rates/scheduler-comparison/overview.png",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "Guardrails",
        "description": "Guardrails is an open-source Python package for specifying structure and type, validating and correcting the outputs of large language models (LLMs).",
        "url": "https://github.com/ShreyaR/guardrails",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "H2O LLM Studio",
        "description": "H2O LLM Studio, a framework and no-code GUI designed for fine-tuning state-of-the-art large language models (LLMs).",
        "url": "https://github.com/h2oai/h2o-llmstudio",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "h2oGPT",
        "description": "h2oGPT - The world's best open source GPT: open-source repository with fully permissive, commercially usable code, data and models and code for fine-tuning large language models.",
        "url": "https://github.com/h2oai/h2ogpt",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Hallucination Leaderboard",
        "description": "Public LLM leaderboard computed using Vectara's Hallucination Evaluation Model.",
        "url": "https://github.com/vectara/hallucination-leaderboard",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "Haystack",
        "description": "Haystack is an end-to-end NLP framework that enables you to build applications powered by LLMs, Transformer models, vector search and more.",
        "url": "https://github.com/deepset-ai/haystack",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "hlb-GPT",
        "description": "Minimalistic, fast, and experimentation-friendly researcher's toolbench for GPT-like models in <350 lines of code. Reaches <3.8 validation loss on wikitext-103 on a single A100 in just over 3 minutes.",
        "url": "https://github.com/tysam-code/hlb-gpt",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Horovod",
        "description": "Horovod is a distributed deep learning training framework for TensorFlow, Keras, PyTorch, and Apache MXNet",
        "url": "https://github.com/horovod/horovod",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "HumanEval-X",
        "description": "HumanEval-X is a benchmark for evaluating the multilingual ability of code generative models. It consists of 820 high-quality human-crafted data samples (each with test cases) in Python, C++, Java, JavaScript, and Go, and can be used for various tasks, such as code generation and translation.",
        "url": "https://huggingface.co/datasets/THUDM/humaneval-x",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "incdbscan",
        "description": "the incremental version of the DBSCAN clustering algorithm",
        "url": "https://github.com/DataOmbudsman/incdbscan",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Initializing Models with Larger Ones",
        "description": "We introduce weight selection, a method for initializing models by selecting a subset of weights from a pretrained larger model. With no extra cost, it is effective for improving the accuracy of a smaller model and reducing its training time needed to reach a certain accuracy level.",
        "url": "https://github.com/OscarXZQ/weight-selection",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "InstructEvalImpact",
        "description": "The IMPACT dataset contains 50 human created prompts for each category, 200 in total, to test LLMs general writing ability.",
        "url": "https://huggingface.co/datasets/declare-lab/InstructEvalImpact",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "Intel Extension for Transformers",
        "description": "Build your chatbot within minutes on your favorite device; offer SOTA compression techniques for LLMs; run LLMs efficiently on Intel Platforms",
        "url": "https://github.com/intel/intel-extension-for-transformers",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "InternLM",
        "description": "InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios.",
        "url": "https://github.com/InternLM/InternLM",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "JARVIS",
        "description": "JARVIS, a system to connect LLMs with ML community.",
        "url": "https://github.com/microsoft/JARVIS",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Kernl",
        "description": "Kernl lets you run Pytorch transformer models several times faster on GPU with a single line of code, and is designed to be easily hackable. Kernl is the first OSS inference engine written in OpenAI Triton, a new language designed by OpenAI to make it easier to write GPU kernels.",
        "url": "https://github.com/ELS-RD/kernl",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "KILT",
        "description": "A Benchmark for Knowledge Intensive Language Tasks.",
        "url": "https://github.com/facebookresearch/KILT",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "L-Eval",
        "description": "L-Eval: Instituting Standardized Evaluation for Long Context Language Models.",
        "url": "https://github.com/openlmlab/leval",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "labml.ai Deep Learning Paper Implementations",
        "description": "60 Implementations/tutorials of deep learning papers with side-by-side notes.",
        "url": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "Lamini",
        "description": "Official repo for Lamini's data generator for generating instructions to train instruction-following LLMs.",
        "url": "https://github.com/lamini-ai/lamini",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Lance",
        "description": "Lance is a modern columnar data format that is optimized for ML workflows and datasets. Convert from parquet in 2-lines of code for 100x faster random access, a vector index, data versioning, and more. Compatible with pandas, duckdb, polars, pyarrow, with more integrations on the way.",
        "url": "https://github.com/lancedb/lance",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LangChain",
        "description": "Building applications with LLMs through composability.",
        "url": "https://github.com/hwchase17/langchain",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Language Models",
        "description": "Python building blocks to explore large language models on any computer with 512MB of RAM.",
        "url": "https://github.com/jncraton/languagemodels",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LASSL",
        "description": "LASSL is a LAnguage framework for Self-Supervised Learning. LASSL aims to provide an easy-to-use framework for pretraining language model by only using Huggingface's Transformers and Datasets.",
        "url": "https://github.com/lassl/lassl",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Latent Diffusion Models",
        "description": "High-Resolution Image Synthesis with Latent Diffusion Models, contains: Text-to-Image, Inpainting",
        "url": "https://github.com/CompVis/latent-diffusion",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Levanter",
        "description": "Levanter is a framework for training large language models (LLMs) and other foundation models that strives for legibility, scalability, and reproducibility.",
        "url": "https://github.com/stanford-crfm/levanter",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "LiBai",
        "description": "LiBai is a large-scale open-source model training toolbox based on OneFlow",
        "url": "https://github.com/Oneflow-Inc/libai",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "LightEval",
        "description": "LightEval is a lightweight LLM evaluation suite that Hugging Face has been using internally with the recently released LLM data processing library datatrove and LLM training library nanotron.",
        "url": "https://github.com/huggingface/lighteval",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LightLLM",
        "description": "LightLLM is a Python-based LLM (Large Language Model) inference and serving framework, notable for its lightweight design, easy scalability, and high-speed performance.",
        "url": "https://github.com/ModelTC/lightllm",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "LightSeq",
        "description": "Official repository for LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. LightSeq achieves up to 2x faster, 2-8x longer sequences vs Megatron-LM on 16 80GB A100s.",
        "url": "https://github.com/RulinShao/LightSeq",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "LIMA",
        "description": "Dataset for LIMA: Less Is More for Alignment.",
        "url": "https://huggingface.co/datasets/GAIR/lima",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "Lit-GPT",
        "description": "Hackable implementation of state-of-the-art open-source LLMs based on nanoGPT. Supports flash attention, 4-bit and 8-bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training.",
        "url": "https://github.com/Lightning-AI/lit-gpt",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Lit-LLaMA",
        "description": "Independent implementation of LLaMA that is fully open source under the Apache 2.0 license.",
        "url": "https://github.com/Lightning-AI/lit-llama",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LiteLLM",
        "description": "lightweight package to simplify LLM API calls - Azure, OpenAI, Cohere, Anthropic, Replicate.",
        "url": "https://github.com/BerriAI/litellm",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Llama 2 Everywhere (Unikraft-centric setup)",
        "description": "Standalone, Binary Portable, Bootable Llama 2. This is a Unikraft-centric setup of Llama 2 Everywhere (L2E).",
        "url": "https://github.com/unikraft/app-llama2-c",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LLaMA-Adapter",
        "description": "LLaMA-Adapter is a lightweight adaption method for fine-tuning instruction-following LLaMA models, using 52K data provided by Stanford Alpaca.",
        "url": "https://github.com/ZrrSkywalker/LLaMA-Adapter",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "llama.cpp",
        "description": "Inference of LLaMA model in pure C/C++.",
        "url": "https://github.com/ggerganov/llama.cpp",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LLaMA2-Accessory",
        "description": "LLaMA2-Accessory is an open-source toolkit for pre-training, fine-tuning and deployment of Large Language Models (LLMs) and mutlimodal LLMs.",
        "url": "https://github.com/Alpha-VLLM/LLaMA2-Accessory",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "llamafile",
        "description": "llamafile lets you distribute and run LLMs with a single file.",
        "url": "https://github.com/Mozilla-Ocho/llamafile",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "LlamaIndex",
        "description": "LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM's with external data.",
        "url": "https://github.com/jerryjliu/llama_index",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "llm",
        "description": "llm is a Rust ecosystem of libraries for running inference on large language models, inspired by llama.cpp.",
        "url": "https://github.com/rustformers/llm",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LLM decontaminator",
        "description": "Use LLM decontaminator to quantify a dataset's rephrased samples relative to a benchmark.",
        "url": "https://github.com/lm-sys/llm-decontaminator",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LLM Foundry",
        "description": "This repository contains code for training, finetuning, evaluating, and deploying LLMs for inference with Composer and the MosaicML platform.",
        "url": "https://github.com/mosaicml/llm-foundry",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "LLMLingua",
        "description": "LLMLingua utilizes a compact, well-trained language model (e.g., GPT2-small, LLaMA-7B) to identify and remove non-essential tokens in prompts",
        "url": "https://github.com/microsoft/LLMLingua",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LLMParser",
        "description": "LLMParser is a simple and flexible tool to classify and extract structured data from text with large language models.",
        "url": "https://github.com/kyang6/llmparser",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "lm-evaluation-harness",
        "description": "A framework for few-shot evaluation of autoregressive language models.",
        "url": "https://github.com/EleutherAI/lm-evaluation-harness",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LMFlow",
        "description": "An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.",
        "url": "https://github.com/OptimalScale/LMFlow",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "LMSYS-Chat",
        "description": "This dataset contains one million real-world conversations with 25 state-of-the-art LLMs.",
        "url": "https://huggingface.co/datasets/lmsys/lmsys-chat-1m",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "LOMO",
        "description": "LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage.",
        "url": "https://arxiv.org/abs/2306.09782",
        "link_type": "Optimizer links",
        "manual_validated": false
    },
    {
        "name": "LongQLoRA",
        "description": "LongQLoRA is a memory-efficient and effective method to extend context length of Large Language Models with less training GPUs.",
        "url": "https://github.com/yangjianxin1/LongQLoRA",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "LongT5",
        "description": "LongT5 is an extension of the T5 model that handles long sequence inputs more efficiently.",
        "url": "https://github.com/google-research/longt5",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Lookahead Decoding",
        "description": "We introduce lookahead decoding - a parallel decoding algorithm to accelerate LLM inference.",
        "url": "https://github.com/hao-ai-lab/LookaheadDecoding",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LoRAX",
        "description": "LoRAX (LoRA eXchange) is a framework that allows users to serve thousands of fine-tuned models on a single GPU, dramatically reducing the cost of serving without compromising on throughput or latency.",
        "url": "https://github.com/predibase/lorax",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Lovely Tensors",
        "description": "Tensors, ready for human consumption.",
        "url": "https://github.com/xl0/lovely-tensors",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "LQ-LoRA",
        "description": "LQ-LoRA: Low-rank plus Quantized Matrix Decomposition for Efficient Language Model Finetuning",
        "url": "https://github.com/HanGuo97/lq-lora",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "M3E",
        "description": "Moka Massive Mixed Embedding",
        "url": "https://github.com/wangyingdong/m3e-base",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Macaw-LLM",
        "description": "Macaw-LLM is an exploratory endeavor that pioneers multi-modal language modeling by seamlessly combining image, video, audio, and text data, built upon the foundations of CLIP, Whisper, and LLaMA.",
        "url": "https://github.com/lyuchenyang/Macaw-LLM",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Mamba-Chat",
        "description": "Mamba-Chat is the first chat language model based on a state-space model architecture, not a transformer.",
        "url": "https://github.com/havenhq/mamba-chat",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Marvin",
        "description": "Meet Marvin: a batteries-included library for building AI-powered software. Marvin's job is to integrate AI directly into your codebase by making it look and feel like any other function.",
        "url": "https://github.com/PrefectHQ/marvin",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "MASSIVE",
        "description": "MASSIVE is a parallel dataset of > 1M utterances across 52 languages with annotations for the Natural Language Understanding tasks of intent prediction and slot annotation.",
        "url": "https://github.com/alexa/massive",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "MathInstruct",
        "description": "MathInstruct is a meticulously curated instruction tuning dataset that is lightweight yet generalizable.",
        "url": "https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "MaxText",
        "description": "MaxText is a high performance, arbitrarily scalable, open-source, simple, easily forkable, well-tested, batteries included LLM written in pure Python/Jax and targeting Google Cloud TPUs.",
        "url": "https://github.com/google/maxtext",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Medusa",
        "description": "Medusa is a simple framework that democratizes the acceleration techniques for LLM generation with multiple decoding heads.",
        "url": "https://github.com/FasterDecoding/Medusa",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "MegaBlocks",
        "description": "MegaBlocks is a light-weight library for mixture-of-experts (MoE) training.",
        "url": "https://github.com/stanford-futuredata/megablocks",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Merlin Dataloader",
        "description": "The merlin dataloader lets you rapidly load tabular data for training deep leaning models with TensorFlow, PyTorch or JAX",
        "url": "https://github.com/NVIDIA-Merlin/dataloader",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "metaseq",
        "description": "A codebase for working with Open Pre-trained Transformers.",
        "url": "https://github.com/facebookresearch/metaseq",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "microagents",
        "description": "microagents: Modular Agents Capable of Self-Editing Their Prompts and Python code",
        "url": "https://github.com/aymenfurter/microagents",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "minbpe",
        "description": "Minimal, clean code for the (byte-level) Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.",
        "url": "https://github.com/karpathy/minbpe",
        "link_type": "Vocabulary links",
        "manual_validated": false
    },
    {
        "name": "MiniChain",
        "description": "MiniChain is a tiny library for coding with large language models.",
        "url": "https://github.com/srush/MiniChain",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "MiniLLM",
        "description": "MiniLLM is a minimal system for running modern LLMs on consumer-grade GPUs.",
        "url": "https://github.com/kuleshov/minillm",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Mixtral offloading",
        "description": "Run Mixtral-8x7B models in Colab or consumer desktops",
        "url": "https://github.com/dvmazur/mixtral-offloading",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "ML Papers Explained",
        "description": "List of LM papers explained.",
        "url": "https://github.com/dair-ai/ML-Papers-Explained",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "MLC LLM",
        "description": "Enable everyone to develop, optimize and deploy AI models natively on everyone's devices.",
        "url": "https://github.com/mlc-ai/mlc-llm",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "MLX",
        "description": "MLX is an array framework for machine learning on Apple silicon, brought to you by Apple machine learning research.",
        "url": "https://github.com/ml-explore/mlx",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "MMF",
        "description": "MMF is a modular framework for vision and language multimodal research",
        "url": "https://github.com/facebookresearch/mmf",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "ModelScope",
        "description": "ModelScope is built upon the notion of “Model-as-a-Service” (MaaS). It seeks to bring together most advanced machine learning models from the AI community, and streamlines the process of leveraging AI models in real-world applications. The core ModelScope library open-sourced in this repository provides the interfaces and implementations that allow developers to perform model inference, training and evaluation.",
        "url": "https://github.com/modelscope/modelscope",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "ModuleFormer",
        "description": "ModuleFormer is a MoE-based architecture that includes two different types of experts: stick-breaking attention heads and feedforward experts. We released a collection of ModuleFormer-based Language Models (MoLM) ranging in scale from 4 billion to 8 billion parameters.",
        "url": "https://github.com/IBM/ModuleFormer",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Mosaic composer",
        "description": "Composer is a PyTorch library that enables you to train neural networks faster, at lower cost, and to higher accuracy. Train neural networks up to 7x faster.",
        "url": "https://github.com/mosaicml/composer",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Mosaic streaming",
        "description": "Fast, accurate streaming of training data from cloud storage.",
        "url": "https://github.com/mosaicml/streaming",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "MosaicML Examples",
        "description": "This repo contains reference examples for training ML models quickly and to high accuracy. MosaicBERT",
        "url": "https://github.com/mosaicml/examples",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "MoV and MoLoRA",
        "description": "This repository contains the official code for the paper: \"Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning.\"",
        "url": "https://github.com/for-ai/parameter-efficient-moe",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "MS-AMP",
        "description": "MS-AMP is an automatic mixed precision package for deep learning developed by Microsoft.",
        "url": "https://github.com/azure/ms-amp",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "MultiLegalPile",
        "description": "The Multi_Legal_Pile is a large-scale multilingual legal dataset suited for pretraining language models. It spans over 24 languages and five legal text types.",
        "url": "https://huggingface.co/datasets/joelito/Multi_Legal_Pile",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "multimodal",
        "description": "TorchMultimodal is a PyTorch library for training state-of-the-art multimodal multi-task models at scale.",
        "url": "https://github.com/facebookresearch/multimodal",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Multimodal-GPT",
        "description": "Train a multi-modal chatbot with visual and language instructions!",
        "url": "https://github.com/open-mmlab/Multimodal-GPT",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "mup",
        "description": "Maximal Update Parametrization (μP) and Hyperparameter Transfer (μTransfer): Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer",
        "url": "https://github.com/microsoft/mup",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "nanoGPT",
        "description": "The simplest, fastest repository for training/finetuning medium-sized GPTs.",
        "url": "https://github.com/karpathy/nanoGPT",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "nanoPALM",
        "description": "nanoPALM is the simplest, fastest repository for training/finetuning small to medium-sized PALM models.",
        "url": "https://github.com/RobertRiachi/nanoPALM",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "nanoT5",
        "description": "Fast & Simple repository for pre-training and fine-tuning T5-style models.",
        "url": "https://github.com/PiotrNawrot/nanoT5",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "nanotron",
        "description": "The objective of this library is to provide easy distributed primitives in order to train a variety of models efficiently using 3D parallelism.",
        "url": "https://github.com/huggingface/nanotron",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "nebullvm",
        "description": "Nebullvm is an ecosystem of plug and play modules to optimize the performances of your AI systems. The optimization modules are stack-agnostic and work with any library. They are designed to be easily integrated into your system, providing a quick and seamless boost to its performance. Simply plug and play to start realizing the benefits of optimized performance right away.",
        "url": "https://github.com/nebuly-ai/nebullvm",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "NEFTune",
        "description": "Noisy Embedding Instruction Fine Tuning (NEFTune) add random noise to the embedding vectors of the training data during the forward pass of fine-tuning.",
        "url": "https://github.com/neelsjain/NEFTune",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "NVIDIA Megatron-LM",
        "description": "Megatron is an efficient, model-parallel (tensor, sequence, and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.",
        "url": "https://github.com/NVIDIA/Megatron-LM",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "NVIDIA Merlin",
        "description": "NVIDIA Merlin is an open source library providing end-to-end GPU-accelerated recommender systems, from feature engineering and preprocessing to training deep learning models and running inference in production.",
        "url": "https://github.com/NVIDIA-Merlin/Merlin",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "NVIDIA NeMo",
        "description": "NVIDIA NeMo is a conversational AI toolkit built for researchers working on automatic speech recognition (ASR), text-to-speech synthesis (TTS), large language models (LLMs), and natural language processing (NLP).",
        "url": "https://github.com/NVIDIA/NeMo",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "NVIDIA Nsight Systems",
        "description": "NVIDIA Nsight™ Systems is a system-wide performance analysis tool designed to visualize an application’s algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs, from large servers to our smallest system on a chip (SoC).",
        "url": "https://developer.nvidia.com/nsight-systems",
        "link_type": "GPU profiling links",
        "manual_validated": false
    },
    {
        "name": "Nvidia TensorRT",
        "description": "TensorRT is an SDK for high-performance deep learning inference, includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications.",
        "url": "https://github.com/NVIDIA/TensorRT",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "NVIDIA Visual Profiler",
        "description": "The NVIDIA Visual Profiler is a cross-platform performance profiling tool that delivers developers vital feedback for optimizing CUDA C/C++ applications.",
        "url": "https://developer.nvidia.com/nvidia-visual-profiler",
        "link_type": "GPU profiling links",
        "manual_validated": false
    },
    {
        "name": "NVIDIA/NVTX",
        "description": "The NVIDIA® Tools Extension SDK (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications.",
        "url": "https://github.com/NVIDIA/NVTX",
        "link_type": "GPU profiling links",
        "manual_validated": false
    },
    {
        "name": "OctoPack",
        "description": "OctoPack: Instruction Tuning Code Large Language Models",
        "url": "https://github.com/bigcode-project/octopack",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Ollama",
        "description": "Run, create, and share large language models (LLMs).",
        "url": "https://github.com/jmorganca/ollama",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "OmniQuant",
        "description": "OmniQuant is a simple and powerful quantization technique for LLMs.",
        "url": "https://github.com/OpenGVLab/OmniQuant",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Opacus",
        "description": "Opacus is a library that enables training PyTorch models with differential privacy. It supports training with minimal code changes required on the client, has little impact on training performance, and allows the client to online track the privacy budget expended at any given moment.",
        "url": "https://github.com/pytorch/opacus",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Open LLMetry",
        "description": "Open-source observability for your LLM application",
        "url": "https://github.com/traceloop/openllmetry",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Open LLMs",
        "description": "A list of open LLMs available for commercial use.",
        "url": "https://github.com/eugeneyan/open-llms",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "OpenChatKit",
        "description": "OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications. The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories.",
        "url": "https://github.com/togethercomputer/OpenChatKit",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "OpenELM",
        "description": "OpenELM is an open-source library by CarperAI, designed to enable evolutionary search with language models in both code and natural language.",
        "url": "https://github.com/CarperAI/OpenELM",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "OpenFlamingo",
        "description": "An open-source framework for training large multimodal models.",
        "url": "https://github.com/mlfoundations/open_flamingo",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "OpenLLM",
        "description": "An open platform for operating large language models (LLMs) in production. Fine-tune, serve, deploy, and monitor any LLMs with ease.",
        "url": "https://github.com/bentoml/OpenLLM",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "OpenLM",
        "description": "OpenLM is a minimal but performative language modeling (LM) repository, aimed to facilitate research on medium sized LMs.",
        "url": "https://github.com/mlfoundations/open_lm",
        "link_type": "Model links",
        "manual_validated": true
    },
    {
        "name": "OpenLM",
        "description": "Drop-in OpenAI-compatible library that can call LLMs from other providers (e.g., HuggingFace, Cohere, and more).",
        "url": "https://github.com/r2d4/openlm",
        "link_type": "Utils links",
        "manual_validated": true
    },
    {
        "name": "OpenMoE",
        "description": "OpenMoE is a project aimed at igniting the open-source MoE community! We are releasing a family of open-sourced Mixture-of-Experts (MoE) Large Language Models.",
        "url": "https://github.com/XueFuzhao/OpenMoE",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "OpenPipe",
        "description": "Test and deploy your LLM prompts in a data-driven way on an open-source and self-hostable platform.",
        "url": "https://github.com/OpenPipe/OpenPipe",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "openplayground",
        "description": "An LLM playground you can run on your laptop.",
        "url": "https://github.com/nat/openplayground",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Optax",
        "description": "Optax is a gradient processing and optimization library for JAX.",
        "url": "https://github.com/google-deepmind/optax",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "OSCAR (Open Super-large Crawled ALMAnaCH coRpus)",
        "description": "OSCAR or Open Super-large Crawled Aggregated coRpus is a huge multilingual corpus obtained by language classification and filtering of the Common Crawl corpus using the Ungoliant architecture.",
        "url": "https://oscar-corpus.com",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "OSLO",
        "description": "OSLO is a framework that provides various GPU based optimization technologies for large-scale modeling. Features like 3D parallelism and kernel fusion which could be useful when training a large model are the key features.",
        "url": "https://github.com/EleutherAI/oslo",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Otter",
        "description": "Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning.",
        "url": "https://github.com/Luodian/otter",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Outlines",
        "description": "Outlines is a library for neural text generation.",
        "url": "https://github.com/normal-computing/outlines",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "P3",
        "description": "P3 (Public Pool of Prompts) is a collection of prompted English datasets covering a diverse set of NLP tasks.",
        "url": "https://huggingface.co/datasets/bigscience/P3",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "PaLM",
        "description": "An open-source implementation of Google's PaLM models.",
        "url": "https://github.com/conceptofmind/PaLM",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "PaLM + RLHF",
        "description": "Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM.",
        "url": "https://github.com/lucidrains/PaLM-rlhf-pytorch",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "PEFT",
        "description": "Parameter-Efficient Fine-Tuning (PEFT) methods enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters.",
        "url": "https://github.com/huggingface/peft",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "PG-19 Language Modelling Benchmark",
        "description": "This repository contains the PG-19 language modeling benchmark. It includes a set of books extracted from the Project Gutenberg books library, that were published before 1919. It also contains metadata of book titles and publication dates.",
        "url": "https://github.com/deepmind/pg19",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "PowerInfer",
        "description": "PowerInfer is a CPU/GPU LLM inference engine leveraging activation locality for your device.",
        "url": "https://github.com/SJTU-IPADS/PowerInfer",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "PRESTO",
        "description": "PRESTO is a dataset of over 550K contextual multilingual conversations between humans and virtual assistants. PRESTO contains a diverse array of challenges that occur in real-world NLU tasks such as disfluencies, code-switching, and revisions.",
        "url": "https://github.com/google-research-datasets/presto",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "Pretrained Language Model",
        "description": "Pretrained language model and its related optimization techniques developed by Huawei Noah's Ark Lab.",
        "url": "https://github.com/huawei-noah/Pretrained-Language-Model",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "privateGPT",
        "description": "Interact privately with your documents using the power of GPT, 100% privately, no data leaks.",
        "url": "https://github.com/imartinez/privateGPT",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "PromptBench",
        "description": "PromptBench: A Unified Library for Evaluating and Understanding Large Language Models.",
        "url": "https://github.com/microsoft/promptbench",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "PROSE Public Benchmark Suite",
        "description": "PROSE Public Benchmark Suite contains benchmarks drawn from three classes of tasks: string-to-string transformation, text-to-table transformation, substring extraction from semi-structured text, syntax (and some semantic) program repair for code that can be achieved with few edits.",
        "url": "https://github.com/microsoft/prose-benchmarks",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "pybaselines",
        "description": "A Python library of algorithms for the baseline correction of experimental data.",
        "url": "https://github.com/derb12/pybaselines",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "pykoi",
        "description": "pykoi is an open-source python library for LLMs providing a unified interface for data & feedback collection, RLHF, and model comparisons.",
        "url": "https://github.com/CambioML/pykoi",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "PyTorch Geometric",
        "description": "PyG (PyTorch Geometric) is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs) for a wide range of applications related to structured data.",
        "url": "https://github.com/pyg-team/pytorch_geometric",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "PyTorch Profiler",
        "description": "This recipe explains how to use PyTorch profiler and measure the time and memory consumption of the model’s operators.",
        "url": "https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html",
        "link_type": "GPU profiling links",
        "manual_validated": false
    },
    {
        "name": "QLoRA",
        "description": "QLoRA uses bitsandbytes for quantization and is integrated with Hugging Face's PEFT and transformers libraries.",
        "url": "https://github.com/artidoro/qlora",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "QMoE",
        "description": "Code for the paper \"QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models\".",
        "url": "https://github.com/IST-DASLab/qmoe",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "RasaGPT",
        "description": "RasaGPT is the first headless LLM chatbot platform built on top of Rasa and Langchain. Built w/ Rasa, FastAPI, Langchain, LlamaIndex, SQLModel, pgvector, ngrok, telegram.",
        "url": "https://github.com/paulpierre/RasaGPT",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "ReCoRD",
        "description": "This repository contains a script to generate question/answer pairs using CNN and Daily Mail articles downloaded from the Wayback Machine.",
        "url": "https://github.com/deepmind/rc-data",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "RedPajama-Data",
        "description": "RedPajama-Data: An Open Source Recipe to Reproduce LLaMA training dataset. This repo contains a reproducible data receipe for the RedPajama data.",
        "url": "https://github.com/togethercomputer/RedPajama-Data",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "RedPajama-V2",
        "description": "RedPajama-V2 is an open dataset for training large language models. The dataset includes over 100B text documents coming from 84 CommonCrawl snapshots and processed using the CCNet pipeline.",
        "url": "https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "RWKV",
        "description": "RWKV is a RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable). So it's combining the best of RNN and transformer: great performance, fast inference, saves VRAM, fast training, \"infinite\" ctx_len, and free sentence embedding.",
        "url": "https://github.com/BlinkDL/RWKV-LM",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "S-LoRA",
        "description": "S-LoRA: Serving Thousands of Concurrent LoRA Adapters",
        "url": "https://github.com/S-LoRA/S-LoRA",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "safari",
        "description": "Convolutions for Sequence Modeling, contains: Hyena, Long Convs, Hungry Hungry Hippos (H3).",
        "url": "https://github.com/HazyResearch/safari",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "SapientML",
        "description": "SapientML is an AutoML technology that can learn from a corpus of existing datasets and their human-written pipelines, and efficiently generate a high-quality pipeline for a predictive task on a new dataset.",
        "url": "https://github.com/sapientml/sapientml",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Seahorse",
        "description": "Seahorse is a dataset for multilingual, multifaceted summarization evaluation. It contains 96K summaries with human ratings along 6 quality dimensions: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4 datasets.",
        "url": "https://github.com/google-research-datasets/seahorse",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "Self-Instruct",
        "description": "Self-Instruct is a framework that helps language models improve their ability to follow natural language instructions.",
        "url": "https://huggingface.co/datasets/yizhongw/self_instruct",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "Semantic Kernel",
        "description": "Semantic Kernel (SK) is a lightweight SDK enabling integration of AI Large Language Models (LLMs) with conventional programming languages.",
        "url": "https://github.com/microsoft/semantic-kernel",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "SentencePiece",
        "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) and unigram language model ) with the extension of direct training from raw sentences.",
        "url": "https://github.com/google/sentencepiece",
        "link_type": "Vocabulary links",
        "manual_validated": false
    },
    {
        "name": "SeqIO",
        "description": "Task-based datasets, preprocessing, and evaluation for sequence models.",
        "url": "https://github.com/google/seqio",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "SGLang",
        "description": "SGLang is a structured generation language designed for large language models (LLMs). It makes your interaction with LLMs faster and more controllable by co-designing the frontend language and the runtime system.",
        "url": "https://github.com/sgl-project/sglang/",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "simpletransformers",
        "description": "This library is based on the Transformers library by HuggingFace. Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model.",
        "url": "https://github.com/ThilinaRajapakse/simpletransformers",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Simplified Transformers",
        "description": "A simple design recipe for deep Transformers is to compose identical building blocks.",
        "url": "https://github.com/bobby-he/simplified_transformers",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "skorch",
        "description": "A scikit-learn compatible neural network library that wraps PyTorch.",
        "url": "https://github.com/skorch-dev/skorch",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "SlimPajama",
        "description": "The dataset consists of 59166 jsonl files and is ~895GB compressed. It is a cleaned and deduplicated version of Together's RedPajama.",
        "url": "https://huggingface.co/datasets/cerebras/SlimPajama-627B",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "slowllama",
        "description": "Fine-tune Llama2 and CodeLLama models, including 70B/35B on Apple M1/M2 devices (for example, Macbook Air or Mac Mini) or consumer nVidia GPUs.",
        "url": "https://github.com/okuvshynov/slowllama",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "small-text",
        "description": "Small-Text provides state-of-the-art Active Learning for Text Classification.",
        "url": "https://github.com/webis-de/small-text",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Sockeye",
        "description": "Sockeye is an open-source sequence-to-sequence framework for Neural Machine Translation built on PyTorch.",
        "url": "https://github.com/awslabs/sockeye",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Sophia",
        "description": "Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training",
        "url": "https://arxiv.org/abs/2305.14342",
        "link_type": "Optimizer links",
        "manual_validated": false
    },
    {
        "name": "SparseML",
        "description": "Libraries for applying sparsification recipes to neural networks with a few lines of code, enabling faster and smaller models.",
        "url": "https://github.com/neuralmagic/sparseml",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "SparseZoo",
        "description": "Neural network model repository for highly sparse and sparse-quantized models with matching sparsification recipes.",
        "url": "https://github.com/neuralmagic/sparsezoo",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "SpikeGPT",
        "description": "SpikeGPT is a lightweight generative language model with pure binary, event-driven spiking activation units.",
        "url": "https://github.com/ridgerchu/SpikeGPT",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "srf-attention",
        "description": "Simplex Random Feature attention, in PyTorch",
        "url": "https://github.com/notarussianteenager/srf-attention",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "StableLM",
        "description": "StableLM: Stability AI Language Models",
        "url": "https://github.com/stability-AI/stableLM",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Stanford Alpaca",
        "description": "This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model.",
        "url": "https://github.com/tatsu-lab/stanford_alpaca",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "StarCoder",
        "description": "Home of StarCoder: fine-tuning & inference!",
        "url": "https://github.com/bigcode-project/starcoder",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "state-spaces",
        "description": "Sequence Modeling with Structured State Spaces, contains: S4D, HTTYH, SaShiMi, S4, LSSL, HiPPO.",
        "url": "https://github.com/HazyResearch/state-spaces",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "StreamingLLM",
        "description": "Efficient Streaming Language Models with Attention Sinks",
        "url": "https://github.com/mit-han-lab/streaming-llm",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "string2string",
        "description": "The string2string library is an open-source tool that offers a comprehensive suite of efficient algorithms for a broad range of string-to-string problems. It includes both traditional algorithmic solutions and recent advanced neural approaches to address various problems in pairwise string alignment, distance measurement, lexical and semantic search, and similarity analysis. Additionally, the library provides several helpful visualization tools and metrics to facilitate the interpretation and analysis of these methods.",
        "url": "https://github.com/stanfordnlp/string2string",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Superagent",
        "description": "Superagent is a powerful tool that simplifies the configuration and deployment of LLM (Large Language Model) Agents to production.",
        "url": "https://github.com/homanp/superagent",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "surgeon-pytorch",
        "description": "A library to inspect and extract intermediate layers of PyTorch models.",
        "url": "https://github.com/archinetai/surgeon-pytorch",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "SwitchHead",
        "description": "The official repository for our paper \"SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention\".",
        "url": "https://github.com/robertcsordas/moe_attention",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "t5-experiments",
        "description": "Tools and scripts for experimenting with Transformers: Bert, T5.",
        "url": "https://github.com/yurakuratov/t5-experiments",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "t5x",
        "description": "T5X is a modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models (starting with language) at many scales.",
        "url": "https://github.com/google-research/t5x",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Tanuki",
        "description": "Easily build LLM-powered apps that get cheaper and faster over time.",
        "url": "https://github.com/Tanuki/tanuki.py",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "tasknet",
        "description": "Integration of HuggingFace Datasets with HuggingFace Trainer, and multitasking.",
        "url": "https://github.com/sileod/tasknet",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "tbparse",
        "description": "A simple yet powerful tensorboard event log parser/reader.",
        "url": "https://github.com/j3soon/tbparse",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "TensorRT-LLM",
        "description": "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs.",
        "url": "https://github.com/NVIDIA/TensorRT-LLM",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Text Generation Inference",
        "description": "Text Generation Inference is a Rust, Python and gRPC server for text generation inference.",
        "url": "https://github.com/huggingface/text-generation-inference",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Text Generation Inference (fork, free)",
        "description": "Text Generation Inference is a Rust, Python and gRPC server for text generation inference.",
        "url": "https://github.com/Preemo-Inc/text-generation-inference",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "TextBox",
        "description": "TextBox 2.0 is a text generation library with pre-trained language models.",
        "url": "https://github.com/RUCAIBox/TextBox",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "TextReducer",
        "description": "TextReducer is a tool for summarization and information extraction powered by the SentenceTransformer library.",
        "url": "https://github.com/helliun/targetedSummarization",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "The Alignment Handbook",
        "description": "obust recipes for to align language models with human and AI preferences.",
        "url": "https://github.com/huggingface/alignment-handbook",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "The Practical Guides for Large Language Models",
        "description": "A curated (still actively updated) list of practical guide resources of LLMs.",
        "url": "https://github.com/Mooler0410/LLMsPracticalGuide",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "TigerLab",
        "description": "Tiger toolkit (TigerRag, TigerTune, TigerDA, TigerArmor) as an open-source resource for developers to create AI models and language applications tailored to their specific needs.",
        "url": "https://github.com/tigerlab-ai/tiger",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "tiktoken",
        "description": "tiktoken is a fast BPE tokeniser for use with OpenAI's models.",
        "url": "https://github.com/openai/tiktoken",
        "link_type": "Vocabulary links",
        "manual_validated": false
    },
    {
        "name": "tiny-codes",
        "description": "This synthetic dataset is a collection of 1.6 millions short and clear code snippets that can help LLM models learn how to reason with both natural and programming languages.",
        "url": "https://huggingface.co/datasets/nampdn-ai/tiny-codes",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "tinygrad",
        "description": "tinygrad is an extremely simple deep-learning framework. It supports both inference and training.",
        "url": "https://github.com/geohot/tinygrad",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "tokenizers",
        "description": "Provides an implementation of today's most used tokenizers, with a focus on performance and versatility.",
        "url": "https://github.com/huggingface/tokenizers",
        "link_type": "Vocabulary links",
        "manual_validated": false
    },
    {
        "name": "TokenMonster",
        "description": "TokenMonster is an ungreedy subword tokenizer and vocabulary generator, enabling language models to run faster, cheaper, smarter and generate longer streams of text.",
        "url": "https://github.com/alasdairforsythe/tokenmonster",
        "link_type": "Vocabulary links",
        "manual_validated": false
    },
    {
        "name": "TorchBench",
        "description": "TorchBench is a collection of open source benchmarks used to evaluate PyTorch performance.",
        "url": "https://github.com/pytorch/benchmark",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "torchdistX",
        "description": "Torch Distributed Experimental",
        "url": "https://github.com/pytorch/torchdistx",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "torchegranate",
        "description": "Fast, flexible and easy to use probabilistic modelling with PyTorch.",
        "url": "https://github.com/jmschrei/torchegranate",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "torchnnprofiler",
        "description": "Context Manager to profile the forward and backward times of PyTorch's nn.Module",
        "url": "https://github.com/kshitij12345/torchnnprofiler",
        "link_type": "GPU profiling links",
        "manual_validated": false
    },
    {
        "name": "TorchScale",
        "description": "TorchScale is a PyTorch library that allows researchers and developers to scale up Transformers efficiently and effectively. It has the implementation of fundamental research to improve modeling generality and capability, as well as training stability and efficiency of scaling Transformers.",
        "url": "https://github.com/microsoft/torchscale",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Training BERT with Compute/Time (Academic) Budget",
        "description": "Repository contains scripts for pre-training and finetuning BERT-like models with limited time and compute budget.",
        "url": "https://github.com/IntelLabs/academic-budget-bert",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Trankit",
        "description": "A Light-Weight Transformer-based Python Toolkit for Multilingual Natural Language Processing",
        "url": "https://github.com/nlp-uoregon/trankit",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Transformer",
        "description": "Explanation of Transformer architecture from the code.",
        "url": "https://nn.labml.ai/transformers/index.html",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "Transformer models: an introduction and catalog, 2023 Edition",
        "description": "An introduction and catalog of Transformer models.",
        "url": "https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "transformer-deploy",
        "description": "Efficient, scalable and enterprise-grade CPU/GPU inference server for Hugging Face transformer models.",
        "url": "https://github.com/ELS-RD/transformer-deploy",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "transformers",
        "description": "State-of-the-art Natural Language Processing for PyTorch and TensorFlow 2.0",
        "url": "https://github.com/huggingface/transformers",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Transformers from Scratch",
        "description": "Explanation of Transformer architecture.",
        "url": "https://e2eml.school/transformers.html",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "Tricksy",
        "description": "Fast approximate inference on a single GPU with sparsity aware offloading",
        "url": "https://github.com/austinsilveria/tricksy",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "tsai",
        "description": "tsai is an open-source deep learning package built on top of Pytorch & fastai focused on state-of-the-art techniques for time series tasks like classification, regression, forecasting, imputation.",
        "url": "https://github.com/timeseriesAI/tsai",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "txtai",
        "description": "txtai executes machine-learning workflows to transform data and build AI-powered semantic search applications.",
        "url": "https://github.com/neuml/txtai",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "UltraChat",
        "description": "UltraChat is an open-source, large-scale, and multi-round dialogue data powered by Turbo APIs.",
        "url": "https://huggingface.co/datasets/stingning/ultrachat",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "UltraFeedback",
        "description": "UltraFeedback is a large-scale, fine-grained, diverse preference dataset, used for training powerful reward models and critic models.",
        "url": "https://huggingface.co/datasets/openbmb/UltraFeedback",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch",
        "description": "Understanding how self-attention works from scratch.",
        "url": "https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html",
        "link_type": "Documentation links",
        "manual_validated": false
    },
    {
        "name": "unilm",
        "description": "Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities.",
        "url": "https://github.com/microsoft/unilm",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "unsloth",
        "description": "2-5x faster 60% less memory local QLoRA finetuning",
        "url": "https://github.com/unslothai/unsloth",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "UpTrain",
        "description": "UpTrain is an open-source, data-secure tool for ML practitioners to observe and refine their ML models by monitoring their performance, checking for (data) distribution shifts, and collecting edge cases to retrain them upon.",
        "url": "https://github.com/uptrain-ai/uptrain",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "Vizro",
        "description": "Vizro is a toolkit for creating modular data visualization applications.",
        "url": "https://github.com/mckinsey/vizro",
        "link_type": "Visualization links",
        "manual_validated": false
    },
    {
        "name": "vLLM",
        "description": "vLLM is a fast and easy-to-use library for LLM inference and serving.",
        "url": "https://github.com/vllm-project/vllm",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "voltaML",
        "description": "VoltaML is a lightweight library to convert and run your ML/DL deep learning models in high performance inference runtimes like TensorRT, TorchScript, ONNX and TVM.",
        "url": "https://github.com/VoltaML/voltaML",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "WeightWatcher",
        "description": "WeightWatcher (WW): is an open-source, diagnostic tool for analyzing Deep Neural Networks (DNN), without needing access to training or even test data.",
        "url": "https://github.com/CalculatedContent/WeightWatcher",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "WikiMatrix",
        "description": "WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia.",
        "url": "https://github.com/facebookresearch/LASER/tree/master/tasks/WikiMatrix",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "WizardLM",
        "description": "WizardLM: An Instruction-following LLM Using Evol-Instruct. At present, our core contributors are preparing the 65B version and we expect to empower WizardLM with the ability to perform instruction evolution itself, aiming to evolve your specific data at a low cost.",
        "url": "https://github.com/nlpxucan/WizardLM",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "word2vec",
        "description": "This tool provides an efficient implementation of the continuous bag-of-words and skip-gram architectures for computing vector representations of words.",
        "url": "https://github.com/dav/word2vec",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "x-transformers",
        "description": "A simple but complete full-attention transformer with a set of promising experimental features from various papers.",
        "url": "https://github.com/lucidrains/x-transformers",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "xFormers",
        "description": "xFormers is a modular and field agnostic library to flexibly generate transformer architectures by interoperable and optimized building blocks.",
        "url": "https://github.com/facebookresearch/xformers",
        "link_type": "Model links",
        "manual_validated": false
    },
    {
        "name": "Xinference",
        "description": "Xorbits Inference(Xinference) is a powerful and versatile library designed to serve language, speech recognition, and multimodal models. With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command.",
        "url": "https://github.com/xorbitsai/inference",
        "link_type": "Utils links",
        "manual_validated": false
    },
    {
        "name": "xP3",
        "description": "xP3 (Crosslingual Public Pool of Prompts) is a collection of prompts & datasets across 46 of languages & 16 NLP tasks.",
        "url": "https://huggingface.co/datasets/bigscience/xP3",
        "link_type": "Dataset links",
        "manual_validated": false
    },
    {
        "name": "xturing",
        "description": "xturing provides fast, efficient and simple fine-tuning of LLMs, such as LLaMA, GPT-J, GPT-2, OPT, Cerebras-GPT, Galactica, and more.",
        "url": "https://github.com/stochasticai/xturing",
        "link_type": "Utils links",
        "manual_validated": false
    }
]