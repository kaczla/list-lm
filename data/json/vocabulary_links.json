[
    {
        "name": "SentencePiece",
        "description": "SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) and unigram language model ) with the extension of direct training from raw sentences.",
        "url": "https://github.com/google/sentencepiece"
    },
    {
        "name": "tiktoken",
        "description": "tiktoken is a fast BPE tokeniser for use with OpenAI's models.",
        "url": "https://github.com/openai/tiktoken"
    },
    {
        "name": "tokenizers",
        "description": "Provides an implementation of today's most used tokenizers, with a focus on performance and versatility.",
        "url": "https://github.com/huggingface/tokenizers"
    },
    {
        "name": "TokenMonster",
        "description": "TokenMonster is an ungreedy subword tokenizer and vocabulary generator, enabling language models to run faster, cheaper, smarter and generate longer streams of text.",
        "url": "https://github.com/alasdairforsythe/tokenmonster"
    }
]