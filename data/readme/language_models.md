- MoE
  - Year: 2017
  - Publication: [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538)

- Transformer
  - Year: 2017
  - Publication: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

- RR-Transformer
  - Year: 2018
  - Publication: [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)

- GPT
  - Year: 2018
  - Publication: [Blog - Improving Language Understanding with Unsupervised Learning](https://openai.com/blog/language-unsupervised)
  - Model weights: [HuggingFace models](https://huggingface.co/openai-gpt)

- flair
  - Year: 2018
  - Publication: [Contextual String Embeddings for Sequence Labeling](https://aclanthology.org/C18-1139)
  - Code: [GitHub](https://github.com/flairNLP/flair)

- BERT
  - Year: 2018
  - Publication: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)

- Transformer-XL
  - Year: 2019
  - Publication: [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860)
  - Code: [GitHub](https://github.com/kimiyoung/transformer-xl)

- LightConv/DynamicConv
  - Year: 2019
  - Publication: [Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/abs/1901.10430)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/pay_less_attention_paper)

- Evolved Transformer
  - Year: 2019
  - Publication: [The Evolved Transformer](https://arxiv.org/abs/1901.11117)

- GPT-2
  - Year: 2019
  - Publication: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
  - Code: [GitHub](https://github.com/openai/gpt-2)

- ERNIE
  - Year: 2019
  - Publication: [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)
  - Model weights: [HuggingFace models](https://huggingface.co/PaddlePaddle/ernie-1.0-base-zh)

- Sparse Transformer
  - Year: 2019
  - Publication: [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509)

- Adaptive Span
  - Year: 2019
  - Publication: [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799)
  - Code: [GitHub](https://github.com/facebookresearch/adaptive-span)

- XLNet
  - Year: 2019
  - Publication: [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)

- All-attention network
  - Year: 2019
  - Publication: [Augmenting Self-attention with Persistent Memory](https://arxiv.org/abs/1907.01470)

- RoBERTa
  - Year: 2019
  - Publication: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/roberta)

- ERNIE 2.0
  - Year: 2019
  - Publication: [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/abs/1907.12412)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)
  - Model weights: [HuggingFace models](https://huggingface.co/PaddlePaddle/ernie-2.0-base-en)

- StructBERT
  - Year: 2019
  - Publication: [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)

- Adaptively Sparse Transformers
  - Year: 2019
  - Publication: [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015)

- CTRL
  - Year: 2019
  - Publication: [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858)
  - Code: [GitHub](https://github.com/salesforce/ctrl)

- Megatron-LM
  - Year: 2019
  - Publication: [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- ALBERT
  - Year: 2019
  - Publication: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
  - Code: [GitHub](https://github.com/google-research/ALBERT)

- kNN-LM
  - Year: 2019
  - Publication: [Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172)
  - Code: [GitHub](https://github.com/urvashik/knnlm)

- TENER
  - Year: 2019
  - Publication: [TENER: Adapting Transformer Encoder for Named Entity Recognition](https://arxiv.org/abs/1911.04474)

- Sandwich Transformers
  - Year: 2019
  - Publication: [https://arxiv.org/abs/1911.03864](https://arxiv.org/abs/1911.03864)
  - Video: [YouTube](https://www.youtube.com/watch?v=rFuuGEj3AhU)
  - Code: [GitHub](https://github.com/ofirpress/sandwich_transformer)

- BiT (Big Transfer)
  - Year: 2019
  - Publication: [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370)

- Feedback Transformer
  - Year: 2020
  - Publication: [Addressing Some Limitations of Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402)

- MiniLM
  - Year: 2020
  - Publication: [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957)

- Routing Transformer
  - Year: 2020
  - Publication: [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/abs/2003.05997)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/routing_transformer)

- Floater
  - Year: 2020
  - Publication: [Learning to Encode Position for Transformer with Continuous Dynamical Model](https://arxiv.org/abs/2003.09229)
  - Code: [GitHub](https://github.com/xuanqing94/FLOATER)

- MobileBERT
  - Year: 2020
  - Publication: [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/mobilebert)

- Poor Man's BERT
  - Year: 2020
  - Publication: [On the Effect of Dropping Layers of Pre-trained Transformer Models](https://arxiv.org/abs/2004.03844)
  - Code: [GitHub](https://github.com/hsajjad/transformers)

- Longformer
  - Year: 2020
  - Publication: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
  - Code: [GitHub](https://github.com/allenai/longformer)

- MPNet
  - Year: 2020
  - Publication: [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297)
  - Code: [GitHub](https://github.com/microsoft/MPNet)

- DeeBERT
  - Year: 2020
  - Publication: [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993)
  - Code: [GitHub](https://github.com/castorini/DeeBERT)

- Synthesizer
  - Year: 2020
  - Publication: [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743)
  - Code: [GitHub](https://github.com/leaderj1001/Synthesizer-Rethinking-Self-Attention-Transformer-Models) / [GitHub](https://github.com/10-zin/Synthesizer)

- Adaptive Transformers
  - Year: 2020
  - Publication: [Adaptive Transformers for Learning Multimodal Representations](https://arxiv.org/abs/2005.07486)
  - Code: [GitHub](https://github.com/prajjwal1/adaptive_transformer)

- DeBERTa
  - Year: 2020
  - Publication: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-base)

- Linformer
  - Year: 2020
  - Publication: [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768)
  - Code: [GitHub](https://github.com/Kyan820815/Linformer)

- BRC
  - Year: 2020
  - Publication: [A bio-inspired bistable recurrent cell allows for long-lasting memory](https://arxiv.org/abs/2006.05252)
  - Code: [GitHub](https://github.com/nvecoven/BRC) / [GitHub](https://github.com/742617000027/nBRC)

- TUPE
  - Year: 2020
  - Publication: [Rethinking Positional Encoding in Language Pre-training](https://arxiv.org/abs/2006.15595)
  - Code: [GitHub](https://github.com/guolinke/TUPE)

- DA-Transformer
  - Year: 2020
  - Publication: [DA-Transformer: Distance-aware Transformer](https://arxiv.org/abs/2010.06925)

- Informer
  - Year: 2020
  - Publication: [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436)

- Key-Value Memory Feed-Forward
  - Year: 2020
  - Publication: [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913)

- Shortformer
  - Year: 2020
  - Publication: [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832)
  - Code: [GitHub](https://github.com/ofirpress/shortformer)

- MiniLM v2
  - Year: 2020
  - Publication: [MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://arxiv.org/abs/2012.15828)

- DeBERTaV2
  - Year: 2021
  - Publication: [Blog - Microsoft DeBERTa surpasses human performance on the SuperGLUE benchmark](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-v2-xlarge)

- Switch Transformer
  - Year: 2021
  - Publication: [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)
  - Code: [GitHub](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)

- WuDao
  - Year: 2021
  - Publication: [Article: Five Key Facts Wu Dao 2.0: The Largest Transformer Model Ever Built](https://medium.com/dataseries/five-key-facts-wu-dao-2-0-the-largest-transformer-model-ever-built-19316159796b)

- UniT
  - Year: 2021
  - Publication: [UniT: Multimodal Multitask Learning with a Unified Transformer](https://arxiv.org/abs/2102.10772)
  - Code: [GitHub](https://github.com/facebookresearch/mmf)

- PLBART
  - Year: 2021
  - Publication: [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333)
  - Code: [GitHub](https://github.com/wasiahmad/PLBART)
  - Model weights: [Direct link](https://github.com/wasiahmad/PLBART/tree/9bf5e12bb7374218c21546aa31020ef102a151e7#fine-tuning)

- GLM
  - Year: 2021
  - Publication: [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360)
  - Code: [GitHub](https://github.com/THUDM/GLM)
  - Model weights: [HuggingFace models](https://huggingface.co/BAAI/glm-10b)

- Megatron-LM v2
  - Year: 2021
  - Publication: [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- 24hBERT - Academic Budget BERT
  - Year: 2021
  - Publication: [How to Train BERT with an Academic Budget](https://arxiv.org/abs/2104.07705)
  - Code: [GitHub](https://github.com/IntelLabs/academic-budget-bert)

- RoFormer
  - Year: 2021
  - Publication: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
  - Code: [GitHub](https://github.com/ZhuiyiTechnology/roformer)
  - Model weights: [HuggingFace models](https://huggingface.co/junnyu/roformer_chinese_base)

- PanGu
  - Year: 2021
  - Publication: [PanGu: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation](https://arxiv.org/abs/2104.12369)

- FNet
  - Year: 2021
  - Publication: [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824)
  - Video: [YouTube](https://www.youtube.com/watch?v=JJR3pBl78zw)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/f_net)

- EL-Attention
  - Year: 2021
  - Publication: [EL-Attention: Memory Efficient Lossless Attention for Generation](https://arxiv.org/abs/2105.04779)
  - Code: [GitHub](https://github.com/microsoft/fastseq)

- ByT5
  - Year: 2021
  - Publication: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626)
  - Code: [GitHub](https://github.com/google-research/byt5)

- Luna
  - Year: 2021
  - Publication: [Luna: Linear Unified Nested Attention](https://arxiv.org/abs/2106.01540)
  - Code: [GitHub](https://github.com/XuezheMax/fairseq-apollo)

- GPT-J
  - Year: 2021
  - Publication: [Blog - GPT-J](https://www.eleuther.ai/artifacts/gpt-j)
  - Code: [GitHub](https://github.com/kingoflolz/mesh-transformer-jax/)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/gpt-j-6b)

- Charformer
  - Year: 2021
  - Publication: [Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](https://arxiv.org/abs/2106.12672)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/charformer)

- ERNIE 3.0
  - Year: 2021
  - Publication: [ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137)

- ALiBi
  - Year: 2021
  - Publication: [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)
  - Code: [GitHub](https://github.com/ofirpress/attention_with_linear_biases)
  - Model weights: [Private page](https://github.com/ofirpress/attention_with_linear_biases/tree/master/examples/language_model)

- CodeT5
  - Year: 2021
  - Publication: [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/abs/2109.00859)
  - Code: [GitHub](https://github.com/salesforce/CodeT5)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codet5-base)

- HyperCLOVA
  - Year: 2021
  - Publication: [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://arxiv.org/abs/2109.04650)

- Jurassic-1
  - Year: 2021
  - Publication: [Jurassic-1: Technical Details And Evaluation](https://assets.website-files.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)
  - Code: [GitHub](https://github.com/ai21labs/lm-evaluation)

- Primer
  - Year: 2021
  - Publication: [Primer: Searching for Efficient Transformers for Language Modeling](https://arxiv.org/abs/2109.08668)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/primer)

- T0
  - Year: 2021
  - Publication: [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)
  - Code: [GitHub](https://github.com/bigscience-workshop/t-zero)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/T0)

- NormFormer
  - Year: 2021
  - Publication: [NormFormer: Improved Transformer Pretraining with Extra Normalization](https://arxiv.org/abs/2110.09456)

- DeBERTaV3
  - Year: 2021
  - Publication: [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing](https://arxiv.org/abs/2111.09543)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-v3-base)

- Gopher
  - Year: 2021
  - Publication: [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446)

- Memory efficient attention
  - Year: 2021
  - Publication: [Self-attention Does Not Need O(n2) Memory](https://arxiv.org/abs/2112.05682)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/memory_efficient_attention)

- XGLM
  - Year: 2021
  - Publication: [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668)
  - Code: [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/xglm)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/xglm-7.5B)

- LaMDA
  - Year: 2022
  - Publication: [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239)

- Megatron-Turing NLG
  - Year: 2022
  - Publication: [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)

- Memorizing Transformers, MemTRM
  - Year: 2022
  - Publication: [Memorizing Transformers](https://arxiv.org/abs/2203.08913)

- Chinchilla
  - Year: 2022
  - Publication: [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)

- PaLM
  - Year: 2022
  - Publication: [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)

- GPT-NeoX
  - Year: 2022
  - Publication: [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)
  - Code: [GitHub](https://github.com/EleutherAI/gpt-neox)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/gpt-neox-20b)

- Jurassic-X
  - Year: 2022
  - Publication: [Blog - Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system)

- OPT
  - Year: 2022
  - Publication: [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)
  - Code: [GitHub](https://github.com/facebookresearch/metaseq)

- UL2
  - Year: 2022
  - Publication: [UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/ul2)

- FlashAttention
  - Year: 2022
  - Publication: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
  - Code: [GitHub](https://github.com/HazyResearch/flash-attention)

- Minerva
  - Year: 2022
  - Publication: [Solving Quantitative Reasoning Problems with Language Models](https://arxiv.org/abs/2206.14858)

- CodeRL
  - Year: 2022
  - Publication: [CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning](https://arxiv.org/abs/2207.01780)
  - Code: [GitHub](https://github.com/salesforce/CodeRL)

- Petals
  - Year: 2022
  - Publication: [Petals: Collaborative Inference and Fine-tuning of Large Models](https://arxiv.org/abs/2209.01188)
  - Code: [GitHub](https://github.com/bigscience-workshop/petals)

- GLM-130B
  - Year: 2022
  - Publication: [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414)
  - Code: [GitHub](https://github.com/THUDM/GLM-130B)
  - Model weights: [HuggingFace models](https://huggingface.co/spaces/THUDM/GLM-130B)

- Magneto
  - Year: 2022
  - Publication: [Foundation Transformers](https://arxiv.org/abs/2210.06423)
  - Code: [GitHub](https://github.com/sunyt32/torchscale)

- TransNormer
  - Year: 2022
  - Publication: [The Devil in Linear Transformer](https://arxiv.org/abs/2210.10340)
  - Code: [GitHub](https://github.com/OpenNLPLab/Transnormer)

- FlanT5
  - Year: 2022
  - Publication: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
  - Code: [GitHub](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)
  - Model weights: [HuggingFace models](https://huggingface.co/google/flan-t5-base)

- BLOOMZ & mT0
  - Year: 2022
  - Publication: [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloomz)

- BLOOM
  - Year: 2022
  - Publication: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloom)

- Galactica
  - Year: 2022
  - Publication: [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)
  - Code: [GitHub](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/galactica-6.7b)

- NPM
  - Year: 2022
  - Publication: [Nonparametric Masked Language Modeling](https://arxiv.org/abs/2212.01349)
  - Code: [GitHub](https://github.com/facebookresearch/NPM)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/npm)

- BioMedLM
  - Year: 2022
  - Publication: [Blog - BioMedLM: a Domain-Specific Large Language Model for Biomedical Text](https://www.mosaicml.com/blog/introducing-pubmed-gpt)
  - Code: [GitHub](https://github.com/stanford-crfm/BioMedLM)
  - Model weights: [HuggingFace models](https://huggingface.co/stanford-crfm/BioMedLM)

- LeX-Transformer
  - Year: 2022
  - Publication: [A Length-Extrapolatable Transformer](https://arxiv.org/abs/2212.10554)
  - Code: [GitHub](https://github.com/sunyt32/torchscale)

- OPT-IML
  - Year: 2022
  - Publication: [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/abs/2212.12017)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/opt-iml-30b)

- Cramming
  - Year: 2022
  - Publication: [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034)
  - Code: [GitHub](https://github.com/JonasGeiping/cramming)

- StitchNet
  - Year: 2023
  - Publication: [StitchNet: Composing Neural Networks from Pre-Trained Fragments](https://arxiv.org/abs/2301.01947)
  - Code: [GitHub](https://github.com/steerapi/stitchnet)

- Palmyra
  - Year: 2023
  - Publication: [Blog - Palmyra LLMs empower secure, enterprise-grade generative AI for business](https://writer.com/blog/palmyra/)
  - Model weights: [HuggingFace models](https://huggingface.co/Writer/palmyra-large)

- SantaCoder
  - Year: 2023
  - Publication: [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988)
  - Code: [GitHub](https://github.com/bigcode-project/Megatron-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/santacoder)

- NarrowBERT
  - Year: 2023
  - Publication: [NarrowBERT: Accelerating Masked Language Model Pretraining and Inference](https://arxiv.org/abs/2301.04761)

- Diff-Codegen
  - Year: 2023
  - Publication: [Blog - Diff Models - A New Way to Edit Code](https://carper.ai/diff-models-a-new-way-to-edit-code/)
  - Code: [GitHub](https://github.com/salesforce/CodeGen)
  - Model weights: [HuggingFace models](https://huggingface.co/CarperAI/diff-codegen-6b-v2)

- E-SPA (Exponential Signal Preserving Attention)
  - Year: 2023
  - Publication: [Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation](https://arxiv.org/abs/2302.10322)

- Hyena
  - Year: 2023
  - Publication: [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866)
  - Code: [GitHub](https://github.com/HazyResearch/safari)

- SpikeGPT
  - Year: 2023
  - Publication: [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks](https://arxiv.org/abs/2302.13939)
  - Code: [GitHub](https://github.com/ridgerchu/SpikeGPT)

- LLama
  - Year: 2023
  - Publication: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
  - Code: [GitHub](https://github.com/facebookresearch/llama)
  - Model weights: [Private page - request required](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)

- KOSMOS-1
  - Year: 2023
  - Publication: [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045)

- ParaFormer
  - Year: 2023
  - Publication: [ParaFormer: Parallel Attention Transformer for Efficient Feature Matching](https://arxiv.org/abs/2303.00941)

- PaLM-E
  - Year: 2023
  - Publication: [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378)

- MosaicBERT
  - Year: 2023
  - Publication: [MosaicBERT: Pretraining BERT from Scratch for $20](https://www.mosaicml.com/blog/mosaicbert)
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mosaic-bert-base)

- Jurassic-2
  - Year: 2023
  - Publication: [Blog - Announcing Jurassic-2 and Task-Specific APIs](https://www.ai21.com/blog/introducing-j2)

- Alpaca
  - Year: 2023
  - Publication: [Alpaca: A Strong Open-Source Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)
  - Code: [GitHub](https://github.com/tatsu-lab/stanford_alpaca)

- TWM
  - Year: 2023
  - Publication: [Transformer-based World Models Are Happy With 100k Interactions](https://arxiv.org/abs/2303.07109)
  - Code: [GitHub](https://github.com/jrobine/twm)

- GPT-4
  - Year: 2023
  - Publication: [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)

- alpaca-opt
  - Year: 2023
  - Publication: [README - alpaca-opt-6.7b repository](https://github.com/Manuel030/alpaca-opt/blob/0466819d3647568a125530488f79c5627dd292ed/README.md)
  - Code: [GitHub](https://github.com/Manuel030/alpaca-opt)
  - Model weights: [HuggingFace models](https://huggingface.co/Manuel030/alpaca-opt-6.7b)

- GALPACA
  - Year: 2023
  - Publication: [HuggingFace model card - GALPACA](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b)
  - Model weights: [HuggingFace models](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b)

- AlpacOOM
  - Year: 2023
  - Publication: [Tweet: Alpaca + BLOOM = Alpacoom](https://twitter.com/alfredplpl/status/1636858660218617857)
  - Model weights: [HuggingFace models](https://huggingface.co/mrm8488/Alpacoom)

- GPT4All
  - Year: 2023
  - Publication: [Tweet: Today we're releasing GPT4All, an assistant-style chatbot distilled from 430k GPT-3.5-Turbo outputs that you can run on your laptop.](https://twitter.com/nomic_ai/status/1640834838578995202)
  - Code: [GitHub](https://github.com/nomic-ai/gpt4all)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/gpt4all-lora)

- OpenFlamingo
  - Year: 2023
  - Publication: [Blog - ANNOUNCING OPENFLAMINGO: AN OPEN-SOURCE FRAMEWORK FOR TRAINING VISION-LANGUAGE MODELS WITH IN-CONTEXT LEARNING](https://laion.ai/blog/open-flamingo/)
  - Code: [GitHub](https://github.com/mlfoundations/open_flamingo)
  - Model weights: [HuggingFace models](https://huggingface.co/openflamingo/OpenFlamingo-9B)

- LLaMA-Adapter
  - Year: 2023
  - Publication: [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- GeoV
  - Year: 2023
  - Publication: [README - GeoV repository](https://github.com/geov-ai/geov/blob/4bd51d2d81168ca1d8917aeb7513a1d95feaf727/readme.md)
  - Code: [GitHub](https://github.com/geov-ai/geov)
  - Model weights: [HuggingFace models](https://huggingface.co/GeoV/GeoV-9b)

- CodeGeeX
  - Year: 2023
  - Publication: [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X](https://arxiv.org/abs/2303.17568)
  - Code: [GitHub](https://github.com/THUDM/CodeGeeX)
  - Model weights: [Private page - request required](https://models.aminer.cn/codegeex/download/request)

- Vicuna
  - Year: 2023
  - Publication: [Blog - Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality](https://lmsys.org/blog/2023-03-30-vicuna/)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/vicuna-7b-v1.5)

- BloombergGPT
  - Year: 2023
  - Publication: [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)

- CodeGeeX2
  - Year: 2023
  - Publication: [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X](https://arxiv.org/abs/2303.17568)
  - Code: [GitHub](https://github.com/THUDM/CodeGeeX2)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/codegeex2-6b)

- GPTrillion
  - Year: 2023
  - Publication: [Tweet: GPTrillion: a 1.5T Parameter Open-Source Model](https://twitter.com/BananaDev_/status/1642211220072673286)

- Pythia
  - Year: 2023
  - Publication: [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)
  - Code: [GitHub](https://github.com/EleutherAI/pythia)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/pythia-70m)

- Koala
  - Year: 2023
  - Publication: [Blog - Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/)
  - Code: [GitHub](https://github.com/young-geng/EasyLM)
  - Model weights: [HuggingFace models](https://huggingface.co/young-geng/koala)

- Cerebras-GPT
  - Year: 2023
  - Publication: [Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster](https://arxiv.org/abs/2304.03208)
  - Code: [GitHub](https://github.com/Cerebras/modelzoo)
  - Model weights: [HuggingFace models](https://huggingface.co/cerebras/Cerebras-GPT-13B)

- Dolly
  - Year: 2023
  - Publication: [Blog - Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)
  - Code: [GitHub](https://github.com/databrickslabs/dolly)
  - Model weights: [HuggingFace models](https://huggingface.co/databricks/dolly-v2-12b)

- GPT4All-J
  - Year: 2023
  - Publication: [Tweet: Announcing GPT4All-J: The First Apache-2 Licensed Chatbot That Runs Locally on Your Machine](https://twitter.com/andriy_mulyar/status/1646622168350875655)
  - Code: [GitHub](https://github.com/nomic-ai/gpt4all)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/gpt4all-j)

- OpenAssistant
  - Year: 2023
  - Publication: [OpenAssistant Conversations -- Democratizing Large Language Model Alignment](https://arxiv.org/abs/2304.07327)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5)

- umT5
  - Year: 2023
  - Publication: [UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining](https://arxiv.org/abs/2304.09151)
  - Code: [GitHub](https://github.com/google-research/t5x)
  - Model weights: [Direct link](https://github.com/google-research/t5x/blob/main/docs/models.md#umt5-checkpoints)

- StableLM-Alpha
  - Year: 2023
  - Publication: [Blog - Stability AI Launches the First of its StableLM Suite of Language Models](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)
  - Code: [GitHub](https://github.com/stability-AI/stableLM/)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)

- RMT
  - Year: 2023
  - Publication: [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/abs/2304.11062)
  - Code: [GitHub](https://github.com/booydar/t5-experiments/tree/scaling-report)

- MPT-1b-RedPajama-200b
  - Year: 2023
  - Publication: [HuggingFace model card - MPT-1b-RedPajama-200b](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)

- MiniGPT-4
  - Year: 2023
  - Publication: [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592)
  - Code: [GitHub](https://github.com/Vision-CAIR/MiniGPT-4)

- WizardLM
  - Year: 2023
  - Publication: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244)
  - Code: [GitHub](https://github.com/nlpxucan/WizardLM)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardLM-30B-V1.0)

- FastChat-T5
  - Year: 2023
  - Publication: [Tweet: We are excited to release FastChat-T5: our compact and commercial-friendly chatbot!](https://twitter.com/lmsysorg/status/1652037026705985537)
  - Code: [GitHub](https://github.com/lm-sys/FastChat)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)

- Lamini
  - Year: 2023
  - Publication: [Blog - Introducing Lamini, the LLM Engine for Rapidly Customizing Models](https://lamini.ai/blog/introducing-lamini)
  - Code: [GitHub](https://github.com/lamini-ai/lamini)
  - Model weights: [HuggingFace models](https://huggingface.co/lamini/instruct-peft-tuned-12b)

- LLaMA-Adapter V2
  - Year: 2023
  - Publication: [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://arxiv.org/abs/2304.15010)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- OpenLLaMA
  - Year: 2023
  - Publication: [README - OpenLLaMA repository](https://github.com/openlm-research/open_llama/blob/6e7f73eab7e799e2464f38ed977e537bae02873e/README.md)
  - Code: [GitHub](https://github.com/openlm-research/open_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/openlm-research/open_llama_13b)

- Unlimiformer
  - Year: 2023
  - Publication: [Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/abs/2305.01625)
  - Code: [GitHub](https://github.com/abertsch72/unlimiformer)
  - Model weights: [HuggingFace models](https://huggingface.co/abertsch/unlimiformer-bart-booksum-retrieval)

- ReplitLM
  - Year: 2023
  - Publication: [Tweet: Last night, we released our new complete code model: replit-code-v1-3b.](https://twitter.com/Replit/status/1653802301331759104)
  - Code: [GitHub](https://github.com/replit/ReplitLM)
  - Model weights: [HuggingFace models](https://huggingface.co/replit/replit-code-v1-3b)

- CodeGen2
  - Year: 2023
  - Publication: [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309)
  - Code: [GitHub](https://github.com/salesforce/CodeGen2)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen2-16B)

- CodeGen2.5
  - Year: 2023
  - Publication: [Blog - CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309)
  - Code: [GitHub](https://github.com/salesforce/CodeGen2)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen25-7b-multi)

- MPT-7B
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b)

- MPT-7B-Instruct
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-instruct)

- MPT-7B-Chat
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-chat)

- Otter
  - Year: 2023
  - Publication: [Otter: A Multi-Modal Model with In-Context Instruction Tuning](https://arxiv.org/abs/2305.03726)
  - Code: [GitHub](https://github.com/Luodian/otter)
  - Model weights: [HuggingFace models](https://huggingface.co/luodian/otter-9b-hf)

- Open-source PaLM
  - Year: 2023
  - Publication: [README - Open-source PaLM repository](https://github.com/conceptofmind/PaLM/blob/dc2425df678d300b9d5b65d028fc70293c01a589/README.md)
  - Code: [GitHub](https://github.com/conceptofmind/PaLM)
  - Model weights: [HuggingFace models](https://huggingface.co/conceptofmind/palm-1b)

- Multimodal-GPT
  - Year: 2023
  - Publication: [MultiModal-GPT: A Vision and Language Model for Dialogue with Humans](https://arxiv.org/abs/2305.04790)
  - Code: [GitHub](https://github.com/open-mmlab/Multimodal-GPT)
  - Model weights: [Direct link](https://download.openmmlab.com/mmgpt/v0/mmgpt-lora-v0-release.pt)

- StarCoder
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoder)

- StarChat Alpha
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceH4/starchat-alpha)

- ImageBind
  - Year: 2023
  - Publication: [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665)
  - Code: [GitHub](https://github.com/facebookresearch/ImageBind)
  - Model weights: [Direct link](https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth)

- StarCoderBase
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoderbase)

- StarCoderPlus
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoderplus)

- DLite
  - Year: 2023
  - Publication: [Blog - Announcing DLite V2: Lightweight, Open LLMs That Can Run Anywhere](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e)
  - Model weights: [HuggingFace models](https://huggingface.co/aisquared/dlite-v2-1_5b)

- PaLM 2
  - Year: 2023
  - Publication: [Blog - Introducing PaLM 2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/)

- MEGABYTE
  - Year: 2023
  - Publication: [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185)

- CodeT5+
  - Year: 2023
  - Publication: [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922)
  - Code: [GitHub](https://github.com/salesforce/CodeT5)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codet5p-16b)

- DarkBERT
  - Year: 2023
  - Publication: [DarkBERT: A Language Model for the Dark Side of the Internet](https://arxiv.org/abs/2305.08596)

- mLongT5
  - Year: 2023
  - Publication: [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://arxiv.org/abs/2305.11129)
  - Code: [GitHub](https://github.com/google-research/longt5)

- LIMA
  - Year: 2023
  - Publication: [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)

- BLOOMChat
  - Year: 2023
  - Publication: [Blog - BLOOMChat: a New Open Multilingual Chat LLM](https://sambanova.ai/blog/introducing-bloomchat-176b-the-multilingual-chat-based-llm/)
  - Model weights: [HuggingFace models](https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1)

- RWKV-LM
  - Year: 2023
  - Publication: [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)
  - Code: [GitHub](https://github.com/BlinkDL/RWKV-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)

- Aurora genAI
  - Year: 2023
  - Publication: [Blog - Intel Announces Aurora genAI, Generative AI Model With 1 Trillion Parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters)

- Goat
  - Year: 2023
  - Publication: [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks](https://arxiv.org/abs/2305.14201)
  - Code: [GitHub](https://github.com/liutiedong/goat)
  - Model weights: [HuggingFace models](https://huggingface.co/tiedong/goat-lora-7b)

- UltraLM / UltraLLaMA
  - Year: 2023
  - Publication: [Enhancing Chat Language Models by Scaling High-quality Instructional Conversations](https://arxiv.org/abs/2305.14233)
  - Code: [GitHub](https://github.com/thunlp/UltraChat)
  - Model weights: [HuggingFace models](https://huggingface.co/openbmb/UltraLM-13b)

- Gorilla
  - Year: 2023
  - Publication: [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334)
  - Code: [GitHub](https://github.com/ShishirPatil/gorilla)

- BiomedGPT
  - Year: 2023
  - Publication: [BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks](https://arxiv.org/abs/2305.17100)
  - Code: [GitHub](https://github.com/taokz/BiomedGPT)

- Refact-1.6B
  - Year: 2023
  - Publication: [Applying All Recent Innovations To Train a Code Model](https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/)
  - Code: [GitHub](https://github.com/smallcloudai/refact/)
  - Model weights: [HuggingFace models](https://huggingface.co/smallcloudai/Refact-1_6B-fim)

- MeZO
  - Year: 2023
  - Publication: [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333)
  - Code: [GitHub](https://github.com/princeton-nlp/MeZO)

- NoPE
  - Year: 2023
  - Publication: [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466)

- MEFT
  - Year: 2023
  - Publication: [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning](https://arxiv.org/abs/2306.00477)
  - Code: [GitHub](https://github.com/baohaoLiao/mefts)

- Falcon
  - Year: 2023
  - Publication: [Blog - The Falcon has landed in the Hugging Face ecosystem](https://huggingface.co/blog/falcon)
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/falcon-40b)

- SpQR
  - Year: 2023
  - Publication: [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/abs/2306.03078)
  - Code: [GitHub](https://github.com/Vahe1994/SpQR)

- RedPajama-INCITE
  - Year: 2023
  - Publication: [Blog - RedPajama 7B now available, instruct model outperforms all open 7B models on HELM benchmarks](https://www.together.xyz/blog/redpajama-7b)
  - Code: [GitHub](https://github.com/togethercomputer/redpajama.cpp)
  - Model weights: [HuggingFace models](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base)

- Tulu
  - Year: 2023
  - Publication: [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751)
  - Code: [GitHub](https://github.com/allenai/open-instruct)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/tulu-65b)

- INSTRUCTEVAL
  - Year: 2023
  - Publication: [INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2306.04757)
  - Code: [GitHub](https://github.com/declare-lab/instruct-eval)
  - Model weights: [HuggingFace models](https://huggingface.co/declare-lab/flan-alpaca-xxl)

- MoLM
  - Year: 2023
  - Publication: [ModuleFormer: Modularity Emerges from Mixture-of-Experts](https://arxiv.org/abs/2306.04640)
  - Code: [GitHub](https://github.com/IBM/ModuleFormer)
  - Model weights: [HuggingFace models](https://huggingface.co/ibm/MoLM-350M-4B)

- FinGPT
  - Year: 2023
  - Publication: [FinGPT: Open-Source Financial Large Language Models](https://arxiv.org/abs/2306.06031)
  - Code: [GitHub](https://github.com/ai4finance-foundation/fingpt)

- LongMem
  - Year: 2023
  - Publication: [Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.07174)
  - Code: [GitHub](https://github.com/Victorwz/LongMem)

- Orca
  - Year: 2023
  - Publication: [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.07174)

- h2ogpt
  - Year: 2023
  - Publication: [h2oGPT: Democratizing Large Language Models](https://arxiv.org/abs/2306.08161)
  - Code: [GitHub](https://github.com/h2oai/h2ogpt)
  - Model weights: [HuggingFace models](https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b)

- SqueezeLLM
  - Year: 2023
  - Publication: [SqueezeLLM: Dense-and-Sparse Quantization](https://export.arxiv.org/abs/2306.07629)
  - Code: [GitHub](https://github.com/SqueezeAILab/SqueezeLLM)

- WizardCoder
  - Year: 2023
  - Publication: [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568)
  - Code: [GitHub](https://github.com/nlpxucan/WizardLM)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0)

- Macaw-LLM
  - Year: 2023
  - Publication: [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/abs/2306.09093)
  - Code: [GitHub](https://github.com/lyuchenyang/Macaw-LLM)

- LOMO
  - Year: 2023
  - Publication: [Full Parameter Fine-tuning for Large Language Models with Limited Resources](https://arxiv.org/abs/2306.09782)
  - Code: [GitHub](https://github.com/OpenLMLab/LOMO)

- phi-1
  - Year: 2023
  - Publication: [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-1)

- Inflection-1
  - Year: 2023
  - Publication: [Blog - Inflection-1: Piâ€™s Best-in-Class LLM](https://inflection.ai/inflection-1)

- ChatGLM-6B
  - Year: 2023
  - Publication: [README - ChatGLM-6B repository](https://github.com/THUDM/ChatGLM-6B/blob/d835c4b0017310d53b98bad98f68671fa861d158/README_en.md)
  - Code: [GitHub](https://github.com/THUDM/ChatGLM-6B)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/chatglm-6b)

- Kosmos-2
  - Year: 2023
  - Publication: [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/kosmos-2)
  - Model weights: [direct link](https://conversationhub.blob.core.windows.net/beit-share-public/kosmos-2/kosmos-2.pt?sv=2021-10-04&st=2023-06-08T11%3A16%3A02Z&se=2033-06-09T11%3A16%3A00Z&sr=c&sp=r&sig=N4pfCVmSeq4L4tS8QbrFVsX6f6q844eft8xSuXdxU48%3D)

- XGen
  - Year: 2023
  - Publication: [Blog - Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length](https://blog.salesforceairesearch.com/xgen/)
  - Code: [GitHub](https://github.com/salesforce/xGen)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/xgen-7b-8k-base)

- LongNet
  - Year: 2023
  - Publication: [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486)
  - Code: [GitHub](https://github.com/microsoft/unilm)

- InternLM
  - Year: 2023
  - Publication: [InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities](https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf)
  - Code: [GitHub](https://github.com/InternLM/InternLM)
  - Model weights: [HuggingFace models](https://huggingface.co/internlm/internlm-7b)

- LongLLaMA
  - Year: 2023
  - Publication: [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/abs/2307.03170)
  - Code: [GitHub](https://github.com/CStanKonrad/long_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/syzymon/long_llama_3b)

- Redmond-Hermes-Coder
  - Year: 2023
  - Publication: [Tweet: Releasing Redmond-Hermes-Coder, a finetune of Wizardcoder on our dataset.](https://twitter.com/NousResearch/status/1674992144170340353)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Redmond-Hermes-Coder)

- PolyLM
  - Year: 2023
  - Publication: [PolyLM: An Open Source Polyglot Large Language Model](https://arxiv.org/abs/2307.06018)
  - Code: [GitHub](https://github.com/DAMO-NLP-MT/PolyLM)
  - Model weights: [HuggingFace models](https://huggingface.co/DAMO-NLP-MT/polylm-13b)

- MPT-7B-8k
  - Year: 2023
  - Publication: [Blog - Announcing MPT-7B-8K: 8K Context Length for Document Understanding](https://www.mosaicml.com/blog/long-context-mpt-7b-8k)
  - Code: [GitHub](https://github.com/CStanKonrad/long_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-8k)

- Llama 2
  - Year: 2023
  - Publication: [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
  - Code: [GitHub](https://github.com/facebookresearch/llama)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Llama-2-7b)

- LRPE
  - Year: 2023
  - Publication: [Linearized Relative Positional Encoding](https://arxiv.org/abs/2307.09270)
  - Code: [GitHub](https://github.com/OpenNLPLab/Lrpe)

- PanGu-Coder2
  - Year: 2023
  - Publication: [PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback](https://arxiv.org/abs/2307.14936)

- Alfred-40B-0723
  - Year: 2023
  - Publication: [Blog - Introducing Alfred-40B-0723](https://www.lighton.ai/blog/lighton-s-blog-4/introducing-alfred-40b-0723-38)
  - Model weights: [HuggingFace models](https://huggingface.co/lightonai/alfred-40b-0723)

- Qwen-7B
  - Year: 2023
  - Publication: [README - Qwen-7B repository](https://github.com/QwenLM/Qwen-7B/blob/96d10ebc9c6c170d294f2014f1b6926c9e80c95b/README.md)
  - Code: [GitHub](https://github.com/QwenLM/Qwen-7B)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen-7B)

- Baby-CoThought
  - Year: 2023
  - Publication: [Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models](https://arxiv.org/abs/2308.01684)
  - Code: [GitHub](https://github.com/oooranz/Baby-CoThought)
  - Model weights: [HuggingFace models](https://huggingface.co/yaanhaan/Baby-CoThought)

- StableCode Complete Alpha
  - Year: 2023
  - Publication: [Blog - Announcing StableCode](https://stability.ai/blog/stablecode-llm-generative-ai-coding)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablecode-completion-alpha-3b)

- Claude Instant 1.2
  - Year: 2023
  - Publication: [Blog - Releasing Claude Instant 1.2](https://www.anthropic.com/index/releasing-claude-instant-1-2)

- OctoCoder
  - Year: 2023
  - Publication: [OctoPack: Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2308.07124)
  - Code: [GitHub](https://github.com/bigcode-project/octopack)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/octocoder)

- OctoGeeX
  - Year: 2023
  - Publication: [OctoPack: Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2308.07124)
  - Code: [GitHub](https://github.com/bigcode-project/octopack)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/octogeex)

- Aquila
  - Year: 2023
  - Publication: [README - Aquila repository](https://github.com/FlagAI-Open/FlagAI/blob/e3062883ca5156d5c6f6b1992a5f1a08edbd437c/examples/Aquila/README_en.md)
  - Code: [GitHub](https://github.com/FlagAI-Open/FlagAI)
  - Model weights: [HuggingFace models](https://huggingface.co/BAAI/Aquila-7B)

- DeciCoder
  - Year: 2023
  - Publication: [Blog - Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation](https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/)
  - Model weights: [HuggingFace models](https://huggingface.co/Deci/DeciCoder-1b)

- WizardMath
  - Year: 2023
  - Publication: [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583)
  - Code: [GitHub](https://github.com/nlpxucan/WizardLM)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardMath-7B-V1.0)

- SQLCoder
  - Year: 2023
  - Publication: [Blog - Open-sourcing SQLCoder: a state-of-the-art LLM for SQL generation](https://defog.ai/blog/open-sourcing-sqlcoder/)
  - Code: [GitHub](https://github.com/defog-ai/sqlcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/defog/sqlcoder)

- Giraffe
  - Year: 2023
  - Publication: [Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882)
  - Code: [GitHub](https://github.com/abacusai/Long-Context)
  - Model weights: [HuggingFace models](https://huggingface.co/abacusai/Giraffe-v2-13b-32k)

- OpenMoE
  - Year: 2023
  - Publication: [Blog - OpenMoE v0.2 Release](https://xuefuzhao.notion.site/Aug-2023-OpenMoE-v0-2-Release-43808efc0f5845caa788f2db52021879)
  - Code: [GitHub](https://github.com/XueFuzhao/OpenMoE)
  - Model weights: [HuggingFace models](https://huggingface.co/fuzhao/OpenMoE_Base)

- Code Llama
  - Year: 2023
  - Publication: [Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950)
  - Code: [GitHub](https://github.com/facebookresearch/codellama)
  - Model weights: [Direct link](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

- Yarn-Llama-2
  - Year: 2023
  - Publication: [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071)
  - Code: [GitHub](https://github.com/jquesnelle/yarn)
  - Model weights: [HuggingFace models](https://huggingface.co/conceptofmind/Yarn-Llama-2-7b-64k)

- TinyLlama
  - Year: 2023
  - Publication: [README - TinyLlama repository](https://github.com/jzhang38/TinyLlama/blob/0fcf9b61130f189b78747b0b013262c72f01286a/README.md)
  - Code: [GitHub](https://github.com/jzhang38/TinyLlama)
  - Model weights: [HuggingFace models](https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b)

- One Wide FFN
  - Year: 2023
  - Publication: [One Wide Feedforward is All You Need](https://arxiv.org/abs/2309.01826)

- Falcon-180B
  - Year: 2023
  - Publication: [Blog - Spread Your Wings: Falcon 180B is here](https://huggingface.co/blog/falcon-180b)
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/falcon-180B)

- MathGLM
  - Year: 2023
  - Publication: [GPT Can Solve Mathematical Problems Without a Calculator](https://arxiv.org/abs/2309.03241)
  - Code: [GitHub](https://github.com/THUDM/MathGLM)
  - Model weights: [Direct link](https://cloud.tsinghua.edu.cn/d/92127e3a1b4144db8d13/)

- Persimmon-8B
  - Year: 2023
  - Publication: [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b)
  - Code: [GitHub](https://github.com/persimmon-ai-labs/adept-inference)
  - Model weights: [Direct link](https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar)

- FLM-101B
  - Year: 2023
  - Publication: [FLM-101B: An Open LLM and How to Train It with $100K Budget](https://arxiv.org/abs/2309.03852)
  - Model weights: [HuggingFace models](https://huggingface.co/CofeAI/FLM-101B)

- phi-1.5
  - Year: 2023
  - Publication: [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-1_5)

- MAmmoTH
  - Year: 2023
  - Publication: [MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning](https://arxiv.org/abs/2309.05653)
  - Code: [GitHub](https://github.com/TIGER-AI-Lab/MAmmoTH)
  - Model weights: [HuggingFace models](https://huggingface.co/TIGER-Lab/MAmmoTH-Coder-7B)

- DeciLM
  - Year: 2023
  - Publication: [Blog - 15 times Faster than Llama 2: Introducing DeciLM â€“ NAS-Generated LLM with Variable GQA](https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/)
  - Model weights: [HuggingFace models](https://huggingface.co/Deci/DeciLM-6b)

- Xwin-LM
  - Year: 2023
  - Publication: [README - README -](https://github.com/Xwin-LM/Xwin-LM/blob/25862b325ee2a5b7c3c0007728eaeeb3daf5f05f/README.md)
  - Code: [GitHub](https://github.com/Xwin-LM/Xwin-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/Xwin-LM/Xwin-LM-13B-V0.2)

- Baichuan 2
  - Year: 2023
  - Publication: [Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305)
  - Code: [GitHub](https://github.com/baichuan-inc/Baichuan2)
  - Model weights: [HuggingFace models](https://huggingface.co/baichuan-inc/Baichuan2-13B-Base)

- BTLM-3B-8k
  - Year: 2023
  - Publication: [BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model](https://arxiv.org/abs/2309.11568)
  - Model weights: [HuggingFace models](https://huggingface.co/cerebras/btlm-3b-8k-base)

- Kosmos-2.5
  - Year: 2023
  - Publication: [Kosmos-2.5: A Multimodal Literate Model](https://arxiv.org/abs/2309.11419)

- Nous-Capybara
  - Year: 2023
  - Publication: [HuggingFace model card - Nous-Capybara](https://huggingface.co/NousResearch/Nous-Capybara-7B-GGUF)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Nous-Capybara-7B-GGUF)

- Glaive-coder
  - Year: 2023
  - Publication: [Blog - Releasing glaive-coder-7B and the Code Models Arena](https://glaive.ai/blog/releasing-code-model-arena)
  - Model weights: [HuggingFace models](https://huggingface.co/glaiveai/glaive-coder-7b)

- LongLoRA
  - Year: 2023
  - Publication: [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
  - Code: [GitHub](https://github.com/dvlab-research/LongLoRA)
  - Model weights: [HuggingFace models](https://huggingface.co/Yukang/Llama-2-7b-longlora-100k-ft)

- Mistral
  - Year: 2023
  - Publication: [Blog - Mistral 7B - The best 7B model to date, Apache 2.0](https://mistral.ai/news/announcing-mistral-7b/)
  - Code: [GitHub](https://github.com/mistralai/mistral-src)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mistral-7B-v0.1)

- Nous-Hermes
  - Year: 2023
  - Publication: [Tweet: Nous-Hermes-13b fp16 weights have been released.](https://twitter.com/NousResearch/status/1664848823687028737)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Nous-Hermes-13b)

- StableLM-3B-4E1T
  - Year: 2023
  - Publication: [Blog - Technical report for StableLM-3B-4E1T](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo)
  - Code: [GitHub](https://github.com/Stability-AI/StableLM)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-3b-4e1t)

- StreamingLLM
  - Year: 2023
  - Publication: [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)
  - Code: [GitHub](https://github.com/mit-han-lab/streaming-llm)

- MiniGPT-5
  - Year: 2023
  - Publication: [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens](https://arxiv.org/abs/2310.02239)
  - Code: [GitHub](https://github.com/eric-ai-lab/MiniGPT-5)

- Kosmos-G
  - Year: 2023
  - Publication: [Kosmos-G: Generating Images in Context with Multimodal Large Language Models](https://arxiv.org/abs/2310.02992)

- LightSeq
  - Year: 2023
  - Publication: [LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers](https://arxiv.org/abs/2310.03294)
  - Code: [GitHub](https://github.com/RulinShao/LightSeq)

- MiniGPT-v2
  - Year: 2023
  - Publication: [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478)
  - Code: [GitHub](https://github.com/Vision-CAIR/MiniGPT-4)

- Llemma
  - Year: 2023
  - Publication: [Llemma: An Open Language Model For Mathematics](https://arxiv.org/abs/2310.10631)
  - Code: [GitHub](https://github.com/EleutherAI/math-lm)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/llemma_7b)

- Fuyu
  - Year: 2023
  - Publication: [Blog - Fuyu-8B: A Multimodal Architecture for AI Agents](https://www.adept.ai/blog/fuyu-8b)
  - Model weights: [HuggingFace models](https://huggingface.co/adept/fuyu-8b)

- Zephyr
  - Year: 2023
  - Publication: [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944)
  - Code: [GitHub](https://github.com/huggingface/alignment-handbook)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)

- CodeFusion
  - Year: 2023
  - Publication: [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680)
  - Code: [GitHub](https://github.com/microsoft/prose-benchmarks)

- FP8-LM
  - Year: 2023
  - Publication: [FP8-LM: Training FP8 Large Language Models](https://arxiv.org/abs/2310.18313)
  - Code: [GitHub](https://github.com/Azure/MS-AMP)
