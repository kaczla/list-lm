- GloVe
  - Year: 2014
  - Publication: [Blog - GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/) (2014-10-25)

- fastText
  - Year: 2016
  - Publication: [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606) (2016-07-15)
  - Code: [GitHub](https://github.com/facebookresearch/fastText)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/fasttext-en-vectors)

- MoE
  - Year: 2017
  - Publication: [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538) (2017-01-23)

- Transformer
  - Year: 2017
  - Publication: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017-06-12)

- RR-Transformer
  - Year: 2018
  - Publication: [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155) (2018-03-06)

- GPT
  - Year: 2018
  - Publication: [Blog - Improving Language Understanding with Unsupervised Learning](https://openai.com/blog/language-unsupervised) (2018-06-11)
  - Model weights: [HuggingFace models](https://huggingface.co/openai-gpt)

- flair
  - Year: 2018
  - Publication: [Contextual String Embeddings for Sequence Labeling](https://aclanthology.org/C18-1139) (2018-08-31)
  - Code: [GitHub](https://github.com/flairNLP/flair)

- BERT
  - Year: 2018
  - Publication: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018-10-11)

- Transformer-XL
  - Year: 2019
  - Publication: [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) (2019-01-09)
  - Code: [GitHub](https://github.com/kimiyoung/transformer-xl)

- LightConv / DynamicConv
  - Year: 2019
  - Publication: [Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/abs/1901.10430) (2019-01-29)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/pay_less_attention_paper)

- Evolved Transformer
  - Year: 2019
  - Publication: [The Evolved Transformer](https://arxiv.org/abs/1901.11117) (2019-01-30)

- GPT-2
  - Year: 2019
  - Publication: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019-02-14)
  - Code: [GitHub](https://github.com/openai/gpt-2)

- ERNIE
  - Year: 2019
  - Publication: [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) (2019-04-19)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)
  - Model weights: [HuggingFace models](https://huggingface.co/PaddlePaddle/ernie-1.0-base-zh)

- Sparse Transformer
  - Year: 2019
  - Publication: [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509) (2019-04-23)

- Adaptive Span
  - Year: 2019
  - Publication: [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799) (2019-05-19)
  - Code: [GitHub](https://github.com/facebookresearch/adaptive-span)

- XLNet
  - Year: 2019
  - Publication: [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) (2019-06-19)

- All-attention network
  - Year: 2019
  - Publication: [Augmenting Self-attention with Persistent Memory](https://arxiv.org/abs/1907.01470) (2019-07-02)

- RoBERTa
  - Year: 2019
  - Publication: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) (2019-07-26)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/roberta)

- ERNIE 2.0
  - Year: 2019
  - Publication: [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/abs/1907.12412) (2019-07-29)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)
  - Model weights: [HuggingFace models](https://huggingface.co/PaddlePaddle/ernie-2.0-base-en)

- StructBERT
  - Year: 2019
  - Publication: [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577) (2019-08-13)

- Adaptively Sparse Transformers
  - Year: 2019
  - Publication: [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015) (2019-08-30)

- CTRL
  - Year: 2019
  - Publication: [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) (2019-09-11)
  - Code: [GitHub](https://github.com/salesforce/ctrl)

- Megatron-LM
  - Year: 2019
  - Publication: [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) (2019-09-17)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- ALBERT
  - Year: 2019
  - Publication: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) (2019-09-26)
  - Code: [GitHub](https://github.com/google-research/ALBERT)

- kNN-LM
  - Year: 2019
  - Publication: [Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172) (2019-11-01)
  - Code: [GitHub](https://github.com/urvashik/knnlm)

- Sandwich Transformers
  - Year: 2019
  - Publication: [Improving Transformer Models by Reordering their Sublayers](https://arxiv.org/abs/1911.03864) (2019-11-10)
  - Video: [YouTube](https://www.youtube.com/watch?v=rFuuGEj3AhU)
  - Code: [GitHub](https://github.com/ofirpress/sandwich_transformer)

- TENER
  - Year: 2019
  - Publication: [TENER: Adapting Transformer Encoder for Named Entity Recognition](https://arxiv.org/abs/1911.04474) (2019-11-10)

- Big Transfer (BiT)
  - Year: 2019
  - Publication: [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) (2019-12-24)

- Feedback Transformer
  - Year: 2020
  - Publication: [Addressing Some Limitations of Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402) (2020-02-21)

- MiniLM
  - Year: 2020
  - Publication: [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957) (2020-02-25)

- Routing Transformer
  - Year: 2020
  - Publication: [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/abs/2003.05997) (2020-03-12)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/routing_transformer)

- Floater
  - Year: 2020
  - Publication: [Learning to Encode Position for Transformer with Continuous Dynamical Model](https://arxiv.org/abs/2003.09229) (2020-03-13)
  - Code: [GitHub](https://github.com/xuanqing94/FLOATER)

- MobileBERT
  - Year: 2020
  - Publication: [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) (2020-04-06)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/mobilebert)

- Poor Man's BERT
  - Year: 2020
  - Publication: [On the Effect of Dropping Layers of Pre-trained Transformer Models](https://arxiv.org/abs/2004.03844) (2020-04-08)
  - Code: [GitHub](https://github.com/hsajjad/transformers)

- Longformer
  - Year: 2020
  - Publication: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) (2020-04-10)
  - Code: [GitHub](https://github.com/allenai/longformer)

- MPNet
  - Year: 2020
  - Publication: [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) (2020-04-20)
  - Code: [GitHub](https://github.com/microsoft/MPNet)

- ColBERT
  - Year: 2020
  - Publication: [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832) (2020-04-27)

- DeeBERT
  - Year: 2020
  - Publication: [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993) (2020-04-27)
  - Code: [GitHub](https://github.com/castorini/DeeBERT)

- Synthesizer
  - Year: 2020
  - Publication: [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743) (2020-05-02)
  - Code: [GitHub](https://github.com/leaderj1001/Synthesizer-Rethinking-Self-Attention-Transformer-Models) / [GitHub](https://github.com/10-zin/Synthesizer)

- Adaptive Transformers
  - Year: 2020
  - Publication: [Adaptive Transformers for Learning Multimodal Representations](https://arxiv.org/abs/2005.07486) (2020-05-15)
  - Code: [GitHub](https://github.com/prajjwal1/adaptive_transformer)

- Retrieval-Augmented Generation (RAG)
  - Year: 2020
  - Publication: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) (2020-05-22)

- DeBERTa
  - Year: 2020
  - Publication: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) (2020-06-05)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-base)

- Linformer
  - Year: 2020
  - Publication: [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768) (2020-06-08)
  - Code: [GitHub](https://github.com/Kyan820815/Linformer)

- BRC
  - Year: 2020
  - Publication: [A bio-inspired bistable recurrent cell allows for long-lasting memory](https://arxiv.org/abs/2006.05252) (2020-06-09)
  - Code: [GitHub](https://github.com/nvecoven/BRC) / [GitHub](https://github.com/742617000027/nBRC)

- TUPE
  - Year: 2020
  - Publication: [Rethinking Positional Encoding in Language Pre-training](https://arxiv.org/abs/2006.15595) (2020-06-28)
  - Code: [GitHub](https://github.com/guolinke/TUPE)

- DeLighT
  - Year: 2020
  - Publication: [DeLighT: Deep and Light-weight Transformer](https://arxiv.org/abs/2008.00623) (2020-08-03)
  - Code: [GitHub](https://github.com/sacmehta/delight)

- DA-Transformer
  - Year: 2020
  - Publication: [DA-Transformer: Distance-aware Transformer](https://arxiv.org/abs/2010.06925) (2020-10-14)

- CharacterBERT
  - Year: 2020
  - Publication: [CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters](https://arxiv.org/abs/2010.10392) (2020-10-20)
  - Code: [GitHub](https://github.com/helboukkouri/character-bert)

- Diff pruning
  - Year: 2020
  - Publication: [Parameter-Efficient Transfer Learning with Diff Pruning](https://arxiv.org/abs/2012.07463) (2020-12-14)
  - Code: [GitHub](https://github.com/dguo98/DiffPruning)

- Informer
  - Year: 2020
  - Publication: [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) (2020-12-14)

- Key-Value Memory Feed-Forward
  - Year: 2020
  - Publication: [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913) (2020-12-29)

- MiniLMv2
  - Year: 2020
  - Publication: [MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://arxiv.org/abs/2012.15828) (2020-12-31)

- Shortformer
  - Year: 2020
  - Publication: [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832) (2020-12-31)
  - Code: [GitHub](https://github.com/ofirpress/shortformer)

- DeBERTaV2
  - Year: 2021
  - Publication: [Blog - Microsoft DeBERTa surpasses human performance on the SuperGLUE benchmark](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/) (2021-01-06)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-v2-xlarge)

- Switch Transformer
  - Year: 2021
  - Publication: [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) (2021-01-11)
  - Code: [GitHub](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)
  - Model weights: [HuggingFace models](https://huggingface.co/google/switch-c-2048)

- WuDao
  - Year: 2021
  - Publication: [Article: Five Key Facts Wu Dao 2.0: The Largest Transformer Model Ever Built](https://medium.com/dataseries/five-key-facts-wu-dao-2-0-the-largest-transformer-model-ever-built-19316159796b) (2021-01-11)

- UniT
  - Year: 2021
  - Publication: [UniT: Multimodal Multitask Learning with a Unified Transformer](https://arxiv.org/abs/2102.10772) (2021-02-22)
  - Code: [GitHub](https://github.com/facebookresearch/mmf)

- PLBART
  - Year: 2021
  - Publication: [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) (2021-03-10)
  - Code: [GitHub](https://github.com/wasiahmad/PLBART)
  - Model weights: [Direct link](https://github.com/wasiahmad/PLBART/tree/9bf5e12bb7374218c21546aa31020ef102a151e7#fine-tuning)

- GLM
  - Year: 2021
  - Publication: [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360) (2021-03-18)
  - Code: [GitHub](https://github.com/THUDM/GLM)
  - Model weights: [HuggingFace models](https://huggingface.co/BAAI/glm-10b)

- Megatron-LM v2
  - Year: 2021
  - Publication: [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021-04-09)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- 24hBERT / Academic Budget BERT
  - Year: 2021
  - Publication: [How to Train BERT with an Academic Budget](https://arxiv.org/abs/2104.07705) (2021-04-15)
  - Code: [GitHub](https://github.com/IntelLabs/academic-budget-bert)

- RoFormer
  - Year: 2021
  - Publication: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) (2021-04-20)
  - Code: [GitHub](https://github.com/ZhuiyiTechnology/roformer)
  - Model weights: [HuggingFace models](https://huggingface.co/junnyu/roformer_chinese_base)

- PanGu
  - Year: 2021
  - Publication: [PanGu-$\alpha$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation](https://arxiv.org/abs/2104.12369) (2021-04-26)

- FNet
  - Year: 2021
  - Publication: [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) (2021-05-09)
  - Video: [YouTube](https://www.youtube.com/watch?v=JJR3pBl78zw)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/f_net)

- EL-Attention
  - Year: 2021
  - Publication: [EL-Attention: Memory Efficient Lossless Attention for Generation](https://arxiv.org/abs/2105.04779) (2021-05-11)
  - Code: [GitHub](https://github.com/microsoft/fastseq)

- ByT5
  - Year: 2021
  - Publication: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) (2021-05-28)
  - Code: [GitHub](https://github.com/google-research/byt5)

- Luna
  - Year: 2021
  - Publication: [Luna: Linear Unified Nested Attention](https://arxiv.org/abs/2106.01540) (2021-06-03)
  - Code: [GitHub](https://github.com/XuezheMax/fairseq-apollo)

- GPT-J
  - Year: 2021
  - Publication: [Blog - GPT-J](https://www.eleuther.ai/artifacts/gpt-j) (2021-06-04)
  - Code: [GitHub](https://github.com/kingoflolz/mesh-transformer-jax/)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/gpt-j-6b)

- LoRA
  - Year: 2021
  - Publication: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) (2021-06-17)
  - Code: [GitHub](https://github.com/microsoft/LoRA)

- BitFit
  - Year: 2021
  - Publication: [BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199) (2021-06-18)
  - Code: [GitHub](https://github.com/benzakenelad/BitFit)

- Charformer
  - Year: 2021
  - Publication: [Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](https://arxiv.org/abs/2106.12672) (2021-06-23)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/charformer)

- ERNIE 3.0
  - Year: 2021
  - Publication: [ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137) (2021-07-05)

- ALiBi
  - Year: 2021
  - Publication: [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409) (2021-08-27)
  - Code: [GitHub](https://github.com/ofirpress/attention_with_linear_biases)
  - Model weights: [Private page](https://github.com/ofirpress/attention_with_linear_biases/tree/master/examples/language_model)

- CodeT5
  - Year: 2021
  - Publication: [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/abs/2109.00859) (2021-09-02)
  - Code: [GitHub](https://github.com/salesforce/CodeT5)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codet5-base)

- HyperCLOVA
  - Year: 2021
  - Publication: [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://arxiv.org/abs/2109.04650) (2021-09-10)

- Jurassic-1
  - Year: 2021
  - Publication: [Jurassic-1: Technical Details And Evaluation](https://assets.website-files.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) (2021-09-11)
  - Code: [GitHub](https://github.com/ai21labs/lm-evaluation)

- Primer
  - Year: 2021
  - Publication: [Primer: Searching for Efficient Transformers for Language Modeling](https://arxiv.org/abs/2109.08668) (2021-09-17)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/primer)

- PLATO-XL
  - Year: 2021
  - Publication: [PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation](https://arxiv.org/abs/2109.09519) (2021-09-20)

- T0
  - Year: 2021
  - Publication: [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207) (2021-10-15)
  - Code: [GitHub](https://github.com/bigscience-workshop/t-zero)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/T0)

- NormFormer
  - Year: 2021
  - Publication: [NormFormer: Improved Transformer Pretraining with Extra Normalization](https://arxiv.org/abs/2110.09456) (2021-10-18)

- DeBERTaV3
  - Year: 2021
  - Publication: [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing](https://arxiv.org/abs/2111.09543) (2021-11-18)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-v3-base)

- ColBERT v2
  - Year: 2021
  - Publication: [ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction](https://arxiv.org/abs/2112.01488) (2021-12-02)
  - Code: [GitHub](https://github.com/stanford-futuredata/ColBERT)
  - Model weights: [HuggingFace models](https://huggingface.co/colbert-ir/colbertv2.0)

- Gopher
  - Year: 2021
  - Publication: [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446) (2021-12-08)

- Memory efficient attention
  - Year: 2021
  - Publication: [Self-attention Does Not Need $O(n^2)$ Memory](https://arxiv.org/abs/2112.05682) (2021-12-10)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/memory_efficient_attention)

- XGLM
  - Year: 2021
  - Publication: [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) (2021-12-20)
  - Code: [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/xglm)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/xglm-7.5B)

- LaMDA
  - Year: 2022
  - Publication: [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239) (2022-01-20)

- Megatron-Turing NLG
  - Year: 2022
  - Publication: [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990) (2022-01-28)

- Memorizing Transformers (MemTRM)
  - Year: 2022
  - Publication: [Memorizing Transformers](https://arxiv.org/abs/2203.08913) (2022-03-16)

- CodeGen
  - Year: 2022
  - Publication: [CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis](https://arxiv.org/abs/2203.13474) (2022-03-25)
  - Code: [GitHub](https://github.com/salesforce/CodeGen)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen-16B-multi)

- BaGuaLu
  - Year: 2022
  - Publication: [BaGuaLu: targeting brain scale pretrained models with over 37 million cores](https://dl.acm.org/doi/abs/10.1145/3503221.3508417) (2022-03-28)

- Chinchilla
  - Year: 2022
  - Publication: [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022-03-29)

- NoPos
  - Year: 2022
  - Publication: [Transformer Language Models without Positional Encodings Still Learn Positional Information](https://arxiv.org/abs/2203.16634) (2022-03-30)
  - Code: [GitHub](https://github.com/adihaviv/NoPos)

- PaLM
  - Year: 2022
  - Publication: [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) (2022-04-05)

- GPT-NeoX
  - Year: 2022
  - Publication: [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) (2022-04-14)
  - Code: [GitHub](https://github.com/EleutherAI/gpt-neox)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/gpt-neox-20b)

- Jurassic-X
  - Year: 2022
  - Publication: [Blog - Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system) (2022-04-20)

- OPT
  - Year: 2022
  - Publication: [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) (2022-05-02)
  - Code: [GitHub](https://github.com/facebookresearch/metaseq)

- UL2
  - Year: 2022
  - Publication: [UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131) (2022-05-10)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/ul2)

- FlashAttention
  - Year: 2022
  - Publication: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135) (2022-05-27)
  - Code: [GitHub](https://github.com/HazyResearch/flash-attention)

- YaLM
  - Year: 2022
  - Publication: [Blog - Yandex Publishes YaLM 100B. Itâ€™s the Largest GPT-Like Neural Network in Open Source](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6) (2022-06-23)

- Minerva
  - Year: 2022
  - Publication: [Solving Quantitative Reasoning Problems with Language Models](https://arxiv.org/abs/2206.14858) (2022-06-29)

- CodeRL
  - Year: 2022
  - Publication: [CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning](https://arxiv.org/abs/2207.01780) (2022-07-05)
  - Code: [GitHub](https://github.com/salesforce/CodeRL)

- CALM
  - Year: 2022
  - Publication: [Confident Adaptive Language Modeling](https://arxiv.org/abs/2207.07061) (2022-07-14)
  - Code: [GitHub](https://github.com/google-research/t5x)

- LLM.int8
  - Year: 2022
  - Publication: [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339) (2022-08-15)
  - Code: [GitHub](https://github.com/TimDettmers/bitsandbytes)

- Petals
  - Year: 2022
  - Publication: [Petals: Collaborative Inference and Fine-tuning of Large Models](https://arxiv.org/abs/2209.01188) (2022-09-02)
  - Code: [GitHub](https://github.com/bigscience-workshop/petals)

- NeMo Megatron
  - Year: 2022
  - Publication: [HuggingFace model card - NeMo Megatron](https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B) (2022-09-10)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B)

- GLM-130B
  - Year: 2022
  - Publication: [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414) (2022-10-05)
  - Code: [GitHub](https://github.com/THUDM/GLM-130B)
  - Model weights: [HuggingFace models](https://huggingface.co/spaces/THUDM/GLM-130B)

- Magneto
  - Year: 2022
  - Publication: [Foundation Transformers](https://arxiv.org/abs/2210.06423) (2022-10-12)
  - Code: [GitHub](https://github.com/sunyt32/torchscale)

- TransNormer
  - Year: 2022
  - Publication: [The Devil in Linear Transformer](https://arxiv.org/abs/2210.10340) (2022-10-19)
  - Code: [GitHub](https://github.com/OpenNLPLab/Transnormer)

- FlanT5
  - Year: 2022
  - Publication: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) (2022-10-20)
  - Code: [GitHub](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)
  - Model weights: [HuggingFace models](https://huggingface.co/google/flan-t5-base)

- OLLA
  - Year: 2022
  - Publication: [OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks](https://arxiv.org/abs/2210.12924) (2022-10-24)

- BLOOMZ / mT0
  - Year: 2022
  - Publication: [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786) (2022-11-03)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloomz)

- BLOOM
  - Year: 2022
  - Publication: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100) (2022-11-09)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloom)

- Galactica
  - Year: 2022
  - Publication: [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085) (2022-11-16)
  - Code: [GitHub](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/galactica-6.7b)

- MegaBlocks
  - Year: 2022
  - Publication: [MegaBlocks: Efficient Sparse Training with Mixture-of-Experts](https://arxiv.org/abs/2211.15841) (2022-11-29)
  - Code: [GitHub](https://github.com/stanford-futuredata/megablocks)

- NPM
  - Year: 2022
  - Publication: [Nonparametric Masked Language Modeling](https://arxiv.org/abs/2212.01349) (2022-12-02)
  - Code: [GitHub](https://github.com/facebookresearch/NPM)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/npm)

- BioMedLM
  - Year: 2022
  - Publication: [Blog - BioMedLM: a Domain-Specific Large Language Model for Biomedical Text](https://www.mosaicml.com/blog/introducing-pubmed-gpt) (2022-12-15)
  - Code: [GitHub](https://github.com/stanford-crfm/BioMedLM)
  - Model weights: [HuggingFace models](https://huggingface.co/stanford-crfm/BioMedLM)

- LeX-Transformer
  - Year: 2022
  - Publication: [A Length-Extrapolatable Transformer](https://arxiv.org/abs/2212.10554) (2022-12-20)
  - Code: [GitHub](https://github.com/sunyt32/torchscale)

- OPT-IML
  - Year: 2022
  - Publication: [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/abs/2212.12017) (2022-12-22)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/opt-iml-30b)

- Cramming
  - Year: 2022
  - Publication: [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034) (2022-12-28)
  - Code: [GitHub](https://github.com/JonasGeiping/cramming)

- StitchNet
  - Year: 2023
  - Publication: [StitchNet: Composing Neural Networks from Pre-Trained Fragments](https://arxiv.org/abs/2301.01947) (2023-01-05)
  - Code: [GitHub](https://github.com/steerapi/stitchnet)

- SantaCoder
  - Year: 2023
  - Publication: [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) (2023-01-09)
  - Code: [GitHub](https://github.com/bigcode-project/Megatron-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/santacoder)

- NarrowBERT
  - Year: 2023
  - Publication: [NarrowBERT: Accelerating Masked Language Model Pretraining and Inference](https://arxiv.org/abs/2301.04761) (2023-01-11)

- Diff-Codegen
  - Year: 2023
  - Publication: [Blog - Diff Models - A New Way to Edit Code](https://carper.ai/diff-models-a-new-way-to-edit-code/) (2023-01-27)
  - Code: [GitHub](https://github.com/salesforce/CodeGen)
  - Model weights: [HuggingFace models](https://huggingface.co/CarperAI/diff-codegen-6b-v2)

- AltUp
  - Year: 2023
  - Publication: [Alternating Updates for Efficient Transformers](https://arxiv.org/abs/2301.13310) (2023-01-30)

- Exponential Signal Preserving Attention (E-SPA)
  - Year: 2023
  - Publication: [Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation](https://arxiv.org/abs/2302.10322) (2023-02-20)

- Hyena
  - Year: 2023
  - Publication: [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866) (2023-02-21)
  - Code: [GitHub](https://github.com/HazyResearch/safari)

- KOSMOS-1
  - Year: 2023
  - Publication: [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045) (2023-02-27)

- LLama
  - Year: 2023
  - Publication: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) (2023-02-27)
  - Code: [GitHub](https://github.com/facebookresearch/llama)
  - Model weights: [Private page - request required](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)

- SpikeGPT
  - Year: 2023
  - Publication: [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks](https://arxiv.org/abs/2302.13939) (2023-02-27)
  - Code: [GitHub](https://github.com/ridgerchu/SpikeGPT)

- Palmyra
  - Year: 2023
  - Publication: [Blog - Palmyra LLMs empower secure, enterprise-grade generative AI for business](https://writer.com/blog/palmyra/) (2023-03-02)
  - Model weights: [HuggingFace models](https://huggingface.co/Writer/palmyra-large)

- ParaFormer
  - Year: 2023
  - Publication: [ParaFormer: Parallel Attention Transformer for Efficient Feature Matching](https://arxiv.org/abs/2303.00941) (2023-03-02)

- PaLM-E
  - Year: 2023
  - Publication: [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) (2023-03-06)

- Jurassic-2
  - Year: 2023
  - Publication: [Blog - Announcing Jurassic-2 and Task-Specific APIs](https://www.ai21.com/blog/introducing-j2) (2023-03-09)

- StarChat
  - Year: 2023
  - Publication: [HuggingFace model card - Creating a Coding Assistant with StarCoder](https://huggingface.co/blog/starchat-alpha) (2023-03-09)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceH4/starchat-alpha)

- Alpaca
  - Year: 2023
  - Publication: [Alpaca: A Strong Open-Source Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html) (2023-03-13)
  - Code: [GitHub](https://github.com/tatsu-lab/stanford_alpaca)

- FlexGen
  - Year: 2023
  - Publication: [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/abs/2303.06865) (2023-03-13)
  - Code: [GitHub](https://github.com/FMInference/FlexGen)

- TWM
  - Year: 2023
  - Publication: [Transformer-based World Models Are Happy With 100k Interactions](https://arxiv.org/abs/2303.07109) (2023-03-13)
  - Code: [GitHub](https://github.com/jrobine/twm)

- GPT-4
  - Year: 2023
  - Publication: [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) (2023-03-15)

- alpaca-opt
  - Year: 2023
  - Publication: [README - alpaca-opt-6.7b repository](https://github.com/Manuel030/alpaca-opt/blob/0466819d3647568a125530488f79c5627dd292ed/README.md) (2023-03-16)
  - Code: [GitHub](https://github.com/Manuel030/alpaca-opt)
  - Model weights: [HuggingFace models](https://huggingface.co/Manuel030/alpaca-opt-6.7b)

- GALPACA
  - Year: 2023
  - Publication: [HuggingFace model card - GALPACA](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b) (2023-03-16)
  - Model weights: [HuggingFace models](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b)

- AlpacOOM
  - Year: 2023
  - Publication: [Tweet: Alpaca + BLOOM = Alpacoom](https://twitter.com/alfredplpl/status/1636858660218617857) (2023-03-17)
  - Model weights: [HuggingFace models](https://huggingface.co/mrm8488/Alpacoom)

- CoLT5
  - Year: 2023
  - Publication: [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/abs/2303.09752) (2023-03-17)

- GPT4All
  - Year: 2023
  - Publication: [Tweet: Today we're releasing GPT4All, an assistant-style chatbot distilled from 430k GPT-3.5-Turbo outputs that you can run on your laptop.](https://twitter.com/nomic_ai/status/1640834838578995202) (2023-03-28)
  - Code: [GitHub](https://github.com/nomic-ai/gpt4all)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/gpt4all-lora)

- LLaMA-Adapter
  - Year: 2023
  - Publication: [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199) (2023-03-28)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- OpenFlamingo
  - Year: 2023
  - Publication: [Blog - ANNOUNCING OPENFLAMINGO: AN OPEN-SOURCE FRAMEWORK FOR TRAINING VISION-LANGUAGE MODELS WITH IN-CONTEXT LEARNING](https://laion.ai/blog/open-flamingo/) (2023-03-28)
  - Code: [GitHub](https://github.com/mlfoundations/open_flamingo)
  - Model weights: [HuggingFace models](https://huggingface.co/openflamingo/OpenFlamingo-9B)

- BloombergGPT
  - Year: 2023
  - Publication: [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564) (2023-03-30)

- CodeGeeX
  - Year: 2023
  - Publication: [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X](https://arxiv.org/abs/2303.17568) (2023-03-30)
  - Code: [GitHub](https://github.com/THUDM/CodeGeeX)
  - Model weights: [Private page - request required](https://models.aminer.cn/codegeex/download/request)

- CodeGeeX2
  - Year: 2023
  - Publication: [README - CodeGeeX2 repository](https://github.com/THUDM/CodeGeeX2/blob/2812282082f0be535235214e379119a69e865415/README_EN.md) (2023-03-30)
  - Code: [GitHub](https://github.com/THUDM/CodeGeeX2)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/codegeex2-6b)

- GeoV
  - Year: 2023
  - Publication: [README - GeoV repository](https://github.com/geov-ai/geov/blob/4bd51d2d81168ca1d8917aeb7513a1d95feaf727/readme.md) (2023-03-30)
  - Code: [GitHub](https://github.com/geov-ai/geov)
  - Model weights: [HuggingFace models](https://huggingface.co/GeoV/GeoV-9b)

- Vicuna
  - Year: 2023
  - Publication: [Blog - Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality](https://lmsys.org/blog/2023-03-30-vicuna/) (2023-03-30)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/vicuna-7b-v1.5)

- GPTrillion
  - Year: 2023
  - Publication: [Tweet: GPTrillion: a 1.5T Parameter Open-Source Model](https://twitter.com/BananaDev_/status/1642211220072673286) (2023-04-01)

- Koala
  - Year: 2023
  - Publication: [Blog - Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/) (2023-04-03)
  - Code: [GitHub](https://github.com/young-geng/EasyLM)
  - Model weights: [HuggingFace models](https://huggingface.co/young-geng/koala)

- Pythia
  - Year: 2023
  - Publication: [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373) (2023-04-03)
  - Code: [GitHub](https://github.com/EleutherAI/pythia)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/pythia-70m)

- Cerebras-GPT
  - Year: 2023
  - Publication: [Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster](https://arxiv.org/abs/2304.03208) (2023-04-06)
  - Code: [GitHub](https://github.com/Cerebras/modelzoo)
  - Model weights: [HuggingFace models](https://huggingface.co/cerebras/Cerebras-GPT-13B)

- Dolly
  - Year: 2023
  - Publication: [Blog - Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) (2023-04-12)
  - Code: [GitHub](https://github.com/databrickslabs/dolly)
  - Model weights: [HuggingFace models](https://huggingface.co/databricks/dolly-v2-12b)

- GPT4All-J
  - Year: 2023
  - Publication: [Tweet: Announcing GPT4All-J: The First Apache-2 Licensed Chatbot That Runs Locally on Your Machine](https://twitter.com/andriy_mulyar/status/1646622168350875655) (2023-04-13)
  - Code: [GitHub](https://github.com/nomic-ai/gpt4all)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/gpt4all-j)

- OpenAssistant
  - Year: 2023
  - Publication: [OpenAssistant Conversations -- Democratizing Large Language Model Alignment](https://arxiv.org/abs/2304.07327) (2023-04-14)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5)

- LLaVA
  - Year: 2023
  - Publication: [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (2023-04-17)
  - Code: [GitHub](https://github.com/haotian-liu/LLaVA)
  - Model weights: [HuggingFace models](https://huggingface.co/liuhaotian/llava-v1.5-7b)

- umT5
  - Year: 2023
  - Publication: [UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining](https://arxiv.org/abs/2304.09151) (2023-04-18)
  - Code: [GitHub](https://github.com/google-research/t5x)
  - Model weights: [Direct link](https://github.com/google-research/t5x/blob/main/docs/models.md#umt5-checkpoints)

- MOSS
  - Year: 2023
  - Publication: [README - MOSS repository](https://github.com/OpenLMLab/MOSS/blob/cb43caf8f662f60c855c70e723b0cb9e943db4c0/README_en.md) (2023-04-19)
  - Code: [GitHub](https://github.com/OpenLMLab/MOSS)
  - Model weights: [HuggingFace models](https://huggingface.co/fnlp/moss-moon-003-base)

- RMT
  - Year: 2023
  - Publication: [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/abs/2304.11062) (2023-04-19)
  - Code: [GitHub](https://github.com/booydar/t5-experiments/tree/scaling-report)

- StableLM-Alpha
  - Year: 2023
  - Publication: [Blog - Stability AI Launches the First of its StableLM Suite of Language Models](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) (2023-04-19)
  - Code: [GitHub](https://github.com/stability-AI/stableLM/)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)

- MiniGPT-4
  - Year: 2023
  - Publication: [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592) (2023-04-20)
  - Code: [GitHub](https://github.com/Vision-CAIR/MiniGPT-4)

- MPT-1b-RedPajama-200b
  - Year: 2023
  - Publication: [HuggingFace model card - MPT-1b-RedPajama-200b](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b) (2023-04-20)
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)

- WizardLM
  - Year: 2023
  - Publication: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244) (2023-04-24)
  - Code: [GitHub](https://github.com/nlpxucan/abcd)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardLM-30B-V1.0)

- FastChat-T5
  - Year: 2023
  - Publication: [Tweet: We are excited to release FastChat-T5: our compact and commercial-friendly chatbot!](https://twitter.com/lmsysorg/status/1652037026705985537) (2023-04-28)
  - Code: [GitHub](https://github.com/lm-sys/FastChat)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)

- Lamini
  - Year: 2023
  - Publication: [Blog - Introducing Lamini, the LLM Engine for Rapidly Customizing Models](https://lamini.ai/blog/introducing-lamini) (2023-04-28)
  - Code: [GitHub](https://github.com/lamini-ai/lamini)
  - Model weights: [HuggingFace models](https://huggingface.co/lamini/instruct-peft-tuned-12b)

- LLaMA-Adapter V2
  - Year: 2023
  - Publication: [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://arxiv.org/abs/2304.15010) (2023-04-28)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- OpenLLaMA
  - Year: 2023
  - Publication: [README - OpenLLaMA repository](https://github.com/openlm-research/open_llama/blob/6e7f73eab7e799e2464f38ed977e537bae02873e/README.md) (2023-04-29)
  - Code: [GitHub](https://github.com/openlm-research/open_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/openlm-research/open_llama_13b)

- Unlimiformer
  - Year: 2023
  - Publication: [Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/abs/2305.01625) (2023-05-02)
  - Code: [GitHub](https://github.com/abertsch72/unlimiformer)
  - Model weights: [HuggingFace models](https://huggingface.co/abertsch/unlimiformer-bart-booksum-retrieval)

- CodeGen2
  - Year: 2023
  - Publication: [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309) (2023-05-03)
  - Code: [GitHub](https://github.com/salesforce/CodeGen2)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen2-16B)

- ReplitLM
  - Year: 2023
  - Publication: [Tweet: Last night, we released our new complete code model: replit-code-v1-3b.](https://twitter.com/Replit/status/1653802301331759104) (2023-05-03)
  - Code: [GitHub](https://github.com/replit/ReplitLM)
  - Model weights: [HuggingFace models](https://huggingface.co/replit/replit-code-v1-3b)

- MPT-7B
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b) (2023-05-05)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b)

- Otter
  - Year: 2023
  - Publication: [Otter: A Multi-Modal Model with In-Context Instruction Tuning](https://arxiv.org/abs/2305.03726) (2023-05-05)
  - Code: [GitHub](https://github.com/Luodian/otter)
  - Model weights: [HuggingFace models](https://huggingface.co/luodian/otter-9b-hf)

- Open-source PaLM
  - Year: 2023
  - Publication: [README - Open-source PaLM repository](https://github.com/conceptofmind/PaLM/blob/dc2425df678d300b9d5b65d028fc70293c01a589/README.md) (2023-05-07)
  - Code: [GitHub](https://github.com/conceptofmind/PaLM)
  - Model weights: [HuggingFace models](https://huggingface.co/conceptofmind/palm-1b)

- Multimodal-GPT
  - Year: 2023
  - Publication: [MultiModal-GPT: A Vision and Language Model for Dialogue with Humans](https://arxiv.org/abs/2305.04790) (2023-05-08)
  - Code: [GitHub](https://github.com/open-mmlab/Multimodal-GPT)
  - Model weights: [Direct link](https://download.openmmlab.com/mmgpt/v0/mmgpt-lora-v0-release.pt)

- TNN
  - Year: 2023
  - Publication: [Toeplitz Neural Network for Sequence Modeling](https://arxiv.org/abs/2305.04749) (2023-05-08)
  - Code: [GitHub](https://github.com/OpenNLPLab/Tnn)

- ImageBind
  - Year: 2023
  - Publication: [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665) (2023-05-09)
  - Code: [GitHub](https://github.com/facebookresearch/ImageBind)
  - Model weights: [Direct link](https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth)

- StarCoder
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161) (2023-05-09)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoder)

- DLite
  - Year: 2023
  - Publication: [Blog - Announcing DLite V2: Lightweight, Open LLMs That Can Run Anywhere](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e) (2023-05-10)
  - Model weights: [HuggingFace models](https://huggingface.co/aisquared/dlite-v2-1_5b)

- PaLM 2
  - Year: 2023
  - Publication: [Blog - Introducing PaLM 2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) (2023-05-10)

- MEGABYTE
  - Year: 2023
  - Publication: [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185) (2023-05-12)

- CodeT5+
  - Year: 2023
  - Publication: [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922) (2023-05-13)
  - Code: [GitHub](https://github.com/salesforce/CodeT5)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codet5p-16b)

- DarkBERT
  - Year: 2023
  - Publication: [DarkBERT: A Language Model for the Dark Side of the Internet](https://arxiv.org/abs/2305.08596) (2023-05-15)

- LIMA
  - Year: 2023
  - Publication: [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) (2023-05-18)

- mLongT5
  - Year: 2023
  - Publication: [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://arxiv.org/abs/2305.11129) (2023-05-18)
  - Code: [GitHub](https://github.com/google-research/longt5)

- BLOOMChat
  - Year: 2023
  - Publication: [Blog - BLOOMChat: a New Open Multilingual Chat LLM](https://sambanova.ai/blog/introducing-bloomchat-176b-the-multilingual-chat-based-llm/) (2023-05-19)
  - Model weights: [HuggingFace models](https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1)

- LLM-Pruner
  - Year: 2023
  - Publication: [LLM-Pruner: On the Structural Pruning of Large Language Models](https://arxiv.org/abs/2305.11627) (2023-05-19)
  - Code: [GitHub](https://github.com/horseee/LLM-Pruner)

- Aurora genAI
  - Year: 2023
  - Publication: [Blog - Intel Announces Aurora genAI, Generative AI Model With 1 Trillion Parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters) (2023-05-22)

- RWKV
  - Year: 2023
  - Publication: [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048) (2023-05-22)
  - Code: [GitHub](https://github.com/BlinkDL/RWKV-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)

- Goat
  - Year: 2023
  - Publication: [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks](https://arxiv.org/abs/2305.14201) (2023-05-23)
  - Code: [GitHub](https://github.com/liutiedong/goat)
  - Model weights: [HuggingFace models](https://huggingface.co/tiedong/goat-lora-7b)

- QLoRA
  - Year: 2023
  - Publication: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) (2023-05-23)
  - Code: [GitHub](https://github.com/artidoro/qlora)

- UltraLLaMA
  - Year: 2023
  - Publication: [Enhancing Chat Language Models by Scaling High-quality Instructional Conversations](https://arxiv.org/abs/2305.14233) (2023-05-23)
  - Code: [GitHub](https://github.com/thunlp/UltraChat)
  - Model weights: [HuggingFace models](https://huggingface.co/openbmb/UltraLM-13b)

- Gorilla
  - Year: 2023
  - Publication: [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334) (2023-05-24)
  - Code: [GitHub](https://github.com/ShishirPatil/gorilla)

- Backpacks
  - Year: 2023
  - Publication: [Backpack Language Models](https://arxiv.org/abs/2305.16765) (2023-05-26)
  - Code: [GitHub](https://github.com/john-hewitt/backpacks-flash-attn)
  - Model weights: [HuggingFace models](https://huggingface.co/stanfordnlp/backpack-gpt2)

- BiomedGPT
  - Year: 2023
  - Publication: [BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks](https://arxiv.org/abs/2305.17100) (2023-05-26)
  - Code: [GitHub](https://github.com/taokz/BiomedGPT)

- Refact-1.6B
  - Year: 2023
  - Publication: [Applying All Recent Innovations To Train a Code Model](https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/) (2023-05-26)
  - Code: [GitHub](https://github.com/smallcloudai/refact/)
  - Model weights: [HuggingFace models](https://huggingface.co/smallcloudai/Refact-1_6B-fim)

- MeZO
  - Year: 2023
  - Publication: [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333) (2023-05-27)
  - Code: [GitHub](https://github.com/princeton-nlp/MeZO)

- NoPE
  - Year: 2023
  - Publication: [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466) (2023-05-31)

- MEFT
  - Year: 2023
  - Publication: [Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning](https://arxiv.org/abs/2306.00477) (2023-06-01)
  - Code: [GitHub](https://github.com/baohaoLiao/mefts)

- Falcon
  - Year: 2023
  - Publication: [Blog - The Falcon has landed in the Hugging Face ecosystem](https://huggingface.co/blog/falcon) (2023-06-05)
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/falcon-40b)

- Orca
  - Year: 2023
  - Publication: [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.02707) (2023-06-05)

- SpQR
  - Year: 2023
  - Publication: [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/abs/2306.03078) (2023-06-05)
  - Code: [GitHub](https://github.com/Vahe1994/SpQR)

- RedPajama-INCITE
  - Year: 2023
  - Publication: [Blog - RedPajama 7B now available, instruct model outperforms all open 7B models on HELM benchmarks](https://www.together.xyz/blog/redpajama-7b) (2023-06-06)
  - Code: [GitHub](https://github.com/togethercomputer/redpajama.cpp)
  - Model weights: [HuggingFace models](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base)

- INSTRUCTEVAL
  - Year: 2023
  - Publication: [INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2306.04757) (2023-06-07)
  - Code: [GitHub](https://github.com/declare-lab/instruct-eval)
  - Model weights: [HuggingFace models](https://huggingface.co/declare-lab/flan-alpaca-xxl)

- MoLM
  - Year: 2023
  - Publication: [ModuleFormer: Modularity Emerges from Mixture-of-Experts](https://arxiv.org/abs/2306.04640) (2023-06-07)
  - Code: [GitHub](https://github.com/IBM/ModuleFormer)
  - Model weights: [HuggingFace models](https://huggingface.co/ibm/MoLM-350M-4B)

- Tulu
  - Year: 2023
  - Publication: [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751) (2023-06-07)
  - Code: [GitHub](https://github.com/allenai/open-instruct)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/tulu-65b)

- FinGPT
  - Year: 2023
  - Publication: [FinGPT: Open-Source Financial Large Language Models](https://arxiv.org/abs/2306.06031) (2023-06-09)
  - Code: [GitHub](https://github.com/ai4finance-foundation/fingpt)

- LongMem
  - Year: 2023
  - Publication: [Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.07174) (2023-06-12)
  - Code: [GitHub](https://github.com/Victorwz/LongMem)

- h2ogpt
  - Year: 2023
  - Publication: [h2oGPT: Democratizing Large Language Models](https://arxiv.org/abs/2306.08161) (2023-06-13)
  - Code: [GitHub](https://github.com/h2oai/h2ogpt)
  - Model weights: [HuggingFace models](https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b)

- SqueezeLLM
  - Year: 2023
  - Publication: [SqueezeLLM: Dense-and-Sparse Quantization](https://export.arxiv.org/abs/2306.07629) (2023-06-13)
  - Code: [GitHub](https://github.com/SqueezeAILab/SqueezeLLM)

- WizardCoder
  - Year: 2023
  - Publication: [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568) (2023-06-14)
  - Code: [GitHub](https://github.com/nlpxucan/abcd)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0)

- Macaw-LLM
  - Year: 2023
  - Publication: [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/abs/2306.09093) (2023-06-15)
  - Code: [GitHub](https://github.com/lyuchenyang/Macaw-LLM)

- LOMO
  - Year: 2023
  - Publication: [Full Parameter Fine-tuning for Large Language Models with Limited Resources](https://arxiv.org/abs/2306.09782) (2023-06-16)
  - Code: [GitHub](https://github.com/OpenLMLab/LOMO)

- phi-1
  - Year: 2023
  - Publication: [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) (2023-06-20)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-1)

- Inflection-1
  - Year: 2023
  - Publication: [Blog - Inflection-1: Piâ€™s Best-in-Class LLM](https://inflection.ai/inflection-1) (2023-06-22)

- ChatGLM-6B
  - Year: 2023
  - Publication: [README - ChatGLM-6B repository](https://github.com/THUDM/ChatGLM-6B/blob/d835c4b0017310d53b98bad98f68671fa861d158/README_en.md) (2023-06-25)
  - Code: [GitHub](https://github.com/THUDM/ChatGLM-6B)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/chatglm-6b)

- Kosmos-2
  - Year: 2023
  - Publication: [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) (2023-06-26)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/kosmos-2)
  - Model weights: [direct link](https://conversationhub.blob.core.windows.net/beit-share-public/kosmos-2/kosmos-2.pt?sv=2021-10-04&st=2023-06-08T11%3A16%3A02Z&se=2033-06-09T11%3A16%3A00Z&sr=c&sp=r&sig=N4pfCVmSeq4L4tS8QbrFVsX6f6q844eft8xSuXdxU48%3D)

- Position Interpolation
  - Year: 2023
  - Publication: [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/abs/2306.15595) (2023-06-27)

- XGen
  - Year: 2023
  - Publication: [Blog - Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length](https://blog.salesforceairesearch.com/xgen/) (2023-06-28)
  - Code: [GitHub](https://github.com/salesforce/xGen)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/xgen-7b-8k-base)

- LongChat
  - Year: 2023
  - Publication: [Blog - How Long Can Open-Source LLMs Truly Promise on Context Length?](https://lmsys.org/blog/2023-06-29-longchat/) (2023-06-29)
  - Code: [GitHub](https://github.com/DachengLi1/LongChat)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/longchat-7b-v1.5-32k)

- NTK
  - Year: 2023
  - Publication: [Blog - NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/) (2023-06-29)

- LongNet
  - Year: 2023
  - Publication: [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486) (2023-07-05)
  - Code: [GitHub](https://github.com/microsoft/unilm)

- CodeGen2.5
  - Year: 2023
  - Publication: [Blog - CodeGen2.5: Small, but mighty](https://blog.salesforceairesearch.com/codegen25/) (2023-07-06)
  - Code: [GitHub](https://github.com/salesforce/CodeGen2)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen25-7b-multi)

- InternLM
  - Year: 2023
  - Publication: [InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities](https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf) (2023-07-06)
  - Code: [GitHub](https://github.com/InternLM/InternLM)
  - Model weights: [HuggingFace models](https://huggingface.co/internlm/internlm-7b)

- LongLLaMA
  - Year: 2023
  - Publication: [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/abs/2307.03170) (2023-07-06)
  - Code: [GitHub](https://github.com/CStanKonrad/long_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/syzymon/long_llama_3b)

- Redmond-Hermes-Coder
  - Year: 2023
  - Publication: [Tweet: Releasing Redmond-Hermes-Coder, a finetune of Wizardcoder on our dataset.](https://twitter.com/NousResearch/status/1674992144170340353) (2023-07-07)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Redmond-Hermes-Coder)

- PolyLM
  - Year: 2023
  - Publication: [PolyLM: An Open Source Polyglot Large Language Model](https://arxiv.org/abs/2307.06018) (2023-07-12)
  - Code: [GitHub](https://github.com/DAMO-NLP-MT/PolyLM)
  - Model weights: [HuggingFace models](https://huggingface.co/DAMO-NLP-MT/polylm-13b)

- FlashAttention-2
  - Year: 2023
  - Publication: [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691) (2023-07-17)
  - Code: [GitHub](https://github.com/Dao-AILab/flash-attention)

- RetNet
  - Year: 2023
  - Publication: [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621) (2023-07-17)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/retnet)

- Llama 2
  - Year: 2023
  - Publication: [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288) (2023-07-18)
  - Code: [GitHub](https://github.com/facebookresearch/llama)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Llama-2-7b)

- LRPE
  - Year: 2023
  - Publication: [Linearized Relative Positional Encoding](https://arxiv.org/abs/2307.09270) (2023-07-18)
  - Code: [GitHub](https://github.com/OpenNLPLab/Lrpe)

- MPT-7B-8k
  - Year: 2023
  - Publication: [Blog - Announcing MPT-7B-8K: 8K Context Length for Document Understanding](https://www.mosaicml.com/blog/long-context-mpt-7b-8k) (2023-07-18)
  - Code: [GitHub](https://github.com/CStanKonrad/long_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-8k)

- PanGu-Coder2
  - Year: 2023
  - Publication: [PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback](https://arxiv.org/abs/2307.14936) (2023-07-27)

- Alfred-40B-0723
  - Year: 2023
  - Publication: [Blog - Introducing Alfred-40B-0723](https://www.lighton.ai/blog/lighton-s-blog-4/introducing-alfred-40b-0723-38) (2023-07-31)
  - Model weights: [HuggingFace models](https://huggingface.co/lightonai/alfred-40b-0723)

- Baby-CoThought
  - Year: 2023
  - Publication: [Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models](https://arxiv.org/abs/2308.01684) (2023-08-03)
  - Code: [GitHub](https://github.com/oooranz/Baby-CoThought)
  - Model weights: [HuggingFace models](https://huggingface.co/yaanhaan/Baby-CoThought)

- OpenHermes
  - Year: 2023
  - Publication: [HuggingFace model card - OpenHermes](https://huggingface.co/teknium/OpenHermes-13B) (2023-08-06)
  - Model weights: [HuggingFace models](https://huggingface.co/teknium/OpenHermes-13B)

- StableCode Complete Alpha
  - Year: 2023
  - Publication: [Blog - Announcing StableCode](https://stability.ai/blog/stablecode-llm-generative-ai-coding) (2023-08-08)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablecode-completion-alpha-3b)

- Claude Instant 1.2
  - Year: 2023
  - Publication: [Blog - Releasing Claude Instant 1.2](https://www.anthropic.com/index/releasing-claude-instant-1-2) (2023-08-09)

- OctoCoder
  - Year: 2023
  - Publication: [HuggingFace model card - OctoCoder](https://huggingface.co/bigcode/octocoder) (2023-08-14)
  - Code: [GitHub](https://github.com/bigcode-project/octopack)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/octocoder)

- OctoGeeX
  - Year: 2023
  - Publication: [HuggingFace model card - OctoGeeX](https://huggingface.co/bigcode/octogeex) (2023-08-14)
  - Code: [GitHub](https://github.com/bigcode-project/octopack)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/octogeex)

- OctoPack
  - Year: 2023
  - Publication: [OctoPack: Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2308.07124) (2023-08-14)
  - Code: [GitHub](https://github.com/bigcode-project/octopack)

- Aquila
  - Year: 2023
  - Publication: [README - Aquila repository](https://github.com/FlagAI-Open/FlagAI/blob/e3062883ca5156d5c6f6b1992a5f1a08edbd437c/examples/Aquila/README_en.md) (2023-08-15)
  - Code: [GitHub](https://github.com/FlagAI-Open/FlagAI)
  - Model weights: [HuggingFace models](https://huggingface.co/BAAI/Aquila-7B)

- DeciCoder
  - Year: 2023
  - Publication: [Blog - Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation](https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/) (2023-08-15)
  - Model weights: [HuggingFace models](https://huggingface.co/Deci/DeciCoder-1b)

- WizardMath
  - Year: 2023
  - Publication: [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583) (2023-08-18)
  - Code: [GitHub](https://github.com/nlpxucan/abcd)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardMath-7B-V1.0)

- SQLCoder
  - Year: 2023
  - Publication: [Blog - Open-sourcing SQLCoder: a state-of-the-art LLM for SQL generation](https://defog.ai/blog/open-sourcing-sqlcoder/) (2023-08-20)
  - Code: [GitHub](https://github.com/defog-ai/sqlcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/defog/sqlcoder)

- Giraffe
  - Year: 2023
  - Publication: [Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882) (2023-08-21)
  - Code: [GitHub](https://github.com/abacusai/Long-Context)
  - Model weights: [HuggingFace models](https://huggingface.co/abacusai/Giraffe-v2-13b-32k)

- OpenMoE
  - Year: 2023
  - Publication: [Blog - OpenMoE v0.2 Release](https://xuefuzhao.notion.site/Aug-2023-OpenMoE-v0-2-Release-43808efc0f5845caa788f2db52021879) (2023-08-21)
  - Code: [GitHub](https://github.com/XueFuzhao/OpenMoE)
  - Model weights: [HuggingFace models](https://huggingface.co/fuzhao/OpenMoE_Base)

- Idefics
  - Year: 2023
  - Publication: [HuggingFace model card - Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model](https://huggingface.co/blog/idefics) (2023-08-22)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct)

- Code Llama
  - Year: 2023
  - Publication: [Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950) (2023-08-24)
  - Code: [GitHub](https://github.com/facebookresearch/codellama)
  - Model weights: [Direct link](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

- Phind-CodeLlama
  - Year: 2023
  - Publication: [HuggingFace model card - Phind-CodeLlama](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2) (2023-08-28)
  - Model weights: [HuggingFace models](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2)

- Jais
  - Year: 2023
  - Publication: [Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models](https://arxiv.org/abs/2308.16149) (2023-08-30)
  - Model weights: [HuggingFace models](https://huggingface.co/core42/jais-13b)

- Yarn
  - Year: 2023
  - Publication: [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071) (2023-08-31)
  - Code: [GitHub](https://github.com/jquesnelle/yarn)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k)

- One Wide FFN
  - Year: 2023
  - Publication: [One Wide Feedforward is All You Need](https://arxiv.org/abs/2309.01826) (2023-09-04)

- Falcon-180B
  - Year: 2023
  - Publication: [Blog - Spread Your Wings: Falcon 180B is here](https://huggingface.co/blog/falcon-180b) (2023-09-06)
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/falcon-180B)

- MathGLM
  - Year: 2023
  - Publication: [GPT Can Solve Mathematical Problems Without a Calculator](https://arxiv.org/abs/2309.03241) (2023-09-06)
  - Code: [GitHub](https://github.com/THUDM/MathGLM)
  - Model weights: [Direct link](https://cloud.tsinghua.edu.cn/d/92127e3a1b4144db8d13/)

- DoLa
  - Year: 2023
  - Publication: [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883) (2023-09-07)
  - Code: [GitHub](https://github.com/voidism/DoLa)

- FLM-101B
  - Year: 2023
  - Publication: [FLM-101B: An Open LLM and How to Train It with $100K Budget](https://arxiv.org/abs/2309.03852) (2023-09-07)
  - Model weights: [HuggingFace models](https://huggingface.co/CofeAI/FLM-101B)

- Persimmon-8B
  - Year: 2023
  - Publication: [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) (2023-09-07)
  - Code: [GitHub](https://github.com/persimmon-ai-labs/adept-inference)
  - Model weights: [Direct link](https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar)

- MAmmoTH
  - Year: 2023
  - Publication: [MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning](https://arxiv.org/abs/2309.05653) (2023-09-11)
  - Code: [GitHub](https://github.com/TIGER-AI-Lab/MAmmoTH)
  - Model weights: [HuggingFace models](https://huggingface.co/TIGER-Lab/MAmmoTH-Coder-7B)

- MoV and MoLoRA
  - Year: 2023
  - Publication: [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://arxiv.org/abs/2309.05444) (2023-09-11)
  - Code: [GitHub](https://github.com/for-ai/parameter-efficient-moe)

- phi-1.5
  - Year: 2023
  - Publication: [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) (2023-09-11)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-1_5)

- OpenHermes-2-Mistral
  - Year: 2023
  - Publication: [HuggingFace model card - OpenHermes-2-Mistral](https://huggingface.co/teknium/OpenHermes-2-Mistral-7B) (2023-09-12)
  - Model weights: [HuggingFace models](https://huggingface.co/teknium/OpenHermes-2-Mistral-7B)

- vLLM
  - Year: 2023
  - Publication: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180) (2023-09-12)
  - Code: [GitHub](https://github.com/vllm-project/vllm)

- DeciLM
  - Year: 2023
  - Publication: [Blog - 15 times Faster than Llama 2: Introducing DeciLM â€“ NAS-Generated LLM with Variable GQA](https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/) (2023-09-13)
  - Model weights: [HuggingFace models](https://huggingface.co/Deci/DeciLM-6b)

- RAIN
  - Year: 2023
  - Publication: [RAIN: Your Language Models Can Align Themselves without Finetuning](https://arxiv.org/abs/2309.07124) (2023-09-13)
  - Code: [GitHub](https://github.com/SafeAILab/RAIN)

- Xwin-LM
  - Year: 2023
  - Publication: [README - README -](https://github.com/Xwin-LM/Xwin-LM/blob/25862b325ee2a5b7c3c0007728eaeeb3daf5f05f/README.md) (2023-09-16)
  - Code: [GitHub](https://github.com/Xwin-LM/Xwin-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/Xwin-LM/Xwin-LM-13B-V0.2)

- AdaptLLM
  - Year: 2023
  - Publication: [Adapting Large Language Models via Reading Comprehension](https://arxiv.org/abs/2309.09530) (2023-09-18)
  - Code: [GitHub](https://github.com/microsoft/LMOps)

- Baichuan 2
  - Year: 2023
  - Publication: [Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305) (2023-09-19)
  - Code: [GitHub](https://github.com/baichuan-inc/Baichuan2)
  - Model weights: [HuggingFace models](https://huggingface.co/baichuan-inc/Baichuan2-13B-Base)

- PoSE
  - Year: 2023
  - Publication: [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training](https://arxiv.org/abs/2309.10400) (2023-09-19)
  - Code: [GitHub](https://github.com/dwzhu-pku/PoSE)
  - Model weights: [HuggingFace models](https://huggingface.co/dwzhu/LLaMA-7B-PoSE-Linear-16k)

- BTLM-3B-8k
  - Year: 2023
  - Publication: [BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model](https://arxiv.org/abs/2309.11568) (2023-09-20)
  - Model weights: [HuggingFace models](https://huggingface.co/cerebras/btlm-3b-8k-base)

- Kosmos-2.5
  - Year: 2023
  - Publication: [Kosmos-2.5: A Multimodal Literate Model](https://arxiv.org/abs/2309.11419) (2023-09-20)

- Nous-Capybara
  - Year: 2023
  - Publication: [HuggingFace model card - Nous-Capybara](https://huggingface.co/NousResearch/Nous-Capybara-7B-GGUF) (2023-09-20)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Nous-Capybara-7B-GGUF)

- OpenChat
  - Year: 2023
  - Publication: [OpenChat: Advancing Open-source Language Models with Mixed-Quality Data](https://arxiv.org/abs/2309.11235) (2023-09-20)
  - Code: [GitHub](https://github.com/imoneoi/openchat)
  - Model weights: [HuggingFace models](https://huggingface.co/openchat/openchat_3.5)

- Glaive-coder
  - Year: 2023
  - Publication: [Blog - Releasing glaive-coder-7B and the Code Models Arena](https://glaive.ai/blog/releasing-code-model-arena) (2023-09-21)
  - Model weights: [HuggingFace models](https://huggingface.co/glaiveai/glaive-coder-7b)

- LongLoRA
  - Year: 2023
  - Publication: [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307) (2023-09-21)
  - Code: [GitHub](https://github.com/dvlab-research/LongLoRA)
  - Model weights: [HuggingFace models](https://huggingface.co/Yukang/Llama-2-7b-longlora-100k-ft)

- MetaMath
  - Year: 2023
  - Publication: [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models](https://arxiv.org/abs/2309.12284) (2023-09-21)
  - Code: [GitHub](https://github.com/meta-math/MetaMath)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-math/MetaMath-7B-V1.0)

- BAMBOO
  - Year: 2023
  - Publication: [BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models](https://arxiv.org/abs/2309.13345) (2023-09-23)
  - Code: [GitHub](https://github.com/RUCAIBox/BAMBOO)
  - Model weights: [HuggingFace models](https://huggingface.co/PowerInfer/Bamboo-base-v0_1)

- DeepSpeed-Ulysses
  - Year: 2023
  - Publication: [DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://arxiv.org/abs/2309.14509) (2023-09-25)

- QA-LoRA
  - Year: 2023
  - Publication: [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717) (2023-09-26)

- Mistral
  - Year: 2023
  - Publication: [Blog - Mistral 7B - The best 7B model to date, Apache 2.0](https://mistral.ai/news/announcing-mistral-7b/) (2023-09-27)
  - Code: [GitHub](https://github.com/mistralai/mistral-src)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mistral-7B-v0.1)

- Qwen
  - Year: 2023
  - Publication: [Qwen Technical Report](https://arxiv.org/abs/2309.16609) (2023-09-28)
  - Code: [GitHub](https://github.com/QwenLM/Qwen)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen-7B)

- Nous-Hermes
  - Year: 2023
  - Publication: [Tweet: Nous-Hermes-13b fp16 weights have been released.](https://twitter.com/NousResearch/status/1664848823687028737) (2023-09-29)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Nous-Hermes-13b)

- OpenHermes-2.5-Mistral
  - Year: 2023
  - Publication: [HuggingFace model card - OpenHermes-2.5-Mistral](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B) (2023-09-29)
  - Model weights: [HuggingFace models](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)

- StableLM-3B-4E1T
  - Year: 2023
  - Publication: [Blog - Technical report for StableLM-3B-4E1T](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) (2023-09-29)
  - Code: [GitHub](https://github.com/Stability-AI/StableLM)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-3b-4e1t)

- StreamingLLM
  - Year: 2023
  - Publication: [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) (2023-09-29)
  - Code: [GitHub](https://github.com/mit-han-lab/streaming-llm)

- MiniGPT-5
  - Year: 2023
  - Publication: [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens](https://arxiv.org/abs/2310.02239) (2023-10-03)
  - Code: [GitHub](https://github.com/eric-ai-lab/MiniGPT-5)

- Kosmos-G
  - Year: 2023
  - Publication: [Kosmos-G: Generating Images in Context with Multimodal Large Language Models](https://arxiv.org/abs/2310.02992) (2023-10-04)

- AgentInstruct
  - Year: 2023
  - Publication: [Agent Instructs Large Language Models to be General Zero-Shot Reasoners](https://arxiv.org/abs/2310.03710) (2023-10-05)
  - Code: [GitHub](https://github.com/wang-research-lab/agentinstruct)
  - Model weights: [HuggingFace models](https://huggingface.co/datasets/WangResearchLab/AgentInstruct)

- LightSeq / DistFlashAttn
  - Year: 2023
  - Publication: [DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training](https://arxiv.org/abs/2310.03294) (2023-10-05)
  - Code: [GitHub](https://github.com/RulinShao/LightSeq)

- FIRE
  - Year: 2023
  - Publication: [Functional Interpolation for Relative Positions Improves Long Context Transformers](https://arxiv.org/abs/2310.04418) (2023-10-06)

- GFlowNets
  - Year: 2023
  - Publication: [Amortizing intractable inference in large language models](https://arxiv.org/abs/2310.04363) (2023-10-06)
  - Code: [GitHub](https://github.com/GFNOrg/gfn-lm-tuning)

- HyperAttention
  - Year: 2023
  - Publication: [HyperAttention: Long-context Attention in Near-Linear Time](https://arxiv.org/abs/2310.05869) (2023-10-09)

- LLMLingua
  - Year: 2023
  - Publication: [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736) (2023-10-09)
  - Code: [GitHub](https://github.com/microsoft/LLMLingua)

- CodeFuse
  - Year: 2023
  - Publication: [CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model](https://arxiv.org/abs/2310.06266) (2023-10-10)
  - Code: [GitHub](https://github.com/codefuse-ai/MFTCoder)
  - Model weights: [HuggingFace models](https://huggingface.co/codefuse-ai/CodeFuse-13B)

- LongLLMLingua
  - Year: 2023
  - Publication: [LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression](https://arxiv.org/abs/2310.06839) (2023-10-10)
  - Code: [GitHub](https://github.com/microsoft/LLMLingua)

- LoftQ
  - Year: 2023
  - Publication: [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659) (2023-10-12)
  - Code: [GitHub](https://github.com/yxli2123/LoftQ)
  - Model weights: [HuggingFace models](https://huggingface.co/LoftQ/Meta-Llama-3-8B-4bit-64rank)

- MemGPT
  - Year: 2023
  - Publication: [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) (2023-10-12)
  - Code: [GitHub](https://github.com/cpacker/MemGPT)

- Prometheus
  - Year: 2023
  - Publication: [Prometheus: Inducing Fine-grained Evaluation Capability in Language Models](https://arxiv.org/abs/2310.08491) (2023-10-12)
  - Code: [GitHub](https://github.com/prometheus-eval/prometheus)

- MiniGPT-v2
  - Year: 2023
  - Publication: [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478) (2023-10-14)
  - Code: [GitHub](https://github.com/Vision-CAIR/MiniGPT-4)

- Llemma
  - Year: 2023
  - Publication: [Llemma: An Open Language Model For Mathematics](https://arxiv.org/abs/2310.10631) (2023-10-16)
  - Code: [GitHub](https://github.com/EleutherAI/math-lm)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/llemma_7b)

- MistralLite
  - Year: 2023
  - Publication: [HuggingFace model card - MistralLite](https://huggingface.co/amazon/MistralLite) (2023-10-16)
  - Model weights: [HuggingFace models](https://huggingface.co/amazon/MistralLite)

- BitNet
  - Year: 2023
  - Publication: [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453) (2023-10-17)

- Fuyu
  - Year: 2023
  - Publication: [Blog - Fuyu-8B: A Multimodal Architecture for AI Agents](https://www.adept.ai/blog/fuyu-8b) (2023-10-17)
  - Model weights: [HuggingFace models](https://huggingface.co/adept/fuyu-8b)

- VeRA
  - Year: 2023
  - Publication: [VeRA: Vector-based Random Matrix Adaptation](https://arxiv.org/abs/2310.11454) (2023-10-17)

- Monarch Mixer:
  - Year: 2023
  - Publication: [Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture](https://arxiv.org/abs/2310.12109) (2023-10-18)

- UltraLM 2.0
  - Year: 2023
  - Publication: [HuggingFace model card - UltraLM 2.0](https://huggingface.co/openbmb/UltraLM-13b-v2.0) (2023-10-22)
  - Code: [GitHub](https://github.com/thunlp/UltraChat)
  - Model weights: [HuggingFace models](https://huggingface.co/openbmb/UltraLM-13b-v2.0)

- Obsidian
  - Year: 2023
  - Publication: [HuggingFace model card - Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5) (2023-10-24)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)

- QMoE
  - Year: 2023
  - Publication: [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](https://arxiv.org/abs/2310.16795) (2023-10-25)
  - Code: [GitHub](https://github.com/IST-DASLab/qmoe)

- Zephyr
  - Year: 2023
  - Publication: [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944) (2023-10-25)
  - Code: [GitHub](https://github.com/huggingface/alignment-handbook)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)

- CodeFusion
  - Year: 2023
  - Publication: [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) (2023-10-26)
  - Code: [GitHub](https://github.com/microsoft/prose-benchmarks)

- FP8-LM
  - Year: 2023
  - Publication: [FP8-LM: Training FP8 Large Language Models](https://arxiv.org/abs/2310.18313) (2023-10-27)
  - Code: [GitHub](https://github.com/Azure/MS-AMP)

- Deepseek Coder
  - Year: 2023
  - Publication: [Blog - DeepSeek Coder: Let the Code Write Itself](https://deepseekcoder.github.io/) (2023-10-28)
  - Code: [GitHub](https://github.com/deepseek-ai/deepseek-coder/)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base)

- Punica
  - Year: 2023
  - Publication: [Punica: Multi-Tenant LoRA Serving](https://arxiv.org/abs/2310.18547) (2023-10-28)
  - Code: [GitHub](https://github.com/punica-ai/punica)

- OpenChat 3.5
  - Year: 2023
  - Publication: [HuggingFace model card - OpenChat 3.5](https://huggingface.co/openchat/openchat_3.5) (2023-10-30)
  - Model weights: [HuggingFace models](https://huggingface.co/openchat/openchat_3.5)

- ChipNeMo
  - Year: 2023
  - Publication: [ChipNeMo: Domain-Adapted LLMs for Chip Design](https://arxiv.org/abs/2311.00176) (2023-10-31)

- LeMa
  - Year: 2023
  - Publication: [Learning From Mistakes Makes LLM Better Reasoner](https://arxiv.org/abs/2310.20689) (2023-10-31)
  - Code: [GitHub](https://github.com/microsoft/CodeT)

- TopicGPT
  - Year: 2023
  - Publication: [TopicGPT: A Prompt-based Topic Modeling Framework](https://arxiv.org/abs/2311.01449) (2023-11-02)
  - Code: [GitHub](https://github.com/chtmp223/topicGPT)

- Simplified Transformers
  - Year: 2023
  - Publication: [Simplifying Transformer Blocks](https://arxiv.org/abs/2311.01906) (2023-11-03)
  - Code: [GitHub](https://github.com/bobby-he/simplified_transformers)

- LSS Transformer
  - Year: 2023
  - Publication: [Ultra-Long Sequence Distributed Transformer](https://arxiv.org/abs/2311.02382) (2023-11-04)

- Octavius
  - Year: 2023
  - Publication: [Octavius: Mitigating Task Interference in MLLMs via LoRA-MoE](https://arxiv.org/abs/2311.02684) (2023-11-05)

- CogVLM
  - Year: 2023
  - Publication: [CogVLM: Visual Expert for Pretrained Language Models](https://arxiv.org/abs/2311.03079) (2023-11-06)
  - Code: [GitHub](https://github.com/THUDM/CogVLM)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/CogVLM)

- S-LoRA
  - Year: 2023
  - Publication: [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://arxiv.org/abs/2311.03285) (2023-11-06)
  - Code: [GitHub](https://github.com/S-LoRA/S-LoRA)

- LLM Decontaminator
  - Year: 2023
  - Publication: [Rethinking Benchmark and Contamination for Language Models with Rephrased Samples](https://arxiv.org/abs/2311.04850) (2023-11-08)
  - Code: [GitHub](https://github.com/lm-sys/llm-decontaminator)

- LongQLoRA
  - Year: 2023
  - Publication: [LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models](https://arxiv.org/abs/2311.04879) (2023-11-08)
  - Code: [GitHub](https://github.com/yangjianxin1/LongQLoRA)

- Mirasol3B
  - Year: 2023
  - Publication: [Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities](https://arxiv.org/abs/2311.05698) (2023-11-09)

- FlashFFTConv
  - Year: 2023
  - Publication: [FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores](https://arxiv.org/abs/2311.05908) (2023-11-10)
  - Code: [GitHub](https://github.com/HazyResearch/flash-fft-conv)

- MiniMA
  - Year: 2023
  - Publication: [Towards the Law of Capacity Gap in Distilling Language Models](https://arxiv.org/abs/2311.07052) (2023-11-13)
  - Code: [GitHub](https://github.com/genezc/minima)
  - Model weights: [HuggingFace models](https://huggingface.co/GeneZC/MiniMA-3B)

- Neural-chat
  - Year: 2023
  - Publication: [HuggingFace model card - Neural-chat](https://huggingface.co/Intel/neural-chat-7b-v3-1) (2023-11-14)
  - Code: [GitHub](https://github.com/intel/intel-extension-for-transformers)
  - Model weights: [HuggingFace models](https://huggingface.co/Intel/neural-chat-7b-v3-1)

- UltraFastBERT
  - Year: 2023
  - Publication: [Exponentially Faster Language Modelling](https://arxiv.org/abs/2311.10770) (2023-11-15)
  - Code: [GitHub](https://github.com/pbelcak/UltraFastBERT)
  - Model weights: [HuggingFace models](https://huggingface.co/pbelcak/UltraFastBERT-1x11-long)

- Tulu 2
  - Year: 2023
  - Publication: [Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2](https://arxiv.org/abs/2311.10702) (2023-11-17)
  - Code: [GitHub](https://github.com/allenai/open-instruct)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/tulu-2-7b)

- Orca 2
  - Year: 2023
  - Publication: [Orca 2: Teaching Small Language Models How to Reason](https://arxiv.org/abs/2311.11045) (2023-11-18)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/Orca-2-13b)

- LQ-LoRA
  - Year: 2023
  - Publication: [LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning](https://arxiv.org/abs/2311.12023) (2023-11-20)
  - Code: [GitHub](https://github.com/HanGuo97/lq-lora)

- System 2 Attention
  - Year: 2023
  - Publication: [System 2 Attention (is something you might need too)](https://arxiv.org/abs/2311.11829) (2023-11-20)

- Lookahead Decoding
  - Year: 2023
  - Publication: [Blog - Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/) (2023-11-21)
  - Code: [GitHub](https://github.com/hao-ai-lab/LookaheadDecoding)

- Inflection-2
  - Year: 2023
  - Publication: [Blog - Inflection-2: The Next Step Up](https://inflection.ai/inflection-2) (2023-11-22)

- QuIP
  - Year: 2023
  - Publication: [Blog - QuIP with Lattice Codebooks](https://cornell-relaxml.github.io/quip-sharp/) (2023-11-23)
  - Code: [GitHub](https://github.com/Cornell-RelaxML/quip-sharp)

- Starling
  - Year: 2023
  - Publication: [Blog - Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF](https://starling.cs.berkeley.edu/) (2023-11-28)
  - Model weights: [HuggingFace models](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha)

- Mamba
  - Year: 2023
  - Publication: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) (2023-12-01)
  - Code: [GitHub](https://github.com/havenhq/mamba-chat)
  - Model weights: [HuggingFace models](https://huggingface.co/havenhq/mamba-chat)

- Notus
  - Year: 2023
  - Publication: [HuggingFace model card - Notus](https://huggingface.co/argilla/notus-7b-v1) (2023-12-01)
  - Code: [GitHub](https://github.com/argilla-io/notus)
  - Model weights: [HuggingFace models](https://huggingface.co/argilla/notus-7b-v1)

- Magicoder
  - Year: 2023
  - Publication: [Magicoder: Source Code Is All You Need](https://arxiv.org/abs/2312.02120) (2023-12-04)
  - Code: [GitHub](https://github.com/ise-uiuc/magicoder)
  - Model weights: [HuggingFace models](https://huggingface.co/ise-uiuc/Magicoder-CL-7B)

- RankZephyr
  - Year: 2023
  - Publication: [RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!](https://arxiv.org/abs/2312.02724) (2023-12-05)
  - Code: [GitHub](https://github.com/castorini/rank_llm)
  - Model weights: [HuggingFace models](https://huggingface.co/castorini/rank_vicuna_7b_v1)

- Xaberius
  - Year: 2023
  - Publication: [HuggingFace model card - Xaberius](https://huggingface.co/fblgit/una-xaberius-34b-v1beta) (2023-12-05)
  - Model weights: [HuggingFace models](https://huggingface.co/fblgit/una-xaberius-34b-v1beta)

- OneLLM
  - Year: 2023
  - Publication: [OneLLM: One Framework to Align All Modalities with Language](https://arxiv.org/abs/2312.03700) (2023-12-06)
  - Code: [GitHub](https://github.com/csuhan/OneLLM)
  - Model weights: [HuggingFace models](https://huggingface.co/csuhan/OneLLM-7B)

- Llama Guard
  - Year: 2023
  - Publication: [Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations](https://arxiv.org/abs/2312.06674) (2023-12-07)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/LlamaGuard-7b)

- StableLM Zephyr
  - Year: 2023
  - Publication: [Blog - Introducing Stable LM Zephyr 3B: A New Addition to Stable LM, Bringing Powerful LLM Assistants to Edge Devices](https://stability.ai/news/stablelm-zephyr-3b-stability-llm) (2023-12-07)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-zephyr-3b)

- EAGLE
  - Year: 2023
  - Publication: [Blog - EAGLE: Lossless Acceleration of LLM Decoding by Feature Extrapolation](https://sites.google.com/view/eagle-llm) (2023-12-08)

- StripedHyena
  - Year: 2023
  - Publication: [Blog - Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers](https://www.together.ai/blog/stripedhyena-7b) (2023-12-08)
  - Code: [GitHub](https://github.com/togethercomputer/stripedhyena)
  - Model weights: [HuggingFace models](https://huggingface.co/togethercomputer/StripedHyena-Hessian-7B)

- Amber
  - Year: 2023
  - Publication: [HuggingFace model card - Amber](https://huggingface.co/LLM360/Amber) (2023-12-11)
  - Code: [HuggingFace models](https://github.com/LLM360/amber-train)
  - Model weights: [HuggingFace models](https://huggingface.co/LLM360/Amber)

- CrystalCoder
  - Year: 2023
  - Publication: [HuggingFace model card - CrystalCoder](https://huggingface.co/LLM360/CrystalCoder) (2023-12-11)
  - Code: [HuggingFace models](https://github.com/LLM360/crystalcoder-train)
  - Model weights: [HuggingFace models](https://huggingface.co/LLM360/CrystalCoder)

- LLM360
  - Year: 2023
  - Publication: [LLM360: Towards Fully Transparent Open-Source LLMs](https://arxiv.org/abs/2312.06550) (2023-12-11)

- LLM in a flash
  - Year: 2023
  - Publication: [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/abs/2312.11514) (2023-12-12)

- phi-2
  - Year: 2023
  - Publication: [Blog - Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) (2023-12-12)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-2)

- SGLang
  - Year: 2023
  - Publication: [Efficiently Programming Large Language Models using SGLang](https://arxiv.org/abs/2312.07104) (2023-12-12)
  - Code: [GitHub](https://github.com/sgl-project/sglang/)

- SwitchHead
  - Year: 2023
  - Publication: [SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention](https://arxiv.org/abs/2312.07987) (2023-12-13)
  - Code: [GitHub](https://github.com/robertcsordas/moe_attention)

- Dolphin 2.5
  - Year: 2023
  - Publication: [HuggingFace model card - Dolphin 2.5](https://huggingface.co/cognitivecomputations/dolphin-2.5-mixtral-8x7b) (2023-12-14)
  - Model weights: [HuggingFace models](https://huggingface.co/cognitivecomputations/dolphin-2.5-mixtral-8x7b)

- ZeroQuant
  - Year: 2023
  - Publication: [ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks](https://arxiv.org/abs/2312.08583) (2023-12-14)
  - Code: [GitHub](https://github.com/microsoft/DeepSpeed)

- PowerInfer
  - Year: 2023
  - Publication: [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](https://arxiv.org/abs/2312.12456) (2023-12-16)
  - Code: [GitHub](https://github.com/SJTU-IPADS/PowerInfer)
  - Model weights: [HuggingFace models](https://huggingface.co/PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF)

- Gemini
  - Year: 2023
  - Publication: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) (2023-12-19)

- InternVL
  - Year: 2023
  - Publication: [InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks](https://arxiv.org/abs/2312.14238) (2023-12-21)
  - Code: [GitHub](https://github.com/OpenGVLab/InternVL)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenGVLab/InternVL)

- WhiteRabbitNeo
  - Year: 2023
  - Publication: [HuggingFace model card - WhiteRabbitNeo](https://huggingface.co/WhiteRabbitNeo/WhiteRabbitNeo-33B-v1) (2023-12-27)
  - Model weights: [HuggingFace models](https://huggingface.co/whiterabbitneo/WhiteRabbitNeo-33B-v1)

- MosaicBERT
  - Year: 2023
  - Publication: [MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining](https://arxiv.org/abs/2312.17482) (2023-12-29)
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mosaic-bert-base)

- LLM Maybe LongLM
  - Year: 2024
  - Publication: [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning](https://arxiv.org/abs/2401.01325) (2024-01-02)

- SPIN
  - Year: 2024
  - Publication: [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/abs/2401.01335) (2024-01-02)
  - Code: [GitHub](https://github.com/uclaml/SPIN)
  - Model weights: [HuggingFace models](https://huggingface.co/UCLA-AGI/zephyr-7b-sft-full-SPIN-iter0)

- LLaMA Pro
  - Year: 2024
  - Publication: [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/abs/2401.02415) (2024-01-04)
  - Code: [GitHub](https://github.com/TencentARC/LLaMA-Pro)
  - Model weights: [HuggingFace models](https://huggingface.co/TencentARC/LLaMA-Pro-8B)

- nomic-bert
  - Year: 2024
  - Publication: [HuggingFace model card - nomic-bert](https://huggingface.co/nomic-ai/nomic-bert-2048) (2024-01-04)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/nomic-bert-2048)

- TinyLlama
  - Year: 2023
  - Publication: [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385) (2024-01-04)
  - Code: [GitHub](https://github.com/jzhang38/TinyLlama)
  - Model weights: [HuggingFace models](https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b)

- Deepseek LLM
  - Year: 2023
  - Publication: [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954) (2024-01-05)
  - Code: [GitHub](https://github.com/deepseek-ai/deepseek-LLM)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-llm-7b-base)

- Genstruct
  - Year: 2024
  - Publication: [HuggingFace model card - Genstruct](https://huggingface.co/NousResearch/Genstruct-7B) (2024-01-05)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Genstruct-7B)

- Infinite-LLM
  - Year: 2024
  - Publication: [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](https://arxiv.org/abs/2401.02669) (2024-01-05)

- Activation Beacon
  - Year: 2024
  - Publication: [Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon](https://arxiv.org/abs/2401.03462) (2024-01-07)
  - Code: [GitHub](https://github.com/FlagOpen/FlagEmbedding)

- Mixtral of Experts
  - Year: 2024
  - Publication: [Mixtral of Experts](https://arxiv.org/abs/2401.04088) (2024-01-08)
  - Code: [GitHub](https://github.com/mistralai/mistral-src)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)

- MoE-Mamba
  - Year: 2024
  - Publication: [MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://arxiv.org/abs/2401.04081) (2024-01-08)
  - Code: [GitHub](https://github.com/llm-random/llm-random)

- TeleChat
  - Year: 2024
  - Publication: [TeleChat Technical Report](https://arxiv.org/abs/2401.03804) (2024-01-08)

- Lightning Attention-2
  - Year: 2024
  - Publication: [Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models](https://arxiv.org/abs/2401.04658) (2024-01-09)
  - Code: [GitHub](https://github.com/OpenNLPLab/lightning-attention)

- RoSA
  - Year: 2024
  - Publication: [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://arxiv.org/abs/2401.04679) (2024-01-09)
  - Code: [GitHub](https://github.com/IST-DASLab/RoSA)

- MegaDolphin
  - Year: 2024
  - Publication: [HuggingFace model card - MegaDolphin](https://huggingface.co/cognitivecomputations/MegaDolphin-120b) (2024-01-10)
  - Model weights: [HuggingFace models](https://huggingface.co/cognitivecomputations/MegaDolphin-120b)

- DeepSeekMoE
  - Year: 2024
  - Publication: [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066) (2024-01-11)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-MoE)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-moe-16b-base)

- Patchscopes
  - Year: 2024
  - Publication: [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://arxiv.org/abs/2401.06102) (2024-01-11)

- Mixtral Nous-Hermes
  - Year: 2024
  - Publication: [Tweet: Introducing our new flagship LLM, Nous-Hermes 2 on Mixtral 8x7B.](https://twitter.com/NousResearch/status/1746988416779309143) (2024-01-15)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO)

- AlphaCodium
  - Year: 2024
  - Publication: [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](https://arxiv.org/abs/2401.08500) (2024-01-16)
  - Code: [GitHub](https://github.com/Codium-ai/AlphaCodium)

- Stable Code
  - Year: 2024
  - Publication: [Blog - Stable Code 3B: Coding on the Edge](https://stability.ai/news/stable-code-2024-llm-code-completion-release) (2024-01-16)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stable-code-3b)

- ReFT
  - Year: 2024
  - Publication: [ReFT: Reasoning with Reinforced Fine-Tuning](https://arxiv.org/abs/2401.08967) (2024-01-17)
  - Code: [GitHub](https://github.com/lqtrung1998/mwp_ReFT)
  - Model weights: [HuggingFace models](https://huggingface.co/lqtrung1998/Codellama-7b-hf-ReFT-GSM8k)

- StableLM 2 Zephyr
  - Year: 2024
  - Publication: [HuggingFace model card - StableLM 2 Zephyr](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b) (2024-01-19)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b)

- Imp
  - Year: 2024
  - Publication: [HuggingFace model card - Imp](https://huggingface.co/MILVLG/imp-v1-3b) (2024-01-24)
  - Code: [GitHub](https://github.com/MILVLG/imp)
  - Model weights: [HuggingFace models](https://huggingface.co/MILVLG/imp-v1-3b)

- MaLA-500
  - Year: 2024
  - Publication: [MaLA-500: Massive Language Adaptation of Large Language Models](https://arxiv.org/abs/2401.13303) (2024-01-24)
  - Model weights: [HuggingFace models](https://huggingface.co/MaLA-LM/mala-500)

- MambaByte
  - Year: 2024
  - Publication: [MambaByte: Token-free Selective State Space Model](https://arxiv.org/abs/2401.13660) (2024-01-24)

- MM-LLMs
  - Year: 2024
  - Publication: [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601) (2024-01-24)

- Snorkel AI
  - Year: 2024
  - Publication: [Blog - New benchmark results demonstrate value of Snorkel AI approach to LLM alignment](https://snorkel.ai/new-benchmark-results-demonstrate-value-of-snorkel-ai-approach-to-llm-alignment/) (2024-01-24)
  - Model weights: [HuggingFace models](https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO)

- SpacTor-T5
  - Year: 2024
  - Publication: [SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection](https://arxiv.org/abs/2401.13160) (2024-01-24)

- DeepSeek-Coder
  - Year: 2024
  - Publication: [DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence](https://arxiv.org/abs/2401.14196) (2024-01-25)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-Coder)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/deepseek-ai/deepseek-coder)

- FP6-LLM
  - Year: 2024
  - Publication: [FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design](https://arxiv.org/abs/2401.14112) (2024-01-25)
  - Code: [GitHub](https://github.com/usyd-fsalab/fp6_llm)

- SliceGPT
  - Year: 2024
  - Publication: [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://arxiv.org/abs/2401.15024) (2024-01-26)
  - Code: [GitHub](https://github.com/microsoft/TransformerCompression)

- Eagle 7B
  - Year: 2024
  - Publication: [Blog - Eagle 7B: Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages](https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers) (2024-01-29)
  - Model weights: [HuggingFace models](https://huggingface.co/RWKV/v5-Eagle-7B)

- H2O-Danube
  - Year: 2024
  - Publication: [H2O-Danube-1.8B Technical Report](https://arxiv.org/abs/2401.16818) (2024-01-30)
  - Model weights: [HuggingFace models](https://huggingface.co/h2oai/h2o-danube-1.8b-base)

- KVQuant
  - Year: 2024
  - Publication: [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079) (2024-01-31)
  - Code: [GitHub](https://github.com/SqueezeAILab/KVQuant/)

- BlackMamba
  - Year: 2024
  - Publication: [Blog - BlackMamba: Mixture of Experts for State-Space Models](https://static1.squarespace.com/static/658ded386c43c219ee47caba/t/65bd73200920d050ccbac40c/1706914594353/blackMamba.pdf) (2024-02-01)
  - Code: [GitHub](https://github.com/Zyphra/BlackMamba)
  - Model weights: [HuggingFace models](https://huggingface.co/Zyphra/BlackMamba-1.5B)

- OLMo
  - Year: 2024
  - Publication: [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) (2024-02-01)
  - Code: [GitHub](https://github.com/allenai/OLMo)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/OLMo-1B)

- Tiny Titans
  - Year: 2024
  - Publication: [Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?](https://arxiv.org/abs/2402.00841) (2024-02-01)

- Nomic Embed
  - Year: 2024
  - Publication: [Nomic Embed: Training a Reproducible Long Context Text Embedder](https://arxiv.org/abs/2402.01613) (2024-02-02)
  - Code: [GitHub](https://github.com/nomic-ai/contrastors)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/nomic-embed-text-v1)

- Smaug
  - Year: 2024
  - Publication: [HuggingFace model card - Smaug](https://huggingface.co/abacusai/Smaug-72B-v0.1) (2024-02-02)
  - Model weights: [HuggingFace models](https://huggingface.co/abacusai/Smaug-72B-v0.1)

- Qwen 1.5
  - Year: 2024
  - Publication: [Blog - Introducing Qwen 1.5](https://qwenlm.github.io/blog/qwen1.5/) (2024-02-04)
  - Code: [GitHub](https://github.com/QwenLM/Qwen1.5)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen1.5-0.5B)

- DeepSeekMath
  - Year: 2024
  - Publication: [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) (2024-02-05)
  - Code: [GitHub](https://github.com/deepseek-ai/deepseek-math)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-math-7b-base)

- RethinkTinyLM
  - Year: 2024
  - Publication: [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) (2024-02-05)
  - Code: [GitHub](https://github.com/YuchuanTian/RethinkTinyLM)

- SQLCoder 2
  - Year: 2024
  - Publication: [HuggingFace model card - SQLCoder 2](https://huggingface.co/defog/sqlcoder-7b-2) (2024-02-05)
  - Code: [GitHub](https://github.com/defog-ai/sqlcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/defog/sqlcoder-7b-2)

- Self-Discover
  - Year: 2024
  - Publication: [Self-Discover: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620) (2024-02-06)

- e5
  - Year: 2024
  - Publication: [Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/abs/2402.05672) (2024-02-08)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/e5)
  - Model weights: [HuggingFace models](https://huggingface.co/intfloat/e5-small-v2)

- InternLM-Math
  - Year: 2024
  - Publication: [InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning](https://arxiv.org/abs/2402.06332) (2024-02-09)
  - Code: [GitHub](https://github.com/InternLM/InternLM-Math)
  - Model weights: [HuggingFace models](https://huggingface.co/internlm/internlm2-math-7b)

- DoRA
  - Year: 2024
  - Publication: [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) (2024-02-14)

- Gemini 1.5
  - Year: 2024
  - Publication: [Blog - Our next-generation model: Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) (2024-02-15)

- GritLM
  - Year: 2024
  - Publication: [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906) (2024-02-15)
  - Code: [GitHub](https://github.com/ContextualAI/gritlm)
  - Model weights: [HuggingFace models](https://huggingface.co/GritLM/GritLM-7B)

- LoRA+
  - Year: 2024
  - Publication: [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) (2024-02-19)
  - Code: [GitHub](https://github.com/nikhil-ghosh-berkeley/loraplus)

- Gemma
  - Year: 2024
  - Publication: [Blog - Gemma: Introducing new state-of-the-art open models](https://blog.google/technology/developers/gemma-open-models/) (2024-02-21)
  - Model weights: [HuggingFace models](https://huggingface.co/google/gemma-7b)

- LongRoPE
  - Year: 2024
  - Publication: [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://arxiv.org/abs/2402.13753) (2024-02-21)
  - Code: [GitHub](https://github.com/microsoft/LongRoPE)

- MobileLLM
  - Year: 2024
  - Publication: [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905) (2024-02-22)
  - Code: [GitHub](https://github.com/facebookresearch/MobileLLM)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/MobileLLM-1.5B)

- MegaScale
  - Year: 2024
  - Publication: [MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs](https://arxiv.org/abs/2402.15627) (2024-02-23)

- MobiLlama
  - Year: 2024
  - Publication: [HuggingFace model card - MobiLlama](https://huggingface.co/MBZUAI/MobiLlama-1B) (2024-02-24)
  - Code: [GitHub](https://github.com/mbzuai-oryx/MobiLlama)
  - Model weights: [HuggingFace models](https://huggingface.co/MBZUAI/MobiLlama-1B)

- Nemotron-4
  - Year: 2024
  - Publication: [Nemotron-4 15B Technical Report](https://arxiv.org/abs/2402.16819) (2024-02-26)

- BitNet b1.58
  - Year: 2024
  - Publication: [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) (2024-02-27)
  - Code: [GitHub](https://github.com/microsoft/BitNet)

- Mistral Pro
  - Year: 2024
  - Publication: [HuggingFace model card - Mistral Pro](https://huggingface.co/TencentARC/Mistral_Pro_8B_v0.1) (2024-02-27)
  - Model weights: [HuggingFace models](https://huggingface.co/TencentARC/Mistral_Pro_8B_v0.1)

- Stable LM 2
  - Year: 2024
  - Publication: [Stable LM 2 1.6B Technical Report](https://arxiv.org/abs/2402.17834) (2024-02-27)
  - Code: [GitHub](https://github.com/Stability-AI/StableLM)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-2-1_6b)

- Consistency Large Language Models (CLLMs)
  - Year: 2024
  - Publication: [CLLMs: Consistency Large Language Models](https://arxiv.org/abs/2403.00835) (2024-02-28)
  - Code: [GitHub](https://github.com/hao-ai-lab/Consistency_LLM)
  - Model weights: [HuggingFace models](https://huggingface.co/cllm/consistency-llm-7b-sharegpt48k)

- Griffin
  - Year: 2024
  - Publication: [Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](https://arxiv.org/abs/2402.19427) (2024-02-29)

- Resonance RoPE
  - Year: 2024
  - Publication: [Resonance RoPE: Improving Context Length Generalization of Large Language Models](https://arxiv.org/abs/2403.00071) (2024-02-29)
  - Code: [GitHub](https://github.com/sheryc/resonance_rope)

- StarCoder-2
  - Year: 2024
  - Publication: [StarCoder 2 and The Stack v2: The Next Generation](https://arxiv.org/abs/2402.19173) (2024-02-29)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoder2-15b)

- InfiMM-HD
  - Year: 2024
  - Publication: [InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding](https://arxiv.org/abs/2403.01487) (2024-03-03)
  - Model weights: [HuggingFace models](https://huggingface.co/Infi-MM/infimm-hd)

- Claude 3
  - Year: 2024
  - Publication: [Blog - Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) (2024-03-04)

- moondream2
  - Year: 2024
  - Publication: [README - moondream repository](https://github.com/vikhyat/moondream/blob/9fe3ad77616b9335c233ebbf6cf443bf176ae1d4/README.md) (2024-03-04)
  - Code: [GitHub](https://github.com/vikhyat/moondream)
  - Model weights: [HuggingFace models](https://huggingface.co/vikhyatk/moondream2)

- Design2Code
  - Year: 2024
  - Publication: [Design2Code: How Far Are We From Automating Front-End Engineering?](https://arxiv.org/abs/2403.03163) (2024-03-05)
  - Code: [GitHub](https://github.com/NoviScl/Design2Code)

- GaLore
  - Year: 2024
  - Publication: [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507) (2024-03-06)

- SaulLM-7B
  - Year: 2024
  - Publication: [SaulLM-7B: A pioneering Large Language Model for Law](https://arxiv.org/abs/2403.03883) (2024-03-06)
  - Model weights: [HuggingFace models](https://huggingface.co/Equall/Saul-Base)

- inflection-2.5
  - Year: 2024
  - Publication: [Blog - Inflection-2.5: meet the world's best personal AI](https://inflection.ai/inflection-2-5) (2024-03-07)

- Yi
  - Year: 2024
  - Publication: [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652) (2024-03-07)
  - Code: [GitHub](https://github.com/01-ai/Yi)
  - Model weights: [HuggingFace models](https://huggingface.co/01-ai/Yi-34B)

- DeepSpeed-FP6
  - Year: 2024
  - Publication: [README - DeepSpeed-FP6: The Power of FP6-Centric Serving for Large Language Models](https://github.com/microsoft/DeepSpeed/blob/d86a68c3d407b4aebb2116cec6f351ea5e799c19/blogs/deepspeed-fp6/03-05-2024/README.md) (2024-03-08)
  - Code: [GitHub](https://github.com/microsoft/DeepSpeed)

- GEAR
  - Year: 2024
  - Publication: [GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527) (2024-03-08)
  - Code: [GitHub](https://github.com/opengear-project/GEAR)

- Command R
  - Year: 2024
  - Publication: [Blog - Introducing Command R Fine-Tuning: Industry-Leading Performance at a Fraction of the Cost](https://cohere.com/blog/commandr-fine-tuning) (2024-03-09)
  - Model weights: [HuggingFace models](https://huggingface.co/CohereForAI/c4ai-command-r-v01)

- BTX (Branch-Train-MiX)
  - Year: 2024
  - Publication: [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](https://arxiv.org/abs/2403.07816) (2024-03-12)

- ORPO
  - Year: 2024
  - Publication: [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691) (2024-03-12)
  - Code: [GitHub](https://github.com/xfactlab/orpo)
  - Model weights: [HuggingFace models](https://huggingface.co/kaist-ai/mistral-orpo-alpha)

- ScatterMoE
  - Year: 2024
  - Publication: [Scattered Mixture-of-Experts Implementation](https://arxiv.org/abs/2403.08245) (2024-03-13)
  - Code: [GitHub](https://github.com/shawntan/scattermoe)

- BurstAttention
  - Year: 2024
  - Publication: [BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences](https://arxiv.org/abs/2403.09347) (2024-03-14)

- Dynamic Memory Compression (DMC)
  - Year: 2024
  - Publication: [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636) (2024-03-14)

- MM1
  - Year: 2024
  - Publication: [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) (2024-03-14)

- Quiet-STaR
  - Year: 2024
  - Publication: [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) (2024-03-14)

- GemMoE
  - Year: 2024
  - Publication: [HuggingFace model card - GemMoE](https://huggingface.co/Crystalcareai/GemMoE-Base-Random) (2024-03-15)

- Idefics2
  - Year: 2024
  - Publication: [HuggingFace model card - Introducing Idefics2: A Powerful 8B Vision-Language Model for the community](https://huggingface.co/blog/idefics2) (2024-03-15)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceM4/idefics2-8b)

- Pile-T5
  - Year: 2024
  - Publication: [Blog - Pile-T5](https://blog.eleuther.ai/pile-t5/) (2024-03-15)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/pile-t5-base)

- Grok-1
  - Year: 2024
  - Publication: [Blog - Open Release of Grok-1](https://x.ai/blog/grok-os) (2024-03-17)
  - Code: [GitHub](https://github.com/xai-org/grok-1)
  - Model weights: [HuggingFace models](https://huggingface.co/xai-org/grok-1)

- Agent-FLAN
  - Year: 2024
  - Publication: [Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models](https://arxiv.org/abs/2403.12881) (2024-03-19)
  - Code: [GitHub](https://github.com/InternLM/Agent-FLAN)
  - Model weights: [HuggingFace models](https://huggingface.co/internlm/Agent-FLAN-7b)

- FollowIR
  - Year: 2024
  - Publication: [FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://arxiv.org/abs/2403.15246) (2024-03-22)
  - Code: [GitHub](https://github.com/orionw/FollowIR)
  - Model weights: [HuggingFace models](https://huggingface.co/jhu-clsp/FollowIR-7B)

- LLM2LLM
  - Year: 2024
  - Publication: [LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement](https://arxiv.org/abs/2403.15042) (2024-03-22)
  - Code: [GitHub](https://github.com/SqueezeAILab/LLM2LLM)

- InternLM2
  - Year: 2024
  - Publication: [InternLM2 Technical Report](https://arxiv.org/abs/2403.17297) (2024-03-26)

- DBRX
  - Year: 2024
  - Publication: [Blog - Introducing DBRX: A New State-of-the-Art Open LLM](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm) (2024-03-27)
  - Code: [GitHub](https://github.com/databricks/dbrx)
  - Model weights: [HuggingFace models](https://huggingface.co/databricks/dbrx-instruct)

- Grok-1.5
  - Year: 2024
  - Publication: [Blog - Announcing Grok-1.5](https://x.ai/blog/grok-1.5) (2024-03-28)

- Jamba
  - Year: 2024
  - Publication: [Jamba: A Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2403.19887) (2024-03-28)
  - Model weights: [HuggingFace models](https://huggingface.co/ai21labs/Jamba-v0.1)

- Qwen1.5-MoE
  - Year: 2024
  - Publication: [Blog - Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters](https://qwenlm.github.io/blog/qwen-moe/) (2024-03-28)
  - Code: [GitHub](https://github.com/QwenLM/Qwen1.5)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen1.5-110B-Chat)

- DiJiang
  - Year: 2024
  - Publication: [DiJiang: Efficient Large Language Models through Compact Kernelization](https://arxiv.org/abs/2403.19928) (2024-03-29)
  - Code: [GitHub](https://github.com/YuchuanTian/DiJiang)

- Gecko
  - Year: 2024
  - Publication: [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/abs/2403.20327) (2024-03-29)

- Transformer-Lite
  - Year: 2024
  - Publication: [Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs](https://arxiv.org/abs/2403.20041) (2024-03-29)

- Aurora-M
  - Year: 2024
  - Publication: [Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order](https://arxiv.org/abs/2404.00399) (2024-03-30)
  - Model weights: [HuggingFace models](https://huggingface.co/aurora-m/aurora-m-base)

- Eurus
  - Year: 2024
  - Publication: [Advancing LLM Reasoning Generalists with Preference Trees](https://arxiv.org/abs/2404.02078) (2024-04-02)
  - Code: [GitHub](https://github.com/OpenBMB/Eurus)
  - Model weights: [HuggingFace models](https://huggingface.co/openbmb/Eurus-7b-sft)

- Mixture-of-Depths
  - Year: 2024
  - Publication: [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258) (2024-04-02)

- Poro
  - Year: 2024
  - Publication: [Poro 34B and the Blessing of Multilinguality](https://arxiv.org/abs/2404.01856) (2024-04-02)
  - Model weights: [HuggingFace models](https://huggingface.co/LumiOpen/Poro-34B)

- ChatGLM-Math
  - Year: 2024
  - Publication: [ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](https://arxiv.org/abs/2404.02893) (2024-04-03)
  - Code: [GitHub](https://github.com/THUDM/ChatGLM-Math)

- Linear Attention Sequence Parallelism (LASP)
  - Year: 2024
  - Publication: [Linear Attention Sequence Parallelism](https://arxiv.org/abs/2404.02882) (2024-04-03)
  - Code: [GitHub](https://github.com/OpenNLPLab/LASP)

- PiSSA
  - Year: 2024
  - Publication: [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](https://arxiv.org/abs/2404.02948) (2024-04-03)
  - Code: [GitHub](https://github.com/GraphPKU/PiSSA)
  - Model weights: [HuggingFace models](https://huggingface.co/fxmeng/PiSSA-Llama-3-8B-Instruct-r16)

- Command R+
  - Year: 2024
  - Publication: [Blog - Introducing Command R+: A Scalable LLM Built for Business](https://cohere.com/blog/command-r-plus-microsoft-azure) (2024-04-04)
  - Model weights: [HuggingFace models](https://huggingface.co/CohereForAI/c4ai-command-r-plus)

- Representation Finetuning (ReFT)
  - Year: 2024
  - Publication: [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592) (2024-04-04)
  - Code: [GitHub](https://github.com/stanfordnlp/pyreft)

- H2O-Danube2
  - Year: 2024
  - Publication: [HuggingFace model card - H2O-Danube2](https://huggingface.co/h2oai/h2o-danube2-1.8b-chat) (2024-04-05)
  - Model weights: [HuggingFace models](https://huggingface.co/h2oai/h2o-danube2-1.8b-chat)

- SqueezeAttention
  - Year: 2024
  - Publication: [SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget](https://arxiv.org/abs/2404.04793) (2024-04-07)
  - Code: [GitHub](https://github.com/hetailang/SqueezeAttention)

- RecurrentGemma
  - Year: 2024
  - Publication: [HuggingFace model card - RecurrentGemma](https://huggingface.co/google/recurrentgemma-2b-it) (2024-04-09)
  - Code: [GitHub](https://github.com/google-deepmind/recurrentgemma)
  - Model weights: [HuggingFace models](https://huggingface.co/google/recurrentgemma-2b-it)

- Infini-Transformer
  - Year: 2024
  - Publication: [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/abs/2404.07143) (2024-04-10)
  - Code: [GitHub](https://github.com/dingo-actual/infini-transformer)

- JetMoE
  - Year: 2024
  - Publication: [JetMoE: Reaching Llama2 Performance with 0.1M Dollars](https://arxiv.org/abs/2404.07413) (2024-04-11)
  - Code: [GitHub](https://github.com/myshell-ai/JetMoE)
  - Model weights: [HuggingFace models](https://huggingface.co/jetmoe/jetmoe-8b)

- Rho-1
  - Year: 2024
  - Publication: [Rho-1: Not All Tokens Are What You Need](https://arxiv.org/abs/2404.07965) (2024-04-11)
  - Code: [GitHub](https://github.com/microsoft/rho)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/rho-math-1b-v0.1)

- Megalodon
  - Year: 2024
  - Publication: [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/abs/2404.08801) (2024-04-12)
  - Code: [GitHub](https://github.com/XuezheMax/megalodon)

- TransformerFAM
  - Year: 2024
  - Publication: [TransformerFAM: Feedback attention is working memory](https://arxiv.org/abs/2404.09173) (2024-04-14)

- Prepacking
  - Year: 2024
  - Publication: [Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models](https://arxiv.org/abs/2404.09529) (2024-04-15)
  - Code: [GitHub](https://github.com/siyan-zhao/prepacking)

- K2
  - Year: 2024
  - Publication: [Direct link - LLM360 K2-65B: Scaling Up Fully Transparent Open-Source LLMs](https://www.llm360.ai/paper2.pdf) (2024-04-17)
  - Code: [GitHub](https://github.com/LLM360/k2-train)
  - Model weights: [HuggingFace models](https://huggingface.co/LLM360/K2)

- Mixtral 8x22B
  - Year: 2024
  - Publication: [Blog - Mixtral 8x22B - Cheaper, Better, Faster, Stronger](https://mistral.ai/news/mixtral-8x22b/) (2024-04-17)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1)

- Llama 3
  - Year: 2024
  - Publication: [Blog - Introducing Meta Llama 3: The most capable openly available LLM to date](https://ai.meta.com/blog/meta-llama-3/) (2024-04-18)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Meta-Llama-3-8B)

- Llama Guard 2
  - Year: 2024
  - Publication: [HuggingFace model card - Llama Guard 2](https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B) (2024-04-18)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Meta-Llama-Guard-2-8B)

- LongEmbed
  - Year: 2024
  - Publication: [LongEmbed: Extending Embedding Models for Long Context Retrieval](https://arxiv.org/abs/2404.12096) (2024-04-18)
  - Code: [GitHub](https://github.com/dwzhu-pku/longembed)
  - Model weights: [HuggingFace models](https://huggingface.co/dwzhu/e5rope-base)

- decoupleQ
  - Year: 2024
  - Publication: [decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points](https://arxiv.org/abs/2404.12759) (2024-04-19)
  - Code: [GitHub](https://github.com/bytedance/decoupleQ)

- MoVA
  - Year: 2024
  - Publication: [MoVA: Adapting Mixture of Vision Experts to Multimodal Context](https://arxiv.org/abs/2404.13046) (2024-04-19)
  - Code: [GitHub](https://github.com/TempleX98/MoVA)

- OpenELM
  - Year: 2024
  - Publication: [OpenELM: An Efficient Language Model Family with Open Training and Inference Framework](https://arxiv.org/abs/2404.14619) (2024-04-22)
  - Code: [GitHub](https://github.com/apple/corenet)
  - Model weights: [HuggingFace models](https://huggingface.co/apple/OpenELM)

- phi-3
  - Year: 2024
  - Publication: [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) (2024-04-22)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)

- SEED-X
  - Year: 2024
  - Publication: [SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation](https://arxiv.org/abs/2404.14396) (2024-04-22)
  - Code: [GitHub](https://github.com/AILab-CVC/SEED-X)
  - Model weights: [HuggingFace models](https://huggingface.co/AILab-CVC/SEED-X-17B)

- NExT
  - Year: 2024
  - Publication: [NExT: Teaching Large Language Models to Reason about Code Execution](https://arxiv.org/abs/2404.14662) (2024-04-23)

- StarCoder2-Instruct
  - Year: 2024
  - Publication: [HuggingFace model card - StarCoder2-Instruct](https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1) (2024-04-23)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder2-self-align)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoder2-15b-instruct-v0.1)

- Snowflake Arctic
  - Year: 2024
  - Publication: [Blog - Snowflake Arctic: The Best LLM for Enterprise AI â€” Efficiently Intelligent, Truly Open](https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/) (2024-04-24)
  - Code: [GitHub](https://github.com/Snowflake-Labs/snowflake-arctic)
  - Model weights: [HuggingFace models](https://huggingface.co/Snowflake/snowflake-arctic-instruct)

- InternVL 1.5
  - Year: 2024
  - Publication: [How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites](https://arxiv.org/abs/2404.16821) (2024-04-25)
  - Code: [GitHub](https://github.com/OpenGVLab/InternVL)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5)

- LayerSkip
  - Year: 2024
  - Publication: [LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding](https://arxiv.org/abs/2404.16710) (2024-04-25)
  - Code: [GitHub](https://github.com/facebookresearch/LayerSkip)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/layerskip-llama3-8B)

- Hermes 2 Pro
  - Year: 2024
  - Publication: [HuggingFace model card - Hermes 2 Pro](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B) (2024-04-30)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B)

- Kolmogorov-Arnold Networks (KAN)
  - Year: 2024
  - Publication: [KAN: Kolmogorov-Arnold Networks](https://arxiv.org/abs/2404.19756) (2024-04-30)
  - Code: [GitHub](https://github.com/KindXiaoming/pykan)

- Multi-token Prediction
  - Year: 2024
  - Publication: [Better & Faster Large Language Models via Multi-token Prediction](https://arxiv.org/abs/2404.19737) (2024-04-30)

- Prometheus 2
  - Year: 2024
  - Publication: [Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models](https://arxiv.org/abs/2405.01535) (2024-05-02)
  - Code: [GitHub](https://github.com/prometheus-eval/prometheus-eval)
  - Model weights: [HuggingFace models](https://huggingface.co/prometheus-eval/prometheus-7b-v2.0)

- Hermes-2 Theta
  - Year: 2024
  - Publication: [HuggingFace model card - Hermes-2 Theta](https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B) (2024-05-05)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B)

- Higgs Llama
  - Year: 2024
  - Publication: [Blog - Announcing the Higgs Family of LLMs](https://boson.ai/higgs-opensource/) (2024-05-05)
  - Model weights: [HuggingFace models](https://huggingface.co/bosonai/Higgs-Llama-3-70B)

- AlphaMath
  - Year: 2024
  - Publication: [AlphaMath Almost Zero: process Supervision without process](https://arxiv.org/abs/2405.03553) (2024-05-06)
  - Code: [GitHub](https://github.com/MARIO-Math-Reasoning/Super_MARIO)
  - Model weights: [HuggingFace models](https://huggingface.co/MARIO-Math-Reasoning/AlphaMath-7B)

- MAmmoTH2
  - Year: 2024
  - Publication: [MAmmoTH2: Scaling Instructions from the Web](https://arxiv.org/abs/2405.03548) (2024-05-06)
  - Code: [GitHub](https://github.com/TIGER-AI-Lab/MAmmoTH2)
  - Model weights: [HuggingFace models](https://huggingface.co/TIGER-Lab/MAmmoTH2-7B)

- DeepSeek-V2
  - Year: 2024
  - Publication: [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434) (2024-05-07)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-V2)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/DeepSeek-V2)

- Granite Code Models
  - Year: 2024
  - Publication: [Granite Code Models: A Family of Open Foundation Models for Code Intelligence](https://arxiv.org/abs/2405.04324) (2024-05-07)
  - Code: [GitHub](https://github.com/ibm-granite/granite-code-models)
  - Model weights: [HuggingFace models](https://huggingface.co/ibm-granite/granite-3b-code-base)

- QServe
  - Year: 2024
  - Publication: [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532) (2024-05-07)
  - Code: [GitHub](https://github.com/mit-han-lab/qserve)

- vAttention
  - Year: 2024
  - Publication: [vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](https://arxiv.org/abs/2405.04437) (2024-05-07)

- xLSTM
  - Year: 2024
  - Publication: [xLSTM: Extended Long Short-Term Memory](https://arxiv.org/abs/2405.04517) (2024-05-07)

- Arctic-Embed
  - Year: 2024
  - Publication: [Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](https://arxiv.org/abs/2405.05374) (2024-05-08)
  - Code: [GitHub](https://github.com/Snowflake-Labs/arctic-embed)
  - Model weights: [HuggingFace models](https://huggingface.co/Snowflake/snowflake-arctic-embed-xs)

- YOCO
  - Year: 2024
  - Publication: [You Only Cache Once: Decoder-Decoder Architectures for Language Models](https://arxiv.org/abs/2405.05254) (2024-05-08)

- CuMo
  - Year: 2024
  - Publication: [CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts](https://arxiv.org/abs/2405.05949) (2024-05-09)
  - Code: [GitHub](https://github.com/SHI-Labs/CuMo)
  - Model weights: [HuggingFace models](https://huggingface.co/shi-labs/CuMo-mistral-7b)

- Zero-Shot Tokenizer Transfer (ZeTT)
  - Year: 2024
  - Publication: [Zero-Shot Tokenizer Transfer](https://arxiv.org/abs/2405.07883) (2024-05-13)
  - Code: [GitHub](https://github.com/bminixhofer/zett)

- Chameleon
  - Year: 2024
  - Publication: [Chameleon: Mixed-Modal Early-Fusion Foundation Models](https://arxiv.org/abs/2405.09818) (2024-05-16)

- Layer-Condensed KV Cache (LCKV)
  - Year: 2024
  - Publication: [Layer-Condensed KV Cache for Efficient Inference of Large Language Models](https://arxiv.org/abs/2405.10637) (2024-05-17)
  - Code: [GitHub](https://github.com/whyNLP/LCKV)

- SLAB
  - Year: 2024
  - Publication: [SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization](https://arxiv.org/abs/2405.11582) (2024-05-19)
  - Code: [GitHub](https://github.com/xinghaochen/SLAB)

- MoRA
  - Year: 2024
  - Publication: [MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2405.12130) (2024-05-20)
  - Code: [GitHub](https://github.com/kongds/MoRA)

- Cross-Layer Attention (CLA)
  - Year: 2024
  - Publication: [Reducing Transformer Key-Value Cache Size with Cross-Layer Attention](https://arxiv.org/abs/2405.12981) (2024-05-21)

- PyramidInfer
  - Year: 2024
  - Publication: [PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference](https://arxiv.org/abs/2405.12532) (2024-05-21)

- SirLLM
  - Year: 2024
  - Publication: [SirLLM: Streaming Infinite Retentive LLM](https://arxiv.org/abs/2405.12528) (2024-05-21)
  - Code: [GitHub](https://github.com/Zoeyyao27/SirLLM)

- Wav-KAN
  - Year: 2024
  - Publication: [Wav-KAN: Wavelet Kolmogorov-Arnold Networks](https://arxiv.org/abs/2405.12832) (2024-05-21)
  - Code: [GitHub](https://github.com/zavareh1/Wav-KAN)

- Aaren
  - Year: 2024
  - Publication: [Attention as an RNN](https://arxiv.org/abs/2405.13956) (2024-05-22)

- Dense Connector
  - Year: 2024
  - Publication: [Dense Connector for MLLMs](https://arxiv.org/abs/2405.13800) (2024-05-22)
  - Code: [GitHub](https://github.com/HJYao00/DenseConnector)
  - Model weights: [HuggingFace models](https://huggingface.co/HuanjinYao/DenseConnector-v1.5-7B)

- AlignGPT
  - Year: 2024
  - Publication: [AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability](https://arxiv.org/abs/2405.14129) (2024-05-23)
  - Code: [GitHub](https://github.com/AlignGPT-VL/AlignGPT)
  - Model weights: [HuggingFace models](https://huggingface.co/nlpzhaof/aligngpt-7b)

- Aya 23
  - Year: 2024
  - Publication: [Aya 23: Open Weight Releases to Further Multilingual Progress](https://arxiv.org/abs/2405.15032) (2024-05-23)
  - Model weights: [HuggingFace models](https://huggingface.co/CohereForAI/aya-23-8B)

- SimPO
  - Year: 2024
  - Publication: [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/abs/2405.14734) (2024-05-23)
  - Code: [GitHub](https://github.com/princeton-nlp/SimPO)
  - Model weights: [HuggingFace models](https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO)

- LLM-Stacking
  - Year: 2024
  - Publication: [Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training](https://arxiv.org/abs/2405.15319) (2024-05-24)
  - Code: [GitHub](https://github.com/tongxuluo/prts)
  - Model weights: [HuggingFace models](https://huggingface.co/llm-stacking/StackingLaw_Factor_1.1B12L)

- SpinQuant
  - Year: 2024
  - Publication: [SpinQuant: LLM quantization with learned rotations](https://arxiv.org/abs/2405.16406) (2024-05-26)
  - Code: [GitHub](https://github.com/facebookresearch/SpinQuant)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct-SpinQuant_INT4_EO8)

- Zamba
  - Year: 2024
  - Publication: [Zamba: A Compact 7B SSM Hybrid Model](https://arxiv.org/abs/2405.16712) (2024-05-26)

- Abacus Embeddings
  - Year: 2024
  - Publication: [Transformers Can Do Arithmetic with the Right Embeddings](https://arxiv.org/abs/2405.17399) (2024-05-27)
  - Code: [GitHub](https://github.com/mcleish7/arithmetic)

- Trans-LoRA
  - Year: 2024
  - Publication: [$\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning](https://arxiv.org/abs/2405.17258) (2024-05-27)

- LLaMA-NAS
  - Year: 2024
  - Publication: [LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models](https://arxiv.org/abs/2405.18377) (2024-05-28)

- VeLoRA
  - Year: 2024
  - Publication: [VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections](https://arxiv.org/abs/2405.17991) (2024-05-28)

- Yuan 2.0-M32
  - Year: 2024
  - Publication: [Yuan 2.0-M32: Mixture of Experts with Attention Router](https://arxiv.org/abs/2405.17976) (2024-05-28)
  - Code: [GitHub](https://github.com/IEIT-Yuan/Yuan2.0-M32)
  - Model weights: [HuggingFace models](https://huggingface.co/IEITYuan/Yuan2-M32-hf)

- Codestral
  - Year: 2024
  - Publication: [Blog - Codestral: Hello, World!](https://mistral.ai/news/codestral) (2024-05-29)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Codestral-22B-v0.1)

- Contextual Position Encoding (CoPE)
  - Year: 2024
  - Publication: [Contextual Position Encoding: Learning to Count What's Important](https://arxiv.org/abs/2405.18719) (2024-05-29)

- MAP-Neo
  - Year: 2024
  - Publication: [MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series](https://arxiv.org/abs/2405.19327) (2024-05-29)
  - Code: [GitHub](https://github.com/multimodal-art-projection/MAP-NEO)
  - Model weights: [HuggingFace models](https://huggingface.co/m-a-p/neo_7b)

- Gemini 1.5 Pro
  - Year: 2024
  - Publication: [Blog - Gemini 1.5 Pro and 1.5 Flash GA, 1.5 Flash tuning support, higher rate limits, and more API updates](https://developers.googleblog.com/en/gemini-15-pro-and-15-flash-now-available/) (2024-05-30)

- Mamba-2
  - Year: 2024
  - Publication: [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060) (2024-05-31)
  - Code: [GitHub](https://github.com/state-spaces/mamba)

- CodeQwen1.5
  - Year: 2024
  - Publication: [Blog - Code with CodeQwen1.5](https://qwenlm.github.io/blog/codeqwen1.5/) (2024-06-02)
  - Code: [GitHub](https://github.com/QwenLM/CodeQwen1.5)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat)

- Block Transformer
  - Year: 2024
  - Publication: [Block Transformer: Global-to-Local Language Modeling for Fast Inference](https://arxiv.org/abs/2406.02657) (2024-06-04)
  - Code: [GitHub](https://github.com/itsnamgyu/block-transformer)

- MatMul-free
  - Year: 2024
  - Publication: [Scalable MatMul-free Language Modeling](https://arxiv.org/abs/2406.02528) (2024-06-04)
  - Code: [GitHub](https://github.com/ridgerchu/matmulfreellm)
  - Model weights: [HuggingFace models](https://huggingface.co/ridger/MMfreeLM-370M)

- Xmodel-LM
  - Year: 2024
  - Publication: [Xmodel-LM Technical Report](https://arxiv.org/abs/2406.02856) (2024-06-05)
  - Code: [GitHub](https://github.com/XiaoduoAILab/XmodelLM)
  - Model weights: [HuggingFace models](https://huggingface.co/XiaoduoAILab/Xmodel_LM)

- Circuit Breakers
  - Year: 2024
  - Publication: [Improving Alignment and Robustness with Circuit Breakers](https://arxiv.org/abs/2406.04313) (2024-06-06)
  - Code: [GitHub](https://github.com/GraySwanAI/circuit-breakers)
  - Model weights: [HuggingFace models](https://huggingface.co/GraySwanAI/Llama-3-8B-Instruct-RR)

- MoA (Mixture-of-Agents)
  - Year: 2024
  - Publication: [Mixture-of-Agents Enhances Large Language Model Capabilities](https://arxiv.org/abs/2406.04692) (2024-06-07)
  - Code: [GitHub](https://github.com/togethercomputer/moa)

- Qwen2
  - Year: 2024
  - Publication: [Blog - Hello Qwen2](https://qwenlm.github.io/blog/qwen2/) (2024-06-07)
  - Code: [GitHub](https://github.com/QwenLM/Qwen2)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen2-7B-Instruct)

- PowerInfer-2
  - Year: 2024
  - Publication: [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](https://arxiv.org/abs/2406.06282) (2024-06-10)
  - Code: [GitHub](https://github.com/SJTU-IPADS/PowerInfer)

- ShiftAddLLM
  - Year: 2024
  - Publication: [ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization](https://arxiv.org/abs/2406.05981) (2024-06-10)
  - Code: [GitHub](https://github.com/GATECH-EIC/ShiftAddLLM)

- Turbo Sparse
  - Year: 2024
  - Publication: [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](https://arxiv.org/abs/2406.05955) (2024-06-10)
  - Model weights: [HuggingFace models](https://huggingface.co/PowerInfer/TurboSparse-Mistral-Instruct)

- Linearized-LLM
  - Year: 2024
  - Publication: [When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models](https://arxiv.org/abs/2406.07368) (2024-06-11)
  - Code: [GitHub](https://github.com/GATECH-EIC/Linearized-LLM)
  - Model weights: [HuggingFace models](https://huggingface.co/LinearizedLLM/llama-2-7b-aug-linear)

- Mamba-based LM
  - Year: 2024
  - Publication: [An Empirical Study of Mamba-based Language Models](https://arxiv.org/abs/2406.07887) (2024-06-12)

- Fox
  - Year: 2024
  - Publication: [Tweet: TensorOpera Unveils Fox Foundation Model: A Pioneering Small Language Model (SLM) for Cloud and Edge](https://blog.tensoropera.ai/tensoropera-unveils-fox-foundation-model-a-pioneering-open-source-slm-leading-the-way-against-tech-giants/) (2024-06-13)
  - Model weights: [HuggingFace models](https://huggingface.co/tensoropera/Fox-1-1.6B)

- Multi-Layer Key-Value Heads (MLKV)
  - Year: 2024
  - Publication: [MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding](https://arxiv.org/abs/2406.09297) (2024-06-13)
  - Code: [GitHub](https://github.com/zaydzuhri/pythia-mlkv)

- DCLM
  - Year: 2024
  - Publication: [DataComp-LM: In search of the next generation of training sets for language models](https://arxiv.org/abs/2406.11794) (2024-06-17)
  - Code: [GitHub](https://github.com/mlfoundations/dclm)
  - Model weights: [HuggingFace models](https://huggingface.co/apple/DCLM-7B)

- DeepSeek-Coder-V2
  - Year: 2024
  - Publication: [DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence](https://arxiv.org/abs/2406.11931) (2024-06-17)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-Coder-V2)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Base)

- ERASE
  - Year: 2024
  - Publication: [Language Modeling with Editable External Knowledge](https://arxiv.org/abs/2406.11830) (2024-06-17)
  - Code: [GitHub](https://github.com/belindal/ERASE)

- GLM-4
  - Year: 2024
  - Publication: [ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools](https://arxiv.org/abs/2406.12793) (2024-06-18)
  - Code: [GitHub](https://github.com/THUDM/GLM-4)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/glm-4-9b-chat)

- LayerMerge
  - Year: 2024
  - Publication: [LayerMerge: Neural Network Depth Compression through Layer Pruning and Merging](https://arxiv.org/abs/2406.12837) (2024-06-18)
  - Code: [GitHub](https://github.com/snu-mllab/LayerMerge)

- NuExtract
  - Year: 2024
  - Publication: [HuggingFace model card - NuExtract](https://huggingface.co/numind/NuExtract-large) (2024-06-19)
  - Model weights: [HuggingFace models](https://huggingface.co/numind/NuExtract-large)

- DeciMamba
  - Year: 2024
  - Publication: [DeciMamba: Exploring the Length Extrapolation Potential of Mamba](https://arxiv.org/abs/2406.14528) (2024-06-20)
  - Code: [GitHub](https://github.com/assafbk/DeciMamba)

- SmartSpec
  - Year: 2024
  - Publication: [Optimizing Speculative Decoding for Serving Large Language Models Using Goodput](https://arxiv.org/abs/2406.14066) (2024-06-20)

- LLM-Drop
  - Year: 2024
  - Publication: [What Matters in Transformers? Not All Attention is Needed](https://arxiv.org/abs/2406.15786) (2024-06-22)
  - Code: [GitHub](https://github.com/Shwai-He/LLM-Drop)
  - Model weights: [HuggingFace models](https://huggingface.co/s1ghhh/Llama-3-70b-Drop)

- EAGLE-2
  - Year: 2024
  - Publication: [EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](https://arxiv.org/abs/2406.16858) (2024-06-24)
  - Code: [GitHub](https://github.com/SafeAILab/EAGLE)

- RouteLLM
  - Year: 2024
  - Publication: [RouteLLM: Learning to Route LLMs with Preference Data](https://arxiv.org/abs/2406.18665) (2024-06-26)
  - Code: [GitHub](https://github.com/lm-sys/RouteLLM)

- Universal Checkpointing (UCP)
  - Year: 2024
  - Publication: [Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training](https://arxiv.org/abs/2406.18820) (2024-06-27)

- Memory^3
  - Year: 2024
  - Publication: [$\text{Memory}^3$: Language Modeling with Explicit Memory](https://arxiv.org/abs/2407.01178) (2024-07-01)

- MInference
  - Year: 2024
  - Publication: [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](https://arxiv.org/abs/2407.02490) (2024-07-02)
  - Code: [GitHub](https://github.com/microsoft/MInference)

- RankRAG
  - Year: 2024
  - Publication: [RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://arxiv.org/abs/2407.02485) (2024-07-02)

- InternVL 2
  - Year: 2024
  - Publication: [Blog - InternVL2: Better than the Best - Expanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/) (2024-07-04)
  - Code: [GitHub](https://github.com/OpenGVLab/InternVL)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenGVLab/InternVL2-26B)

- TTT-Linear
  - Year: 2024
  - Publication: [Learning to (Learn at Test Time): RNNs with Expressive Hidden States](https://arxiv.org/abs/2407.04620) (2024-07-05)
  - Code: [GitHub](https://github.com/test-time-training/ttt-lm-pytorch)

- Lookback Lens
  - Year: 2024
  - Publication: [Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps](https://arxiv.org/abs/2407.07071) (2024-07-09)
  - Code: [GitHub](https://github.com/voidism/Lookback-Lens)

- FlashAttention-3
  - Year: 2024
  - Publication: [FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision](https://arxiv.org/abs/2407.08608) (2024-07-11)
  - Code: [GitHub](https://github.com/Dao-AILab/flash-attention)

- Lynx
  - Year: 2024
  - Publication: [Lynx: An Open Source Hallucination Evaluation Model](https://arxiv.org/abs/2407.08488) (2024-07-11)
  - Code: [GitHub](https://github.com/patronus-ai/Lynx-hallucination-detection)
  - Model weights: [HuggingFace models](https://huggingface.co/PatronusAI/Llama-3-Patronus-Lynx-70B-Instruct)

- Q-GaLore
  - Year: 2024
  - Publication: [Q-GaLore: Quantized GaLore with INT4 Projection and Layer-Adaptive Low-Rank Gradients](https://arxiv.org/abs/2407.08296) (2024-07-11)
  - Code: [GitHub](https://github.com/VITA-Group/Q-GaLore)

- H2O-Danube3
  - Year: 2024
  - Publication: [H2O-Danube3 Technical Report](https://arxiv.org/abs/2407.09276) (2024-07-12)
  - Model weights: [HuggingFace models](https://huggingface.co/h2oai/h2o-danube3-4b-chat)

- Arcee-Nova
  - Year: 2024
  - Publication: [HuggingFace model card - Arcee-Nova](https://huggingface.co/arcee-ai/Arcee-Nova) (2024-07-16)
  - Model weights: [HuggingFace models](https://huggingface.co/arcee-ai/Arcee-Nova?ref=blog.arcee.ai)

- Codestral Mamba
  - Year: 2024
  - Publication: [Blog - Codestral Mamba](https://mistral.ai/news/codestral-mamba/) (2024-07-16)
  - Code: [GitHub](https://github.com/mistralai/mistral-inference)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1)

- MathÎ£tral
  - Year: 2024
  - Publication: [Blog - MathÎ£tral](https://mistral.ai/news/mathstral/) (2024-07-16)
  - Code: [GitHub](https://github.com/mistralai/mistral-inference)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mathstral-7B-v0.1)

- SmolLM
  - Year: 2024
  - Publication: [HuggingFace model card - SmolLM - blazingly fast and remarkably powerful](https://huggingface.co/blog/smollm) (2024-07-16)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceTB/SmolLM-135M)

- Spectra
  - Year: 2024
  - Publication: [Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models](https://arxiv.org/abs/2407.12327) (2024-07-17)
  - Code: [GitHub](https://github.com/NolanoOrg/SpectraSuite)
  - Model weights: [HuggingFace models](https://huggingface.co/SpectraSuite/FloatLM_99M)

- Higgs Llama V2
  - Year: 2024
  - Publication: [Blog - Announcing Higgs Llama V2](https://boson.ai/higgs-v2/) (2024-07-18)

- Mistral NeMo
  - Year: 2024
  - Publication: [Blog - Mistral NeMo](https://mistral.ai/news/mistral-nemo/) (2024-07-18)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407)

- BOND
  - Year: 2024
  - Publication: [BOND: Aligning LLMs with Best-of-N Distillation](https://arxiv.org/abs/2407.14622) (2024-07-19)

- LazyLLM
  - Year: 2024
  - Publication: [LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference](https://arxiv.org/abs/2407.14057) (2024-07-19)

- Minitron
  - Year: 2024
  - Publication: [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2407.14679) (2024-07-19)
  - Code: [GitHub](https://github.com/NVlabs/Minitron)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Nemotron-4-Minitron-4B-Base)

- Mistral Large 2
  - Year: 2024
  - Publication: [Blog - Large Enough](https://mistral.ai/news/mistral-large-2407/) (2024-07-24)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407)

- u-uP
  - Year: 2024
  - Publication: [u-$\mu$P: The Unit-Scaled Maximal Update Parametrization](https://arxiv.org/abs/2407.17465) (2024-07-24)

- SaulLM-141B
  - Year: 2024
  - Publication: [SaulLM-54B & SaulLM-141B: Scaling Up Domain Adaptation for the Legal Domain](https://arxiv.org/abs/2407.19584) (2024-07-28)

- Generalized Long-Context Text Representation (mGTE)
  - Year: 2024
  - Publication: [mGTE: Generalized Long-Context Text Representation and Reranking Models for Multilingual Text Retrieval](https://arxiv.org/abs/2407.19669) (2024-07-29)
  - Model weights: [HuggingFace models](https://huggingface.co/Alibaba-NLP/gte-en-mlm-base)

- Gemma 2
  - Year: 2024
  - Publication: [Gemma 2: Improving Open Language Models at a Practical Size](https://arxiv.org/abs/2408.00118) (2024-07-31)
  - Model weights: [HuggingFace models](https://huggingface.co/google/gemma-2-2b)

- Gemma Scope
  - Year: 2024
  - Publication: [HuggingFace model card - Gemma Scope](https://huggingface.co/google/gemma-scope) (2024-07-31)
  - Model weights: [HuggingFace models](https://huggingface.co/google/gemma-scope)

- Llama 3.1
  - Year: 2024
  - Publication: [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783) (2024-07-31)
  - Code: [GitHub](https://github.com/meta-llama/llama3)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B)

- Llama Guard 3
  - Year: 2024
  - Publication: [HuggingFace model card - Llama Guard 3](https://huggingface.co/meta-llama/Llama-Guard-3-8B) (2024-07-31)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Llama-Guard-3-8B)

- Prompt Guard
  - Year: 2024
  - Publication: [HuggingFace model card - Prompt Guard](https://huggingface.co/meta-llama/Prompt-Guard-86M) (2024-07-31)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Prompt-Guard-86M)

- ShieldGemma
  - Year: 2024
  - Publication: [ShieldGemma: Generative AI Content Moderation Based on Gemma](https://arxiv.org/abs/2407.21772) (2024-07-31)
  - Model weights: [HuggingFace models](https://huggingface.co/google/shieldgemma-2b)

- InfiMM
  - Year: 2024
  - Publication: [Blog - InfiMM: Advancing Multimodal Understanding with an Open-Sourced Visual Language Model](https://aclanthology.org/2024.findings-acl.27/) (2024-08-01)
  - Model weights: [HuggingFace models](https://huggingface.co/Infi-MM/infimm-zephyr)

- InternLM2.5
  - Year: 2024
  - Publication: [HuggingFace model card - InternLM2.5](https://huggingface.co/internlm/internlm2_5-20b-chat) (2024-08-06)
  - Code: [GitHub](https://github.com/InternLM/InternLM)
  - Model weights: [HuggingFace models](https://huggingface.co/internlm/internlm2_5-20b-chat)

- EXAONE 3.0
  - Year: 2024
  - Publication: [EXAONE 3.0 7.8B Instruction Tuned Language Model](https://arxiv.org/abs/2408.03541) (2024-08-07)
  - Code: [GitHub](https://github.com/LG-AI-EXAONE/EXAONE-3.0)
  - Model weights: [HuggingFace models](https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct)

- Tree Attention
  - Year: 2024
  - Publication: [Tree Attention: Topology-aware Decoding for Long-Context Attention on GPU clusters](https://arxiv.org/abs/2408.04093) (2024-08-07)
  - Code: [GitHub](https://github.com/Zyphra/tree_attention)

- Qwen2-Math
  - Year: 2024
  - Publication: [Blog - Introducing Qwen2-Math](https://qwenlm.github.io/blog/qwen2-math/) (2024-08-08)
  - Code: [GitHub](https://github.com/QwenLM/Qwen2-Math)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen2-Math-72B-Instruct)

- rStar
  - Year: 2024
  - Publication: [Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers](https://arxiv.org/abs/2408.06195) (2024-08-12)
  - Code: [GitHub](https://github.com/zhentingqi/rStar)

- Grok-2
  - Year: 2024
  - Publication: [Tweet: Grok-2 Beta Release](https://x.ai/blog/grok-2) (2024-08-13)
  - Model weights: [HuggingFace models](https://huggingface.co/xai-org/grok-2)

- LongWriter
  - Year: 2024
  - Publication: [LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs](https://arxiv.org/abs/2408.07055) (2024-08-13)
  - Code: [GitHub](https://github.com/THUDM/LongWriter)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/LongWriter-glm4-9b)

- DeepSeek-Prover-V1.5
  - Year: 2024
  - Publication: [DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search](https://arxiv.org/abs/2408.08152) (2024-08-15)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-Prover-V1.5)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-Base)

- Hermes 3
  - Year: 2024
  - Publication: [Blog - Nous research presents Hermes 3](https://nousresearch.com/hermes3/) (2024-08-16)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Hermes-3-Llama-3.2-3B)

- Phi 3.5
  - Year: 2024
  - Publication: [HuggingFace model card - Phi 3.5](https://huggingface.co/microsoft/Phi-3.5-mini-instruct) (2024-08-16)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)

- Zamba 2
  - Year: 2024
  - Publication: [HuggingFace model card - Zamba 2](https://huggingface.co/Zyphra/Zamba2-1.2B) (2024-08-16)
  - Code: [GitHub](https://github.com/Zyphra/transformers_zamba2)
  - Model weights: [HuggingFace models](https://huggingface.co/Zyphra/Zamba2-1.2B)

- Magnum v2
  - Year: 2024
  - Publication: [HuggingFace model card - Magnum v2](https://huggingface.co/anthracite-org/magnum-v2-123b) (2024-08-17)
  - Model weights: [HuggingFace models](https://huggingface.co/anthracite-org/magnum-v2-123b)

- Llama 3.1 Storm
  - Year: 2024
  - Publication: [HuggingFace model card - Llama-3.1-Storm-8B: Improved SLM with Self-Curation + Model Merging](https://huggingface.co/blog/akjindal53244/llama31-storm8b) (2024-08-19)
  - Model weights: [HuggingFace models](https://huggingface.co/akjindal53244/Llama-3.1-Storm-8B)

- MagicDec
  - Year: 2024
  - Publication: [MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding](https://arxiv.org/abs/2408.11049) (2024-08-20)
  - Code: [GitHub](https://github.com/Infini-AI-Lab/MagicDec/)

- Minitron Approach
  - Year: 2024
  - Publication: [LLM Pruning and Distillation in Practice: The Minitron Approach](https://arxiv.org/abs/2408.11796) (2024-08-21)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base)

- Jamba 1.5
  - Year: 2024
  - Publication: [Blog - The Jamba 1.5 Open Model Family: The Most Powerful and Efficient Long Context Models](https://www.ai21.com/blog/announcing-jamba-model-family) (2024-08-22)
  - Model weights: [HuggingFace models](https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large)

- LlavaOLMoBitnet1B
  - Year: 2024
  - Publication: [LLaVaOLMoBitnet1B: Ternary LLM goes Multimodal!](https://arxiv.org/abs/2408.13402) (2024-08-23)
  - Model weights: [HuggingFace models](https://huggingface.co/IntelLabs/LlavaOLMoBitnet1B)

- Mamba2-Llama3
  - Year: 2024
  - Publication: [The Mamba in the Llama: Distilling and Accelerating Hybrid Models](https://arxiv.org/abs/2408.15237) (2024-08-27)

- Rene
  - Year: 2024
  - Publication: [Blog - The Onâ€‘Device Intelligence Update](https://cartesia.ai/blog/2024-08-27-on-device) (2024-08-27)
  - Model weights: [HuggingFace models](https://huggingface.co/cartesia-ai/Rene-v0.1-1.3b-pytorch)

- LTM-2-mini
  - Year: 2024
  - Publication: [Tweet: 100M Token Context Windows](https://magic.dev/blog/100m-token-context-windows) (2024-08-29)

- Command R 08-2024
  - Year: 2024
  - Publication: [Blog - Updates to the Command R Series](https://cohere.com/blog/command-series-0824) (2024-08-30)
  - Model weights: [HuggingFace models](https://huggingface.co/CohereForAI/c4ai-command-r-08-2024)

- Fully Pipelined Distributed Transformer (FPDT)
  - Year: 2024
  - Publication: [Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer](https://arxiv.org/abs/2408.16978) (2024-08-30)

- MemLong
  - Year: 2024
  - Publication: [MemLong: Memory-Augmented Retrieval for Long Text Modeling](https://arxiv.org/abs/2408.16967) (2024-08-30)
  - Code: [GitHub](https://github.com/Bui1dMySea/MemLong)

- OLMoE
  - Year: 2024
  - Publication: [OLMoE: Open Mixture-of-Experts Language Models](https://arxiv.org/abs/2409.02060) (2024-09-03)
  - Code: [GitHub](https://github.com/allenai/OLMoE)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/OLMoE-1B-7B-0924)

- LongLLaVA
  - Year: 2024
  - Publication: [LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via a Hybrid Architecture](https://arxiv.org/abs/2409.02889) (2024-09-04)
  - Code: [GitHub](https://github.com/freedomintelligence/longllava)
  - Model weights: [HuggingFace models](https://huggingface.co/FreedomIntelligence/LongLLaVA)

- DeepSeek-V2.5
  - Year: 2024
  - Publication: [HuggingFace model card - DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5) (2024-09-05)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)

- Yi-Coder
  - Year: 2024
  - Publication: [Blog - Meet Yi-Coder: A Small but Mighty LLM for Code](https://01-ai.github.io/blog.html?post=en/2024-09-05-A-Small-but-Mighty-LLM-for-Code.md) (2024-09-05)
  - Code: [GitHub](https://github.com/01-ai/Yi-Coder)
  - Model weights: [HuggingFace models](https://huggingface.co/01-ai/Yi-Coder-9B-Chat)

- FlashSigmoid
  - Year: 2024
  - Publication: [Theory, Analysis, and Best Practices for Sigmoid Self-Attention](https://arxiv.org/abs/2409.04431) (2024-09-06)
  - Code: [GitHub](https://github.com/apple/ml-sigmoid-attention)

- OneGen
  - Year: 2024
  - Publication: [OneGen: Efficient One-Pass Unified Generation and Retrieval for LLMs](https://arxiv.org/abs/2409.05152) (2024-09-08)
  - Code: [GitHub](https://github.com/zjunlp/OneGen)
  - Model weights: [HuggingFace models](https://huggingface.co/zjunlp/OneGen-EntityLinking-Llama2-7B)

- Arcee-SuperNova
  - Year: 2024
  - Publication: [Blog - Arcee-SuperNova: Training Pipeline and Model Composition](https://blog.arcee.ai/arcee-supernova-training-pipeline-and-model-composition/) (2024-09-10)

- SaRA
  - Year: 2024
  - Publication: [SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation](https://arxiv.org/abs/2409.06633) (2024-09-10)
  - Code: [GitHub](https://github.com/sjtuplayer/SaRA)

- Gated Slot Attention (GSA)
  - Year: 2024
  - Publication: [Gated Slot Attention for Efficient Linear-Time Sequence Modeling](https://arxiv.org/abs/2409.07146) (2024-09-11)
  - Code: [GitHub](https://github.com/berlino/gated_linear_attention)
  - Model weights: [HuggingFace models](https://huggingface.co/fla-hub/gla-2.7B-100B)

- Reader-LM
  - Year: 2024
  - Publication: [Blog - Reader-LM: Small Language Models for Cleaning and Converting HTML to Markdown](https://jina.ai/news/reader-lm-small-language-models-for-cleaning-and-converting-html-to-markdown/) (2024-09-11)
  - Model weights: [HuggingFace models](https://huggingface.co/jinaai/reader-lm-1.5b)

- jina-embeddings-v3
  - Year: 2024
  - Publication: [jina-embeddings-v3: Multilingual Embeddings With Task LoRA](https://arxiv.org/abs/2409.10173) (2024-09-16)
  - Model weights: [HuggingFace models](https://huggingface.co/jinaai/jina-embeddings-v3)

- Kolmogorov-Arnold Transformer (KAT)
  - Year: 2024
  - Publication: [Kolmogorov-Arnold Transformer](https://arxiv.org/abs/2409.10594) (2024-09-16)
  - Code: [GitHub](https://github.com/Adamdad/kat)

- NVLM
  - Year: 2024
  - Publication: [NVLM: Open Frontier-Class Multimodal LLMs](https://arxiv.org/abs/2409.11402) (2024-09-17)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/NVLM-D-72B)

- GRIN MOE
  - Year: 2024
  - Publication: [GRIN: GRadient-INformed MoE](https://arxiv.org/abs/2409.12136) (2024-09-18)

- Qwen2.5-Coder
  - Year: 2024
  - Publication: [Qwen2.5-Coder Technical Report](https://arxiv.org/abs/2409.12186) (2024-09-18)
  - Code: [GitHub](https://github.com/QwenLM/Qwen2.5-Coder)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen2.5-Coder-7B)

- Qwen2.5-Math
  - Year: 2024
  - Publication: [Blog - Introducing Qwen2-MathQwen2.5-Math: The world's leading open-sourced mathematical LLMs](https://qwenlm.github.io/blog/qwen2.5-math/) (2024-09-19)
  - Code: [GitHub](https://github.com/QwenLM/Qwen2.5-Math)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen2.5-Math-72B)

- Llama-3.1-Nemotron
  - Year: 2024
  - Publication: [Blog - Advancing the Accuracy-Efficiency Frontier with Llama-3.1-Nemotron-51B](https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/) (2024-09-23)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Llama-3_1-Nemotron-51B-Instruct)

- BitQ
  - Year: 2024
  - Publication: [BitQ: Tailoring Block Floating Point Precision for Improved DNN Efficiency on Resource-Constrained Devices](https://arxiv.org/abs/2409.17093) (2024-09-25)
  - Code: [GitHub](https://github.com/Cheliosoops/BitQ)

- Llama 3.2
  - Year: 2024
  - Publication: [Blog - Llama 3.2: Revolutionizing edge AI and vision with open, customizable models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/) (2024-09-25)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)

- Molmo
  - Year: 2024
  - Publication: [Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models](https://arxiv.org/abs/2409.17146) (2024-09-25)

- Vector Post-Training Quantization (VPTQ)
  - Year: 2024
  - Publication: [VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2409.17066) (2024-09-25)
  - Code: [GitHub](https://github.com/microsoft/VPTQ)

- EMOVA
  - Year: 2024
  - Publication: [EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions](https://arxiv.org/abs/2409.18042) (2024-09-26)

- MIO
  - Year: 2024
  - Publication: [MIO: A Foundation Model on Multimodal Tokens](https://arxiv.org/abs/2409.17692) (2024-09-26)

- Cottention
  - Year: 2024
  - Publication: [Cottention: Linear Transformers With Cosine Attention](https://arxiv.org/abs/2409.18747) (2024-09-27)
  - Code: [GitHub](https://github.com/gmongaras/Cottention_Transformer)
  - Model weights: [HuggingFace models](https://huggingface.co/gmongaras/Cosine_Attention_GPT_300M)

- Hyper-Connections
  - Year: 2024
  - Publication: [Hyper-Connections](https://arxiv.org/abs/2409.19606) (2024-09-29)

- Dracarys2
  - Year: 2024
  - Publication: [HuggingFace model card - Dracarys2](https://huggingface.co/abacusai/Dracarys2-72B-Instruct) (2024-09-30)
  - Model weights: [HuggingFace models](https://huggingface.co/abacusai/Dracarys2-72B-Instruct)

- LFM
  - Year: 2024
  - Publication: [Blog - Liquid Foundation Models: Our First Series of Generative AI Models](https://www.liquid.ai/liquid-foundation-models) (2024-09-30)

- MM1.5
  - Year: 2024
  - Publication: [MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning](https://arxiv.org/abs/2409.20566) (2024-09-30)

- Normalized Transformer (nGPT)
  - Year: 2024
  - Publication: [nGPT: Normalized Transformer with Representation Learning on the Hypersphere](https://arxiv.org/abs/2410.01131) (2024-10-01)

- Encoder-only Next Token Prediction (ENTP)
  - Year: 2024
  - Publication: [ENTP: Encoder-only Next Token Prediction](https://arxiv.org/abs/2410.01600) (2024-10-02)

- FactAlign
  - Year: 2024
  - Publication: [FactAlign: Long-form Factuality Alignment of Large Language Models](https://arxiv.org/abs/2410.01691) (2024-10-02)
  - Code: [GitHub](https://github.com/MiuLab/FactAlign)
  - Model weights: [HuggingFace models](https://huggingface.co/datasets/chaoweihuang/factalign-gemma2-f1_0.75)

- Leopard
  - Year: 2024
  - Publication: [Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks](https://arxiv.org/abs/2410.01744) (2024-10-02)
  - Code: [GitHub](https://github.com/tencent-ailab/Leopard)

- minLSTMs
  - Year: 2024
  - Publication: [Were RNNs All We Needed?](https://arxiv.org/abs/2410.01201) (2024-10-02)

- Reflection
  - Year: 2024
  - Publication: [Blog - Update on Reflection-70B](https://glaive.ai/blog/post/reflection-postmortem) (2024-10-02)
  - Code: [GitHub](https://github.com/glaive-ai/reflection_70b_training)
  - Model weights: [HuggingFace models](https://huggingface.co/glaiveai/Reflection-Llama-3.1-70B)

- SageAttention
  - Year: 2024
  - Publication: [SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration](https://arxiv.org/abs/2410.02367) (2024-10-03)
  - Code: [GitHub](https://github.com/thu-ml/SageAttention)

- Selective Attention
  - Year: 2024
  - Publication: [Selective Attention Improves Transformer](https://arxiv.org/abs/2410.02703) (2024-10-03)

- ToolGen
  - Year: 2024
  - Publication: [ToolGen: Unified Tool Retrieval and Calling via Generation](https://arxiv.org/abs/2410.03439) (2024-10-04)
  - Code: [GitHub](https://github.com/Reason-Wang/ToolGen)
  - Model weights: [HuggingFace models](https://huggingface.co/reasonwang/ToolGen-Llama-3-8B)

- Diff Transformer
  - Year: 2024
  - Publication: [Differential Transformer](https://arxiv.org/abs/2410.05258) (2024-10-07)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/Diff-Transformer)

- Falcon Mamba
  - Year: 2024
  - Publication: [Falcon Mamba: The First Competitive Attention-free 7B Language Model](https://arxiv.org/abs/2410.05355) (2024-10-07)
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/falcon-mamba-7b)

- PredFormer
  - Year: 2024
  - Publication: [PredFormer: Transformers Are Effective Spatial-Temporal Predictive Learners](https://arxiv.org/abs/2410.04733) (2024-10-07)
  - Model weights: [Direct link](https://github.com/yyyujintang/PredFormer)

- Aria
  - Year: 2024
  - Publication: [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://arxiv.org/abs/2410.05993) (2024-10-08)
  - Code: [GitHub](https://github.com/rhymes-ai/Aria)
  - Model weights: [HuggingFace models](https://huggingface.co/rhymes-ai/Aria)

- MoE++
  - Year: 2024
  - Publication: [MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts](https://arxiv.org/abs/2410.07348) (2024-10-09)
  - Code: [GitHub](https://github.com/SkyworkAI/MoE-plus-plus)
  - Model weights: [HuggingFace models](https://huggingface.co/Chat-UniVi/MoE-Plus-Plus-7B)

- Pixtral
  - Year: 2024
  - Publication: [Pixtral 12B](https://arxiv.org/abs/2410.07073) (2024-10-09)

- KV Prediction
  - Year: 2024
  - Publication: [KV Prediction for Improved Time to First Token](https://arxiv.org/abs/2410.08391) (2024-10-10)
  - Code: [GitHub](https://github.com/apple/corenet/tree/main/projects/kv-prediction)

- FlatQuant
  - Year: 2024
  - Publication: [FlatQuant: Flatness Matters for LLM Quantization](https://arxiv.org/abs/2410.09426) (2024-10-12)
  - Code: [GitHub](https://github.com/ruikangliu/FlatQuant)

- DuoAttention
  - Year: 2024
  - Publication: [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](https://arxiv.org/abs/2410.10819) (2024-10-14)
  - Code: [GitHub](https://github.com/mit-han-lab/duo-attention)

- Mixture-of-Experts Embedding (MoEE)
  - Year: 2024
  - Publication: [Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free](https://arxiv.org/abs/2410.10814) (2024-10-14)
  - Code: [GitHub](https://github.com/tianyi-lab/MoE-Embedding)

- SeedLM
  - Year: 2024
  - Publication: [SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators](https://arxiv.org/abs/2410.10714) (2024-10-14)

- Zamba 2 7B
  - Year: 2024
  - Publication: [Blog - Zamba2-7B](https://zyphra.webflow.io/post/zamba2-7b) (2024-10-14)
  - Code: [GitHub](https://github.com/Zyphra/Zamba2)
  - Model weights: [HuggingFace models](https://huggingface.co/Zyphra/Zamba2-7B-Instruct)

- Mini-Omni2
  - Year: 2024
  - Publication: [Mini-Omni2: Towards Open-source GPT-4o with Vision, Speech and Duplex Capabilities](https://arxiv.org/abs/2410.11190) (2024-10-15)
  - Code: [GitHub](https://github.com/gpt-omni/mini-omni2)
  - Model weights: [HuggingFace models](https://huggingface.co/gpt-omni/mini-omni2)

- Multi-Head Attention as Mixture-of-Head Attention (MoH)
  - Year: 2024
  - Publication: [MoH: Multi-Head Attention as Mixture-of-Head Attention](https://arxiv.org/abs/2410.11842) (2024-10-15)
  - Code: [GitHub](https://github.com/SkyworkAI/MoH)
  - Model weights: [HuggingFace models](https://huggingface.co/Chat-UniVi/MoH-LLaMA3-8B)

- Ministral
  - Year: 2024
  - Publication: [Tweet: Un Ministral, des Ministraux](https://mistral.ai/news/ministraux/) (2024-10-16)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410)

- Janus
  - Year: 2024
  - Publication: [Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2410.13848) (2024-10-17)
  - Code: [GitHub](https://github.com/deepseek-ai/Janus)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/Janus-1.3B)

- Neural Attention Memory Models (NAMMs)
  - Year: 2024
  - Publication: [An Evolved Universal Transformer Memory](https://arxiv.org/abs/2410.13166) (2024-10-17)
  - Code: [GitHub](https://github.com/SakanaAI/evo-memory)

- SeerAttention
  - Year: 2024
  - Publication: [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](https://arxiv.org/abs/2410.13276) (2024-10-17)
  - Code: [GitHub](https://github.com/microsoft/SeerAttention)

- Granite 3.0
  - Year: 2024
  - Publication: [Blog - IBM Granite 3.0: open, state-of-the-art enterprise models](https://www.ibm.com/new/ibm-granite-3-0-open-state-of-the-art-enterprise-models) (2024-10-21)
  - Code: [GitHub](https://github.com/ibm-granite/granite-3.0-language-models)
  - Model weights: [HuggingFace models](https://huggingface.co/ibm-granite/granite-3.0-8b-instruct)

- Mini-InternVL
  - Year: 2024
  - Publication: [Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance](https://arxiv.org/abs/2410.16261) (2024-10-21)
  - Code: [GitHub](https://github.com/OpenGVLab/InternVL)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5)

- Pangea
  - Year: 2024
  - Publication: [Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages](https://arxiv.org/abs/2410.16153) (2024-10-21)
  - Code: [GitHub](https://github.com/neulab/Pangea)
  - Model weights: [HuggingFace models](https://huggingface.co/neulab/Pangea-7B)

- Claude 3.5 Sonnet and Haiku
  - Year: 2024
  - Publication: [Blog - Claude 3.5 Sonnet and Claude 3.5 Haiku](https://www.anthropic.com/news/3-5-models-and-computer-use) (2024-10-22)

- MiniPLM
  - Year: 2024
  - Publication: [MiniPLM: Knowledge Distillation for Pre-Training Language Models](https://arxiv.org/abs/2410.17215) (2024-10-22)
  - Code: [GitHub](https://github.com/thu-coai/MiniPLM)
  - Model weights: [HuggingFace models](https://huggingface.co/MiniLLM/MiniPLM-Qwen-1.2B)

- Aya Expanse
  - Year: 2024
  - Publication: [Tweet: Aya Expanse: Connecting Our World](https://cohere.com/blog/aya-expanse-connecting-our-world) (2024-10-24)
  - Model weights: [HuggingFace models](https://huggingface.co/CohereForAI/aya-expanse-32b)

- MrT5 (MergeT5)
  - Year: 2024
  - Publication: [MrT5: Dynamic Token Merging for Efficient Byte-level Language Models](https://arxiv.org/abs/2410.20771) (2024-10-28)
  - Code: [GitHub](https://github.com/jkallini/mrt5)

- Relaxed Recursive Transformers
  - Year: 2024
  - Publication: [Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA](https://arxiv.org/abs/2410.20672) (2024-10-28)

- SmolLM 2
  - Year: 2024
  - Publication: [HuggingFace model card - SmolLM 2](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B) (2024-10-30)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B)

- TokenFormer
  - Year: 2024
  - Publication: [TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters](https://arxiv.org/abs/2410.23168) (2024-10-30)
  - Code: [GitHub](https://github.com/Haiyang-W/TokenFormer)
  - Model weights: [HuggingFace models](https://huggingface.co/Haiyang-W/TokenFormer-1-5B)

- AMD-OLMo
  - Year: 2024
  - Publication: [HuggingFace model card - AMD-OLMo](https://huggingface.co/amd/AMD-OLMo) (2024-10-31)
  - Model weights: [HuggingFace models](https://huggingface.co/amd/AMD-OLMo)

- Sample-Efficient Alignment (SEA)
  - Year: 2024
  - Publication: [Sample-Efficient Alignment for LLMs](https://arxiv.org/abs/2411.01493) (2024-11-03)
  - Code: [GitHub](https://github.com/sail-sg/oat)

- Context Parallelism
  - Year: 2024
  - Publication: [Context Parallelism for Scalable Million-Token Inference](https://arxiv.org/abs/2411.01783) (2024-11-04)

- Hunyuan-Large
  - Year: 2024
  - Publication: [Hunyuan-Large: An Open-Source MoE Model with 52 Billion Activated Parameters by Tencent](https://arxiv.org/abs/2411.02265) (2024-11-04)
  - Code: [GitHub](https://github.com/Tencent/Hunyuan-Large)
  - Model weights: [HuggingFace models](https://huggingface.co/tencent/Tencent-Hunyuan-Large)

- LASER
  - Year: 2024
  - Publication: [LASER: Attention with Exponential Transformation](https://arxiv.org/abs/2411.03493) (2024-11-05)

- BitNet a4.8
  - Year: 2024
  - Publication: [BitNet a4.8: 4-bit Activations for 1-bit LLMs](https://arxiv.org/abs/2411.04965) (2024-11-07)

- Online-LoRA
  - Year: 2024
  - Publication: [README - Online-LoRA repository](https://github.com/Christina200/Online-LoRA-official/blob/01598cf1f1d6ba33fef5852daeeceb4c0b4a71ff/README.md) (2024-11-07)
  - Code: [GitHub](https://github.com/christina200/online-lora-official)

- OpenCoder
  - Year: 2024
  - Publication: [OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models](https://arxiv.org/abs/2411.04905) (2024-11-07)
  - Code: [GitHub](https://github.com/OpenCoder-llm/OpenCoder-llm)
  - Model weights: [HuggingFace models](https://huggingface.co/infly/OpenCoder-8B-Instruct)

- Q-SFT
  - Year: 2024
  - Publication: [Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning](https://arxiv.org/abs/2411.05193) (2024-11-07)

- CodeRankEmbed
  - Year: 2024
  - Publication: [HuggingFace model card - CodeRankEmbed](https://huggingface.co/cornstack/CodeRankEmbed) (2024-11-08)
  - Model weights: [HuggingFace models](https://huggingface.co/cornstack/CodeRankEmbed)

- CodeRankLLM
  - Year: 2024
  - Publication: [HuggingFace model card - CodeRankLLM](https://huggingface.co/cornstack/CodeRankLLM) (2024-11-08)
  - Model weights: [HuggingFace models](https://huggingface.co/cornstack/CodeRankLLM)

- JanusFlow
  - Year: 2024
  - Publication: [JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2411.07975) (2024-11-12)
  - Code: [GitHub](https://github.com/deepseek-ai/Janus)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/JanusFlow-1.3B)

- InternVL 2.5 MPO
  - Year: 2024
  - Publication: [Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization](https://arxiv.org/abs/2411.10442) (2024-11-15)
  - Code: [GitHub](https://github.com/OpenGVLab/InternVL)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO)

- Qwen2.5-Turbo
  - Year: 2024
  - Publication: [Tweet: Extending the Context Length to 1M Tokens!](Extending the Context Length to 1M Tokens!) (2024-11-15)

- Xmodel-1.5
  - Year: 2024
  - Publication: [Xmodel-1.5: An 1B-scale Multilingual LLM](https://arxiv.org/abs/2411.10083) (2024-11-15)
  - Code: [GitHub](https://github.com/XiaoduoAILab/XmodelLM-1.5)
  - Model weights: [HuggingFace models](https://huggingface.co/XiaoduoAILab/XmodelLM1.5)

- SageAttention 2
  - Year: 2024
  - Publication: [SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization](https://arxiv.org/abs/2411.10958) (2024-11-17)
  - Code: [GitHub](https://github.com/thu-ml/SageAttention)

- Bi-Mamba
  - Year: 2024
  - Publication: [Bi-Mamba: Towards Accurate 1-Bit State Space Models](https://arxiv.org/abs/2411.11843) (2024-11-18)

- UltraMem
  - Year: 2024
  - Publication: [Ultra-Sparse Memory Network](https://arxiv.org/abs/2411.12364) (2024-11-19)

- Hymba
  - Year: 2024
  - Publication: [Hymba: A Hybrid-head Architecture for Small Language Models](https://arxiv.org/abs/2411.13676) (2024-11-20)
  - Code: [GitHub](https://github.com/NVlabs/hymba)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Hymba-1.5B-Base)

- InstCache
  - Year: 2024
  - Publication: [InstCache: A Predictive Cache for LLM Serving](https://arxiv.org/abs/2411.13820) (2024-11-21)

- Tulu 3
  - Year: 2024
  - Publication: [Tulu 3: Pushing Frontiers in Open Language Model Post-Training](https://arxiv.org/abs/2411.15124) (2024-11-22)
  - Code: [GitHub](https://github.com/allenai/olmes)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B)

- SmolVLM
  - Year: 2024
  - Publication: [HuggingFace model card - SmolVLM - small yet mighty Vision Language Model](https://huggingface.co/blog/smolvlm) (2024-11-24)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceTB/SmolVLM-Instruct)

- Multi-Head Mixture-of-Experts (MH-MoE)
  - Year: 2024
  - Publication: [MH-MoE: Multi-Head Mixture-of-Experts](https://arxiv.org/abs/2411.16205) (2024-11-25)

- LongKey
  - Year: 2024
  - Publication: [LongKey: Keyphrase Extraction for Long Documents](https://arxiv.org/abs/2411.17863) (2024-11-26)
  - Code: [GitHub](https://github.com/jeohalves/longkey)

- Low-Bit Quantization
  - Year: 2024
  - Publication: [Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens](https://arxiv.org/abs/2411.17691) (2024-11-26)

- Star Attention
  - Year: 2024
  - Publication: [Star Attention: Efficient LLM Inference over Long Sequences](https://arxiv.org/abs/2411.17116) (2024-11-26)
  - Code: [GitHub](https://github.com/NVIDIA/Star-Attention)

- DeepThought-8B
  - Year: 2024
  - Publication: [Blog - Introducing DeepThought-8B: A small, capable reasoning model](https://www.ruliad.co/news/introducing-deepthought8b) (2024-11-27)
  - Model weights: [HuggingFace models](https://huggingface.co/ruliad/deepthought-8b-llama-v0.01-alpha)

- QwQ-32B-Preview
  - Year: 2024
  - Publication: [Blog - QwQ: Reflect Deeply on the Boundaries of the Unknown](https://qwenlm.github.io/blog/qwq-32b-preview/) (2024-11-28)
  - Code: [GitHub](https://github.com/QwenLM/Qwen3)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/QwQ-32B-Preview)

- Adaptive Inference of Multi-Modal LLMs (AIM)
  - Year: 2024
  - Publication: [AIM: Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](https://arxiv.org/abs/2412.03248) (2024-12-04)
  - Code: [GitHub](https://github.com/LaVi-Lab/AIM)

- PaliGemma 2
  - Year: 2024
  - Publication: [PaliGemma 2: A Family of Versatile VLMs for Transfer](https://arxiv.org/abs/2412.03555) (2024-12-04)

- InternVL 2.5
  - Year: 2024
  - Publication: [Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](https://arxiv.org/abs/2412.05271) (2024-12-06)
  - Code: [GitHub](https://github.com/OpenGVLab/InternVL)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenGVLab/InternVL2_5-78B)

- Llama 3.3
  - Year: 2024
  - Publication: [Blog - Llama 3.3](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_3/) (2024-12-06)
  - Code: [GitHub](https://github.com/meta-llama/llama-models)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct)

- DeepSeek-V2.5-1210
  - Year: 2024
  - Publication: [Blog - DeepSeek-V2.5-1210](https://api-docs.deepseek.com/news/news1210) (2024-12-10)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-V2)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/DeepSeek-V2.5-1210)

- Granite Guardian
  - Year: 2024
  - Publication: [Granite Guardian](https://arxiv.org/abs/2412.07724) (2024-12-10)
  - Code: [GitHub](https://github.com/ibm-granite/granite-guardian)
  - Model weights: [HuggingFace models](https://huggingface.co/ibm-granite/granite-guardian-3.3-8b)

- Maya
  - Year: 2024
  - Publication: [Maya: An Instruction Finetuned Multilingual Multimodal Model](https://arxiv.org/abs/2412.07112) (2024-12-10)
  - Code: [GitHub](https://github.com/nahidalam/maya)
  - Model weights: [HuggingFace models](https://huggingface.co/maya-multimodal/maya)

- Phi-4
  - Year: 2024
  - Publication: [Phi-4 Technical Report](https://arxiv.org/abs/2412.08905) (2024-12-12)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-4)

- Byte Latent Transformer (BLT)
  - Year: 2024
  - Publication: [Byte Latent Transformer: Patches Scale Better Than Tokens](https://arxiv.org/abs/2412.09871) (2024-12-13)
  - Code: [GitHub](https://github.com/facebookresearch/blt)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/blt-7b)

- Bamba
  - Year: 2024
  - Publication: [Blog - Bamba: A Hybrid Mamba Architecture](https://huggingface.co/blog/bamba) (2024-12-18)
  - Code: [GitHub](https://github.com/foundation-model-stack/bamba)
  - Model weights: [HuggingFace models](https://huggingface.co/ibm-ai-platform/Bamba-9B-v2)

- Granite 3.1
  - Year: 2024
  - Publication: [README - Granite 3.1 language models repository](https://github.com/ibm-granite/granite-3.1-language-models/blob/336bd5550422205f7e2b895151783cea887dd616/README.md) (2024-12-18)
  - Code: [GitHub](https://github.com/ibm-granite/granite-3.1-language-models)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/ibm-granite/granite-31-language-models-6751dbbf2f3389bec5c6f02d)

- ModernBERT
  - Year: 2024
  - Publication: [Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference](https://arxiv.org/abs/2412.13663) (2024-12-18)
  - Code: [GitHub](https://github.com/AnswerDotAI/ModernBERT)
  - Model weights: [HuggingFace models](https://huggingface.co/answerdotai/ModernBERT-base)

- Qwen2.5
  - Year: 2024
  - Publication: [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115) (2024-12-19)
  - Code: [GitHub](https://github.com/QwenLM/Qwen2.5)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen2.5-7B)

- ReMoE
  - Year: 2024
  - Publication: [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing](https://arxiv.org/abs/2412.14711) (2024-12-19)
  - Code: [GitHub](https://github.com/thu-ml/ReMoE)

- Multi-matrix Factorization Attention (MFA)
  - Year: 2024
  - Publication: [Multi-matrix Factorization Attention](https://arxiv.org/abs/2412.19255) (2024-12-26)

- Xmodel-2
  - Year: 2024
  - Publication: [Xmodel-2 Technical Report](https://arxiv.org/abs/2412.19638) (2024-12-27)
  - Code: [GitHub](https://github.com/XiaoduoAILab/Xmodel-2)
  - Model weights: [HuggingFace models](https://huggingface.co/XiaoduoAILab/Xmodel-2)

- INTELLECT-1
  - Year: 2024
  - Publication: [Blog - INTELLECT-1 Release: The First Globally Trained 10B Parameter Model](https://www.primeintellect.ai/blog/intellect-1-release) (2024-12-29)
  - Code: [GitHub](https://github.com/PrimeIntellect-ai/prime)
  - Model weights: [HuggingFace models](https://huggingface.co/PrimeIntellect/INTELLECT-1-Instruct)

- OLMo 2
  - Year: 2024
  - Publication: [2 OLMo 2 Furious](https://arxiv.org/abs/2501.00656) (2024-12-31)
  - Code: [GitHub](https://github.com/allenai/OLMo)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/OLMo-2-1124-7B)

- TensorGRaD
  - Year: 2025
  - Publication: [TensorGRaD: Tensor Gradient Robust Decomposition for Memory-Efficient Neural Operator Training](https://arxiv.org/abs/2501.02379) (2025-01-04)

- Dolphin 3.0 Llama 3.2 1B
  - Year: 2025
  - Publication: [HuggingFace - Dolphin3.0-Llama3.2-1B](https://huggingface.co/cognitivecomputations/Dolphin3.0-Llama3.2-1B) (2025-01-05)
  - Model weights: [HuggingFace models](https://huggingface.co/cognitivecomputations/Dolphin3.0-Llama3.2-1B)

- MixNet
  - Year: 2025
  - Publication: [MixNet: A Runtime Reconfigurable Optical-Electrical Fabric for Distributed Mixture-of-Experts Training](https://arxiv.org/abs/2501.03905) (2025-01-07)

- rStar-Math
  - Year: 2025
  - Publication: [rStar-Math: Small LLMs Can Master Math Reasoning with Self-Evolved Deep Thinking](https://arxiv.org/abs/2501.04519) (2025-01-08)
  - Code: [GitHub](https://github.com/microsoft/rStar)

- Sky-T1-32B-Preview
  - Year: 2025
  - Publication: [Blog - Sky-T1: Train your own O1 preview model within $450](https://novasky-ai.github.io/posts/sky-t1/) (2025-01-10)
  - Code: [GitHub](https://github.com/NovaSky-AI/SkyThought)
  - Model weights: [HuggingFace models](https://huggingface.co/NovaSky-AI/Sky-T1-32B-Preview)

- MiniMax-Text-01 / MiniMax-VL-01
  - Year: 2025
  - Publication: [MiniMax-01: Scaling Foundation Models with Lightning Attention](https://arxiv.org/abs/2501.08313) (2025-01-14)
  - Code: [GitHub](https://github.com/MiniMax-AI/MiniMax-01)

- Eagle 2
  - Year: 2025
  - Publication: [Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models](https://arxiv.org/abs/2501.14818) (2025-01-20)
  - Code: [GitHub](https://github.com/NVlabs/EAGLE)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Eagle2-1B)

- DeepSeek-R1
  - Year: 2025
  - Publication: [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948) (2025-01-22)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-R1)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/DeepSeek-R1)

- Baichuan-Omni-1.5
  - Year: 2025
  - Publication: [Baichuan-Omni-1.5 Technical Report](https://arxiv.org/abs/2501.15368) (2025-01-26)
  - Code: [GitHub](https://github.com/baichuan-inc/Baichuan-Omni-1.5)
  - Model weights: [HuggingFace models](https://huggingface.co/baichuan-inc/Baichuan-Omni-1d5)

- Qwen2.5-1M
  - Year: 2025
  - Publication: [Blog - Qwen2.5-1M](https://qwenlm.github.io/blog/qwen2.5-1m/) (2025-01-27)
  - Code: [GitHub](https://github.com/QwenLM/Qwen3)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen)

- Qwen2.5-Max
  - Year: 2025
  - Publication: [Blog - Qwen2.5-Max](https://qwenlm.github.io/blog/qwen2.5-max/) (2025-01-28)

- Janus-Pro
  - Year: 2025
  - Publication: [Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling](https://arxiv.org/abs/2501.17811) (2025-01-29)
  - Code: [GitHub](https://github.com/deepseek-ai/Janus)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/Janus-Pro-7B)

- Mistral Small 3
  - Year: 2025
  - Publication: [Blog - Mistral Small 3](https://mistral.ai/news/mistral-small-3/) (2025-01-30)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501)

- VaultGemma
  - Year: 2025
  - Publication: [Scaling Laws for Differentially Private Language Models](https://arxiv.org/abs/2501.18914) (2025-01-31)
  - Model weights: [HuggingFace models](https://huggingface.co/google/vaultgemma-1b)

- Self-MoA
  - Year: 2025
  - Publication: [Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?](https://arxiv.org/abs/2502.00674) (2025-02-02)

- SmolLM2
  - Year: 2025
  - Publication: [SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model](https://arxiv.org/abs/2502.02737) (2025-02-04)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B)

- TransMLA
  - Year: 2025
  - Publication: [TransMLA: Multi-Head Latent Attention Is All You Need](https://arxiv.org/abs/2502.07864) (2025-02-11)
  - Code: [GitHub](https://github.com/MuLabPKU/TransMLA)

- Natively trainable Sparse Attention (NSA)
  - Year: 2025
  - Publication: [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](https://arxiv.org/abs/2502.11089) (2025-02-16)

- MoBA
  - Year: 2025
  - Publication: [MoBA: Mixture of Block Attention for Long-Context LLMs](https://arxiv.org/abs/2502.13189) (2025-02-18)
  - Code: [GitHub](https://github.com/MoonshotAI/MoBA)

- R1 1776
  - Year: 2025
  - Publication: [Blog - R1 1776](https://www.perplexity.ai/pl/hub/blog/open-sourcing-r1-1776) (2025-02-18)
  - Model weights: [HuggingFace models](https://huggingface.co/perplexity-ai/r1-1776)

- SYNTHETIC-1
  - Year: 2025
  - Publication: [Blog - SYNTHETIC-1 Release: Two Million Collaboratively Generated Reasoning Traces from Deepseek-R1](https://www.primeintellect.ai/blog/synthetic-1-release) (2025-02-20)
  - Model weights: [HuggingFace models](https://huggingface.co/PrimeIntellect/SYNTHETIC-1-SFT-7B)

- Claude 3.7 Sonnet
  - Year: 2025
  - Publication: [Blog - Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet) (2025-02-24)

- Moonlight
  - Year: 2025
  - Publication: [Muon is Scalable for LLM Training](https://arxiv.org/abs/2502.16982) (2025-02-24)
  - Code: [GitHub](https://github.com/MoonshotAI/Moonlight)
  - Model weights: [HuggingFace models](https://huggingface.co/moonshotai/Moonlight-16B-A3B-Instruct)

- DeepHermes 3 Llama 3 8B
  - Year: 2025
  - Publication: [HuggingFace - DeepHermes-3-Llama-3-8B-Preview](https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview) (2025-02-28)
  - Code: [GitHub](https://github.com/NousResearch/Hermes-Function-Calling)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/DeepHermes-3-Llama-3-8B-Preview)

- FlexPrefill
  - Year: 2025
  - Publication: [FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference](https://arxiv.org/abs/2502.20766) (2025-02-28)
  - Code: [GitHub](https://github.com/ByteDance-Seed/FlexPrefill)

- Babel
  - Year: 2025
  - Publication: [Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers](https://arxiv.org/abs/2503.00865) (2025-03-02)
  - Code: [GitHub](https://github.com/babel-llm/babel-llm)
  - Model weights: [HuggingFace models](https://huggingface.co/Tower-Babel/Babel-9B)

- Forgetting Transformer
  - Year: 2025
  - Publication: [Forgetting Transformer: Softmax Attention with a Forget Gate](https://arxiv.org/abs/2503.02130) (2025-03-03)
  - Code: [GitHub](https://github.com/zhixuan-lin/forgetting-transformer)

- ReaderLM-v2
  - Year: 2025
  - Publication: [ReaderLM-v2: Small Language Model for HTML to Markdown and JSON](https://arxiv.org/abs/2503.01151) (2025-03-03)
  - Code: [GitHub](https://github.com/jina-ai/reader)
  - Model weights: [HuggingFace models](https://huggingface.co/jinaai/ReaderLM-v2)

- Conformal-sympow
  - Year: 2025
  - Publication: [Conformal Transformations for Symmetric Power Transformers](https://arxiv.org/abs/2503.03269) (2025-03-05)

- PowerAttention
  - Year: 2025
  - Publication: [PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention](https://arxiv.org/abs/2503.03588) (2025-03-05)

- Jamba 1.6
  - Year: 2025
  - Publication: [AI21's Jamba 1.6: The Best Open Model for Private Enterprise Deployment](https://www.ai21.com/blog/introducing-jamba-1-6/) (2025-03-06)
  - Model weights: [HuggingFace models](https://huggingface.co/ai21labs)

- L1
  - Year: 2025
  - Publication: [L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning](https://arxiv.org/abs/2503.04697) (2025-03-06)
  - Code: [GitHub](https://github.com/cmu-l3/l1)
  - Model weights: [HuggingFace models](https://huggingface.co/l3lab/L1-Qwen3-8B-Max)

- Symbolic Mixture-of-Experts (Symbolic-MoE)
  - Year: 2025
  - Publication: [Symbolic Mixture-of-Experts: Adaptive Skill-based Routing for Heterogeneous Reasoning](https://arxiv.org/abs/2503.05641) (2025-03-07)
  - Code: [GitHub](https://github.com/dinobby/Symbolic-MoE/)

- OlympicCoder
  - Year: 2025
  - Publication: [Blog - Open-R1 Update #3: Open Reasoning Models for Code and Mathematics](https://huggingface.co/blog/open-r1/update-3) (2025-03-11)
  - Code: [GitHub](https://github.com/huggingface/open-r1)
  - Model weights: [HuggingFace models](https://huggingface.co/open-r1/OlympicCoder-7B)

- Reka Flash 3
  - Year: 2025
  - Publication: [Blog - Reasoning with Reka Flash 3](https://reka.ai/news/introducing-reka-flash) (2025-03-11)
  - Code: [GitHub](https://github.com/reka-ai)
  - Model weights: [HuggingFace models](https://huggingface.co/RekaAI/reka-flash-3)

- Command A
  - Year: 2025
  - Publication: [Blog - Command A](https://cohere.com/blog/command-a) (2025-03-13)
  - Model weights: [HuggingFace models](https://huggingface.co/CohereLabs/c4ai-command-a-03-2025)

- Dynamic Tanh (DyT)
  - Year: 2025
  - Publication: [Transformers without Normalization](https://arxiv.org/abs/2503.10622) (2025-03-13)
  - Code: [GitHub](https://github.com/jiachenzhu/DyT)

- Mistral Small 3.1
  - Year: 2025
  - Publication: [Blog - Mistral Small 3.1](https://mistral.ai/news/mistral-small-3-1) (2025-03-17)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503)

- SuperBPE
  - Year: 2025
  - Publication: [SuperBPE: Space Travel for Language Models](https://arxiv.org/abs/2503.13423) (2025-03-17)
  - Code: [GitHub](https://github.com/PythonNut/superbpe)
  - Model weights: [HuggingFace models](https://huggingface.co/UW/OLMo2-8B-SuperBPE-t180k)

- FFN Fusion
  - Year: 2025
  - Publication: [FFN Fusion: Rethinking Sequential Computation in Large Language Models](https://arxiv.org/abs/2503.18908) (2025-03-24)

- Trajectory Balance with Asynchrony (TBA)
  - Year: 2025
  - Publication: [Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training](https://arxiv.org/abs/2503.18929) (2025-03-24)
  - Code: [GitHub](https://github.com/bbartoldson/TBA)

- DeepSeek-V3-0324
  - Year: 2025
  - Publication: [Blog - DeepSeek-V3-0324 Release](https://api-docs.deepseek.com/news/news250325) (2025-03-25)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-V3)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324)

- Gemma 3
  - Year: 2025
  - Publication: [Gemma 3 Technical Report](https://arxiv.org/abs/2503.19786) (2025-03-25)
  - Model weights: [HuggingFace models](https://huggingface.co/google/gemma-3-27b-it)

- Qwen2.5-Omni
  - Year: 2025
  - Publication: [Blog - Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/) (2025-03-27)
  - Code: [GitHub](https://github.com/QwenLM/Qwen2.5-Omni)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen2.5-Omni-7B)

- TransMamba
  - Year: 2025
  - Publication: [TransMamba: A Sequence-Level Hybrid Transformer-Mamba Language Model](https://arxiv.org/abs/2503.24067) (2025-03-31)
  - Code: [GitHub](https://github.com/Yixing-Li/TransMamba)

- Multi-Token Attention (MTA)
  - Year: 2025
  - Publication: [Multi-Token Attention](https://arxiv.org/abs/2504.00927) (2025-04-01)
  - Code: [GitHub](https://github.com/facebookresearch/RAM)

- ShieldGemma 2
  - Year: 2025
  - Publication: [ShieldGemma 2: Robust and Tractable Image Content Moderation](https://arxiv.org/abs/2504.01081) (2025-04-01)
  - Model weights: [HuggingFace models](https://huggingface.co/google/shieldgemma-2-4b-it)

- MegaScale-Infer
  - Year: 2025
  - Publication: [MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism](https://arxiv.org/abs/2504.02263) (2025-04-03)

- Nemotron-H
  - Year: 2025
  - Publication: [Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models](https://arxiv.org/abs/2504.03624) (2025-04-04)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Nemotron-H-56B-Base-8K)

- Llama 4
  - Year: 2025
  - Publication: [Blog - Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) (2025-04-05)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/meta-llama/llama-4)

- DeepCoder-14B-Preview
  - Year: 2025
  - Publication: [Blog - DeepCoder-14B-Preview](https://www.together.ai/blog/deepcoder) (2025-04-08)
  - Code: [GitHub](https://github.com/agentica-project/rllm)
  - Model weights: [HuggingFace models](https://huggingface.co/agentica-org/DeepCoder-14B-Preview)

- Encoder-Decoder Gemma
  - Year: 2025
  - Publication: [Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation](https://arxiv.org/abs/2504.06225) (2025-04-08)

- Cluster-driven Expert Pruning (C-Prune)
  - Year: 2025
  - Publication: [Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2504.07807) (2025-04-10)
  - Code: [GitHub](https://github.com/Fighoture/MoE_unsupervised_pruning)

- Critical-Layer, Core-Expert, Collaborative Pathway Optimization (C3PO)
  - Year: 2025
  - Publication: [C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization for Test-Time Expert Re-Mixing](https://arxiv.org/abs/2504.07964) (2025-04-10)
  - Code: [GitHub](https://github.com/tianyi-lab/C3PO)

- InternVL 3
  - Year: 2025
  - Publication: [InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models](https://arxiv.org/abs/2504.10479) (2025-04-14)
  - Code: [GitHub](https://github.com/OpenGVLab/InternVL)

- M1
  - Year: 2025
  - Publication: [M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models](https://arxiv.org/abs/2504.10449) (2025-04-14)
  - Code: [GitHub](https://github.com/jxiw/M1)

- Trelawney
  - Year: 2025
  - Publication: [Looking beyond the next token](https://arxiv.org/abs/2504.11336) (2025-04-15)

- Eagle 2.5
  - Year: 2025
  - Publication: [Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models](https://arxiv.org/abs/2504.15271) (2025-04-21)
  - Code: [GitHub](https://github.com/NVlabs/EAGLE)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Eagle2.5-8B)

- Minos
  - Year: 2025
  - Publication: [README - Minos-v1 repository](https://huggingface.co/NousResearch/Minos-v1/blob/edb1e8b7cc80a86cc356534948cca0ef4c46f8e2/README.md) (2025-04-24)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Minos-v1)

- Qwen3
  - Year: 2025
  - Publication: [Blog - Qwen3](https://qwen.ai/blog?id=qwen3) (2025-04-29)
  - Code: [GitHub](https://github.com/QwenLM/Qwen3)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/Qwen/qwen3)

- Llama-Nemotron
  - Year: 2025
  - Publication: [Llama-Nemotron: Efficient Reasoning Models](https://arxiv.org/abs/2505.00949) (2025-05-02)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Llama-3.1-Nemotron-Nano-8B-v1)

- Absolute Zero Reasoner (AZR)
  - Year: 2025
  - Publication: [Absolute Zero: Reinforced Self-play Reasoning with Zero Data](https://arxiv.org/abs/2505.03335) (2025-05-06)
  - Code: [GitHub](https://github.com/LeapLabTHU/Absolute-Zero-Reasoner)
  - Model weights: [HuggingFace models](https://huggingface.co/andrewzh/Absolute_Zero_Reasoner-Coder-7b)

- Mistral Medium 3
  - Year: 2025
  - Publication: [Blog - Mistral Medium 3](https://mistral.ai/news/mistral-medium-3) (2025-05-07)

- INTELLECT-2
  - Year: 2025
  - Publication: [Blog - INTELLECT-2](https://www.primeintellect.ai/blog/intellect-2-release) (2025-05-11)
  - Code: [GitHub](https://github.com/PrimeIntellect-ai/prime-rl)
  - Model weights: [HuggingFace models](https://huggingface.co/PrimeIntellect/INTELLECT-2)

- Seed1.5-VL
  - Year: 2025
  - Publication: [Seed1.5-VL Technical Report](https://arxiv.org/abs/2505.07062) (2025-05-11)
  - Code: [GitHub](https://github.com/ByteDance-Seed/Seed1.5-VL)

- UMoE
  - Year: 2025
  - Publication: [UMoE: Unifying Attention and FFN with Shared Experts](https://arxiv.org/abs/2505.07260) (2025-05-12)
  - Code: [GitHub](https://github.com/ysngki/UMoE)

- DeepSeek-V3/R1
  - Year: 2025
  - Publication: [Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures](https://arxiv.org/abs/2505.09343) (2025-05-14)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/DeepSeek-V3)

- J1
  - Year: 2025
  - Publication: [J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning](https://arxiv.org/abs/2505.10320) (2025-05-15)

- SageAttention 3
  - Year: 2025
  - Publication: [SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](https://arxiv.org/abs/2505.11594) (2025-05-16)
  - Code: [GitHub](https://github.com/thu-ml/SageAttention)

- Model Merging
  - Year: 2025
  - Publication: [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082) (2025-05-17)

- Quartet
  - Year: 2025
  - Publication: [Quartet: Native FP4 Training Can Be Optimal for Large Language Models](https://arxiv.org/abs/2505.14669) (2025-05-20)
  - Code: [GitHub](https://github.com/IST-DASLab/Quartet)

- Devstral
  - Year: 2025
  - Publication: [Blog - Devstral](https://mistral.ai/news/devstral) (2025-05-21)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Devstral-Small-2505)

- Claude Opus 4
  - Year: 2025
  - Publication: [Blog - Claude 4](https://www.anthropic.com/news/claude-4) (2025-05-22)

- QwenLong-L1
  - Year: 2025
  - Publication: [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667) (2025-05-23)
  - Code: [GitHub](https://github.com/Tongyi-Zhiwen/Qwen-Doc)
  - Model weights: [HuggingFace models](https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B)

- SageAttention 2++
  - Year: 2025
  - Publication: [SageAttention2++: A More Efficient Implementation of SageAttention2](https://arxiv.org/abs/2505.21136) (2025-05-27)
  - Code: [GitHub](https://github.com/thu-ml/SageAttention)

- StochasTok
  - Year: 2025
  - Publication: [StochasTok: Improving Fine-Grained Subword Understanding in LLMs](https://arxiv.org/abs/2506.01687) (2025-06-02)
  - Code: [GitHub](https://github.com/anyasims/stochastok)

- Magistral
  - Year: 2025
  - Publication: [Magistral](https://arxiv.org/abs/2506.10910) (2025-06-12)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Magistral-Small-2506)

- AceReason-Nemotron 1.1
  - Year: 2025
  - Publication: [AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy](https://arxiv.org/abs/2506.13284) (2025-06-16)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B)

- MiniMax-M1
  - Year: 2025
  - Publication: [MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention](https://arxiv.org/abs/2506.13585) (2025-06-16)
  - Code: [GitHub](https://github.com/MiniMax-AI/MiniMax-M1)
  - Model weights: [HuggingFace models](https://huggingface.co/MiniMaxAI/MiniMax-M1-80k)

- Autoregressive U-Nets (AU-Net)
  - Year: 2025
  - Publication: [From Bytes to Ideas: Language Modeling with Autoregressive U-Nets](https://arxiv.org/abs/2506.14761) (2025-06-17)

- Mercury Coder
  - Year: 2025
  - Publication: [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298) (2025-06-17)

- MEM1
  - Year: 2025
  - Publication: [MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents](https://arxiv.org/abs/2506.15841) (2025-06-18)
  - Code: [GitHub](https://github.com/MIT-MI/MEM1)
  - Model weights: [HuggingFace models](https://huggingface.co/Mem-Lab/Qwen2.5-7B-RL-RAG-Q2-EM-Release)

- DiffuCoder
  - Year: 2025
  - Publication: [DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation](https://arxiv.org/abs/2506.20639) (2025-06-25)
  - Code: [GitHub](https://github.com/apple/ml-diffucoder)

- Gemma 3n
  - Year: 2025
  - Publication: [Blog - Introducing Gemma 3n](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/) (2025-06-26)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/google/gemma-3n)

- Hierarchical Reasoning Model (HRM)
  - Year: 2025
  - Publication: [Hierarchical Reasoning Model](https://arxiv.org/abs/2506.21734) (2025-06-26)
  - Code: [GitHub](https://github.com/sapientinc/HRM)

- ERNIE 4.5
  - Year: 2025
  - Publication: [Blog - ERNIE 4.5](https://ernie.baidu.com/blog/posts/ernie4.5/) (2025-06-30)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)

- 2-Simplicial Transformer
  - Year: 2025
  - Publication: [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754) (2025-07-03)

- Power Attention
  - Year: 2025
  - Publication: [Scaling Context Requires Rethinking Attention](https://arxiv.org/abs/2507.04239) (2025-07-06)
  - Code: [GitHub](https://github.com/m-a-n-i-f-e-s-t/power-attention)

- EI-BERT
  - Year: 2025
  - Publication: [Put Teacher in Student's Shoes: Cross-Distillation for Ultra-compact Model Compression Framework](https://arxiv.org/abs/2507.04636) (2025-07-07)

- SmolLM3
  - Year: 2025
  - Publication: [Blog - SmolLM3](https://huggingface.co/blog/smollm3) (2025-07-08)
  - Code: [GitHub](https://github.com/huggingface/smollm)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23)

- H-Net
  - Year: 2025
  - Publication: [Dynamic Chunking for End-to-End Hierarchical Sequence Modeling](https://arxiv.org/abs/2507.07955) (2025-07-10)

- SYNTHETIC-2
  - Year: 2025
  - Publication: [Blog - SYNTHETIC-2 Release: Four Million Collaboratively Generated Reasoning Traces](https://www.primeintellect.ai/blog/synthetic-2-release) (2025-07-10)

- Neighborhood Adaptive Block-Level Attention (NABLA)
  - Year: 2025
  - Publication: [$\nabla$NABLA: Neighborhood Adaptive Block-Level Attention](https://arxiv.org/abs/2507.13546) (2025-07-17)
  - Code: [GitHub](https://github.com/gen-ai-team/Wan2.1-NABLA)
  - Model weights: [HuggingFace models](https://huggingface.co/ai-forever/Wan2.1-T2V-14B-NABLA-0.7)

- Group Sequence Policy Optimization (GSPO)
  - Year: 2025
  - Publication: [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071) (2025-07-24)

- Kimi K2
  - Year: 2025
  - Publication: [Kimi K2: Open Agentic Intelligence](https://arxiv.org/abs/2507.20534) (2025-07-28)
  - Code: [GitHub](https://github.com/MoonshotAI/Kimi-K2)
  - Model weights: [HuggingFace models](https://huggingface.co/moonshotai/Kimi-K2-Instruct)

- Falcon-H1
  - Year: 2025
  - Publication: [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance](https://arxiv.org/abs/2507.22448) (2025-07-30)
  - Code: [GitHub](https://github.com/tiiuae/falcon-h1)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/tiiuae/falcon-h1)

- Claude Opus 4.1
  - Year: 2025
  - Publication: [Blog - Claude Opus 4.1](https://www.anthropic.com/news/claude-opus-4-1) (2025-08-05)

- GLM-4.5
  - Year: 2025
  - Publication: [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471) (2025-08-08)
  - Code: [GitHub](https://github.com/zai-org/GLM-4.5)
  - Model weights: [HuggingFace models](https://huggingface.co/zai-org/GLM-4.5)

- gpt-oss-120b / gpt-oss-20b
  - Year: 2025
  - Publication: [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925) (2025-08-08)
  - Model weights: [HuggingFace models](https://huggingface.co/openai/gpt-oss-120b)

- Nemotron Nano 2
  - Year: 2025
  - Publication: [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444) (2025-08-20)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2)

- Seed-OSS-36B
  - Year: 2025
  - Publication: [HuggingFace - Seed-OSS-36B-Instruct](https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct) (2025-08-20)
  - Code: [GitHub](https://github.com/ByteDance-Seed/seed-oss)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/ByteDance-Seed/seed-oss)

- Jet-Nemotron
  - Year: 2025
  - Publication: [Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search](https://www.arxiv.org/abs/2508.15884) (2025-08-21)
  - Code: [GitHub](https://github.com/NVlabs/Jet-Nemotron)
  - Model weights: [HuggingFace models](https://huggingface.co/jet-ai/Jet-Nemotron-4B)

- InternVL 3.5
  - Year: 2025
  - Publication: [InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency](https://arxiv.org/abs/2508.18265) (2025-08-25)
  - Code: [GitHub](https://github.com/OpenGVLab/InternVL)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenGVLab/InternVL3_5-8B)

- R-4B
  - Year: 2025
  - Publication: [R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning](https://arxiv.org/abs/2508.21113) (2025-08-28)
  - Code: [GitHub](https://github.com/yannqi/R-4B)
  - Model weights: [HuggingFace models](https://huggingface.co/YannQi/R-4B)

- Set Block Decoding (SBD)
  - Year: 2025
  - Publication: [Set Block Decoding is a Language Model Inference Accelerator](https://arxiv.org/abs/2509.04185) (2025-09-04)

- Tongyi DeepResearch-30B-A3B
  - Year: 2025
  - Publication: [Blog - Tongyi DeepResearch-30B-A3B](https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/) (2025-09-16)
  - Code: [GitHub](https://github.com/Alibaba-NLP/DeepResearch)
  - Model weights: [HuggingFace models](https://huggingface.co/Alibaba-NLP/Tongyi-DeepResearch-30B-A3B)

- DeepScaleR
  - Year: 2025
  - Publication: [DeepScaleR: Effective RL Scaling of Reasoning Models via Iterative Context Lengthening](https://openreview.net/forum?id=I6GzDCne7U) (2025-09-19)
  - Code: [GitHub](https://github.com/agentica-project/rllm)
  - Model weights: [HuggingFace models](https://huggingface.co/agentica-org/DeepScaleR-1.5B-Preview)

- SPELL
  - Year: 2025
  - Publication: [SPELL: Self-Play Reinforcement Learning for evolving Long-Context Language Models](https://arxiv.org/abs/2509.23863) (2025-09-28)
  - Code: [GitHub](https://github.com/Tongyi-Zhiwen/Qwen-Doc)

- Claude Sonnet 4.5
  - Year: 2025
  - Publication: [Blog - Claude Sonnet 4.5](https://www.anthropic.com/news/claude-sonnet-4-5) (2025-09-29)

- MobileLLM-R1
  - Year: 2025
  - Publication: [MobileLLM-R1: Exploring the Limits of Sub-Billion Language Model Reasoners with Open Training Recipes](https://arxiv.org/abs/2509.24945) (2025-09-29)
  - Code: [GitHub](https://github.com/facebookresearch/MobileLLM-R1)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/MobileLLM-R1-950M)

- Tiny Recursive Model (TRM)
  - Year: 2025
  - Publication: [Less is More: Recursive Reasoning with Tiny Networks](https://arxiv.org/abs/2510.04871) (2025-10-06)

- Jamba Reasoning 3B
  - Year: 2025
  - Publication: [Introducing Jamba Reasoning 3B: Tiny Model, Huge Possibilities](https://www.ai21.com/blog/introducing-jamba-reasoning-3b/) (2025-10-08)
  - Model weights: [HuggingFace models](https://huggingface.co/ai21labs/AI21-Jamba-Reasoning-3B)

- Dr.LLM
  - Year: 2025
  - Publication: [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773) (2025-10-14)

- Claude Haiku 4.5
  - Year: 2025
  - Publication: [Blog - Claude Haiku 4.5](https://www.anthropic.com/news/claude-haiku-4-5) (2025-10-15)

- Elastic-Cache
  - Year: 2025
  - Publication: [Attention Is All You Need for KV Cache in Diffusion LLMs](https://arxiv.org/abs/2510.14973) (2025-10-16)

- SWE-1.5
  - Year: 2025
  - Publication: [Blog - SWE-1.5](https://cognition.ai/blog/swe-1-5) (2025-10-29)

- ZAYA1
  - Year: 2025
  - Publication: [Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design](https://arxiv.org/abs/2511.17127) (2025-11-21)
  - Model weights: [HuggingFace models](https://huggingface.co/Zyphra/ZAYA1-base)

- Xmodel-2.5
  - Year: 2025
  - Publication: [Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM](https://arxiv.org/abs/2511.19496) (2025-11-23)
  - Code: [GitHub](https://github.com/XiaoduoAILab/Xmodel-2.5)
  - Model weights: [HuggingFace models](https://huggingface.co/XiaoduoAILab/Xmodel-2.5)

- Claude Opus 4.5
  - Year: 2025
  - Publication: [Blog - Claude Opus 4.5](https://www.anthropic.com/news/claude-opus-4-5) (2025-11-24)

- MobileLLM-R1.5
  - Year: 2025
  - Publication: [HuggingFace - MobileLLM-R1.5-950M](https://huggingface.co/facebook/MobileLLM-R1.5-950M) (2025-11-24)
  - Code: [GitHub](https://github.com/facebookresearch/MobileLLM-R1)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/MobileLLM-R1.5-950M)

- INTELLECT-3
  - Year: 2025
  - Publication: [Blog - INTELLECT-3: A 100B+ MoE trained with large-scale RL](https://www.primeintellect.ai/blog/intellect-3) (2025-11-26)

- Rnj-1
  - Year: 2025
  - Publication: [Blog - Rnj-1](https://www.essential.ai/research/rnj-1) (2025-12-05)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/EssentialAI/rnj-1)

- Nanbeige4
  - Year: 2025
  - Publication: [Nanbeige4-3B Technical Report: Exploring the Frontier of Small Language Models](https://arxiv.org/abs/2512.06266) (2025-12-06)
  - Model weights: [HuggingFace models](https://huggingface.co/Nanbeige/Nanbeige4-3B-Base)

- DeepCode
  - Year: 2025
  - Publication: [DeepCode: Open Agentic Coding](https://arxiv.org/abs/2512.07921) (2025-12-08)
  - Code: [GitHub](https://github.com/HKUDS/DeepCode)

- VulnLLM-R
  - Year: 2025
  - Publication: [VulnLLM-R: Specialized Reasoning LLM with Agent Scaffold for Vulnerability Detection](https://arxiv.org/abs/2512.07533) (2025-12-08)
  - Code: [GitHub](https://github.com/ucsb-mlsec/VulnLLM-R)
  - Model weights: [HuggingFace models](https://huggingface.co/UCSB-SURFI/VulnLLM-R-7B)

- Confucius Code Agent
  - Year: 2025
  - Publication: [Confucius Code Agent: Scalable Agent Scaffolding for Real-World Codebases](https://arxiv.org/abs/2512.10398) (2025-12-11)
  - Code: [GitHub](https://github.com/luby72/confucius)

- DroPE
  - Year: 2025
  - Publication: [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167) (2025-12-13)
  - Code: [GitHub](https://github.com/SakanaAI/DroPE)
  - Model weights: [HuggingFace models](https://huggingface.co/SakanaAI/Llama-2-7b-hf-DroPE)

- Nemotron-Cascade
  - Year: 2025
  - Publication: [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607) (2025-12-15)
  - Code: [GitHub](https://github.com/NVIDIA-NeMo/Nemotron)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/Nemotron-Cascade-8B)

- Olmo 3
  - Year: 2025
  - Publication: [Olmo 3](https://arxiv.org/abs/2512.13961) (2025-12-15)
  - Code: [GitHub](https://github.com/allenai/open-instruct/)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/Olmo-3-1125-32B)

- QwenLong-L1.5
  - Year: 2025
  - Publication: [QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management](https://arxiv.org/abs/2512.12967) (2025-12-15)
  - Code: [GitHub](https://github.com/Tongyi-Zhiwen/Qwen-Doc)
  - Model weights: [HuggingFace models](https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B)

- MiMo-V2-Flash
  - Year: 2025
  - Publication: [Blog - MiMo-V2-Flash](https://mimo.xiaomi.com/blog/mimo-v2-flash) (2025-12-16)
  - Code: [GitHub](https://github.com/XiaomiMiMo/MiMo-V2-Flash)
  - Model weights: [HuggingFace models](https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash)

- T5Gemma 2
  - Year: 2025
  - Publication: [T5Gemma 2: Seeing, Reading, and Understanding Longer](https://arxiv.org/abs/2512.14856) (2025-12-16)

- Bolmo
  - Year: 2025
  - Publication: [Bolmo: Byteifying the Next Generation of Language Models](https://arxiv.org/abs/2512.15586) (2025-12-17)
  - Code: [GitHub](https://github.com/allenai/bolmo-core)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/Bolmo-7B)

- GLM-4.7
  - Year: 2025
  - Publication: [Blog - GLM-4.7](https://z.ai/blog/glm-4.7) (2025-12-22)
  - Code: [GitHub](https://github.com/zai-org/GLM-4.7)
  - Model weights: [HuggingFace models](https://huggingface.co/zai-org/GLM-4.7)

- Recursive Language Models (RLM)
  - Year: 2025
  - Publication: [Recursive Language Models](https://arxiv.org/abs/2512.24601) (2025-12-31)
  - Code: [GitHub](https://github.com/alexzhang13/rlm)

- Falcon-H1R
  - Year: 2026
  - Publication: [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346) (2026-01-05)
  - Code: [GitHub](https://github.com/tiiuae/falcon-h1r)
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/Falcon-H1R-7B)

- NousCoder-14B
  - Year: 2026
  - Publication: [Blog - NousCoder-14B](https://nousresearch.com/nouscoder-14b-a-competitive-olympiad-programming-model) (2026-01-06)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/NousCoder-14B)

- GDPO
  - Year: 2026
  - Publication: [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242) (2026-01-08)
  - Code: [GitHub](https://github.com/NVlabs/GDPO)

- Jamba 2
  - Year: 2026
  - Publication: [Introducing Jamba2: The open source model family for enterprise reliability and efficiency](https://www.ai21.com/blog/introducing-jamba2/) (2026-01-08)
  - Model weights: [HuggingFace models](https://huggingface.co/ai21labs/AI21-Jamba2-3B)

- Ministral 3
  - Year: 2026
  - Publication: [Ministral 3](https://arxiv.org/abs/2601.08584) (2026-01-13)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/mistralai/ministral-3)

- DASD-4B-Thinking
  - Year: 2026
  - Publication: [Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning](https://arxiv.org/abs/2601.09088) (2026-01-14)
  - Code: [GitHub](https://github.com/D2I-ai/dasd-thinking)
  - Model weights: [HuggingFace models](https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking)

- Kimi K2.5
  - Year: 2026
  - Publication: [Blog - Kimi K2.5](https://www.kimi.com/blog/kimi-k2-5.html) (2026-01-27)
  - Model weights: [HuggingFace models](https://huggingface.co/moonshotai/Kimi-K2.5)

- GLM-5
  - Year: 2026
  - Publication: [Blog - GLM-5](https://z.ai/blog/glm-5) (2026-02-11)
  - Code: [GitHub](https://github.com/zai-org/GLM-5)
  - Model weights: [HuggingFace models](https://huggingface.co/collections/zai-org/glm-5)
