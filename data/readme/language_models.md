- GloVe
  - Year: 2014
  - Publication: [Blog - GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162/) (2014-10-25)

- fastText
  - Year: 2016
  - Publication: [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606) (2016-07-15)
  - Code: [GitHub](https://github.com/facebookresearch/fastText)

- MoE
  - Year: 2017
  - Publication: [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538) (2017-01-23)

- Transformer
  - Year: 2017
  - Publication: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (2017-06-12)

- RR-Transformer
  - Year: 2018
  - Publication: [Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155) (2018-03-06)

- GPT
  - Year: 2018
  - Publication: [Blog - Improving Language Understanding with Unsupervised Learning](https://openai.com/blog/language-unsupervised) (2018-06-11)
  - Model weights: [HuggingFace models](https://huggingface.co/openai-gpt)

- flair
  - Year: 2018
  - Publication: [Contextual String Embeddings for Sequence Labeling](https://aclanthology.org/C18-1139) (2018-08-31)
  - Code: [GitHub](https://github.com/flairNLP/flair)

- BERT
  - Year: 2018
  - Publication: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) (2018-10-11)

- Transformer-XL
  - Year: 2019
  - Publication: [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) (2019-01-09)
  - Code: [GitHub](https://github.com/kimiyoung/transformer-xl)

- LightConv/DynamicConv
  - Year: 2019
  - Publication: [Pay Less Attention with Lightweight and Dynamic Convolutions](https://arxiv.org/abs/1901.10430) (2019-01-29)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/pay_less_attention_paper)

- Evolved Transformer
  - Year: 2019
  - Publication: [The Evolved Transformer](https://arxiv.org/abs/1901.11117) (2019-01-30)

- GPT-2
  - Year: 2019
  - Publication: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (2019-02-14)
  - Code: [GitHub](https://github.com/openai/gpt-2)

- ERNIE
  - Year: 2019
  - Publication: [ERNIE: Enhanced Representation through Knowledge Integration](https://arxiv.org/abs/1904.09223) (2019-04-19)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)
  - Model weights: [HuggingFace models](https://huggingface.co/PaddlePaddle/ernie-1.0-base-zh)

- Sparse Transformer
  - Year: 2019
  - Publication: [Generating Long Sequences with Sparse Transformers](https://arxiv.org/abs/1904.10509) (2019-04-23)

- Adaptive Span
  - Year: 2019
  - Publication: [Adaptive Attention Span in Transformers](https://arxiv.org/abs/1905.07799) (2019-05-19)
  - Code: [GitHub](https://github.com/facebookresearch/adaptive-span)

- XLNet
  - Year: 2019
  - Publication: [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237) (2019-06-19)

- All-attention network
  - Year: 2019
  - Publication: [Augmenting Self-attention with Persistent Memory](https://arxiv.org/abs/1907.01470) (2019-07-02)

- RoBERTa
  - Year: 2019
  - Publication: [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692) (2019-07-26)
  - Code: [GitHub](https://github.com/pytorch/fairseq/tree/master/examples/roberta)

- ERNIE 2.0
  - Year: 2019
  - Publication: [ERNIE 2.0: A Continual Pre-training Framework for Language Understanding](https://arxiv.org/abs/1907.12412) (2019-07-29)
  - Code: [GitHub](https://github.com/PaddlePaddle/ERNIE)
  - Model weights: [HuggingFace models](https://huggingface.co/PaddlePaddle/ernie-2.0-base-en)

- StructBERT
  - Year: 2019
  - Publication: [StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577) (2019-08-13)

- Adaptively Sparse Transformers
  - Year: 2019
  - Publication: [Adaptively Sparse Transformers](https://arxiv.org/abs/1909.00015) (2019-08-30)

- CTRL
  - Year: 2019
  - Publication: [CTRL: A Conditional Transformer Language Model for Controllable Generation](https://arxiv.org/abs/1909.05858) (2019-09-11)
  - Code: [GitHub](https://github.com/salesforce/ctrl)

- Megatron-LM
  - Year: 2019
  - Publication: [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) (2019-09-17)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- ALBERT
  - Year: 2019
  - Publication: [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942) (2019-09-26)
  - Code: [GitHub](https://github.com/google-research/ALBERT)

- kNN-LM
  - Year: 2019
  - Publication: [Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/abs/1911.00172) (2019-11-01)
  - Code: [GitHub](https://github.com/urvashik/knnlm)

- TENER
  - Year: 2019
  - Publication: [TENER: Adapting Transformer Encoder for Named Entity Recognition](https://arxiv.org/abs/1911.04474) (2019-11-10)

- Sandwich Transformers
  - Year: 2019
  - Publication: [https://arxiv.org/abs/1911.03864](https://arxiv.org/abs/1911.03864) (2019-11-10)
  - Video: [YouTube](https://www.youtube.com/watch?v=rFuuGEj3AhU)
  - Code: [GitHub](https://github.com/ofirpress/sandwich_transformer)

- BiT (Big Transfer)
  - Year: 2019
  - Publication: [Big Transfer (BiT): General Visual Representation Learning](https://arxiv.org/abs/1912.11370) (2019-12-24)

- Feedback Transformer
  - Year: 2020
  - Publication: [Addressing Some Limitations of Transformers with Feedback Memory](https://arxiv.org/abs/2002.09402) (2020-02-21)

- MiniLM
  - Year: 2020
  - Publication: [MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers](https://arxiv.org/abs/2002.10957) (2020-02-25)

- Routing Transformer
  - Year: 2020
  - Publication: [Efficient Content-Based Sparse Attention with Routing Transformers](https://arxiv.org/abs/2003.05997) (2020-03-12)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/routing_transformer)

- Floater
  - Year: 2020
  - Publication: [Learning to Encode Position for Transformer with Continuous Dynamical Model](https://arxiv.org/abs/2003.09229) (2020-03-13)
  - Code: [GitHub](https://github.com/xuanqing94/FLOATER)

- MobileBERT
  - Year: 2020
  - Publication: [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984) (2020-04-06)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/mobilebert)

- Poor Man's BERT
  - Year: 2020
  - Publication: [On the Effect of Dropping Layers of Pre-trained Transformer Models](https://arxiv.org/abs/2004.03844) (2020-04-08)
  - Code: [GitHub](https://github.com/hsajjad/transformers)

- Longformer
  - Year: 2020
  - Publication: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) (2020-04-10)
  - Code: [GitHub](https://github.com/allenai/longformer)

- MPNet
  - Year: 2020
  - Publication: [MPNet: Masked and Permuted Pre-training for Language Understanding](https://arxiv.org/abs/2004.09297) (2020-04-20)
  - Code: [GitHub](https://github.com/microsoft/MPNet)

- DeeBERT
  - Year: 2020
  - Publication: [DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference](https://arxiv.org/abs/2004.12993) (2020-04-27)
  - Code: [GitHub](https://github.com/castorini/DeeBERT)

- ColBERT
  - Year: 2020
  - Publication: [ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT](https://arxiv.org/abs/2004.12832) (2020-04-27)

- Synthesizer
  - Year: 2020
  - Publication: [Synthesizer: Rethinking Self-Attention in Transformer Models](https://arxiv.org/abs/2005.00743) (2020-05-02)
  - Code: [GitHub](https://github.com/leaderj1001/Synthesizer-Rethinking-Self-Attention-Transformer-Models) / [GitHub](https://github.com/10-zin/Synthesizer)

- Adaptive Transformers
  - Year: 2020
  - Publication: [Adaptive Transformers for Learning Multimodal Representations](https://arxiv.org/abs/2005.07486) (2020-05-15)
  - Code: [GitHub](https://github.com/prajjwal1/adaptive_transformer)

- DeBERTa
  - Year: 2020
  - Publication: [DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654) (2020-06-05)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-base)

- Linformer
  - Year: 2020
  - Publication: [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/abs/2006.04768) (2020-06-08)
  - Code: [GitHub](https://github.com/Kyan820815/Linformer)

- BRC
  - Year: 2020
  - Publication: [A bio-inspired bistable recurrent cell allows for long-lasting memory](https://arxiv.org/abs/2006.05252) (2020-06-09)
  - Code: [GitHub](https://github.com/nvecoven/BRC) / [GitHub](https://github.com/742617000027/nBRC)

- TUPE
  - Year: 2020
  - Publication: [Rethinking Positional Encoding in Language Pre-training](https://arxiv.org/abs/2006.15595) (2020-06-28)
  - Code: [GitHub](https://github.com/guolinke/TUPE)

- DeLighT
  - Year: 2020
  - Publication: [DeLighT: Deep and Light-weight Transformer](https://arxiv.org/abs/2008.00623) (2020-08-03)
  - Code: [GitHub](https://github.com/sacmehta/delight)

- DA-Transformer
  - Year: 2020
  - Publication: [DA-Transformer: Distance-aware Transformer](https://arxiv.org/abs/2010.06925) (2020-10-14)

- CharacterBERT
  - Year: 2020
  - Publication: [CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters](https://arxiv.org/abs/2010.10392) (2020-10-20)
  - Code: [GitHub](https://github.com/helboukkouri/character-bert)

- Informer
  - Year: 2020
  - Publication: [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting](https://arxiv.org/abs/2012.07436) (2020-12-14)

- Diff pruning
  - Year: 2020
  - Publication: [Parameter-Efficient Transfer Learning with Diff Pruning](https://arxiv.org/abs/2012.07463) (2020-12-14)
  - Code: [GitHub](https://github.com/dguo98/DiffPruning)

- Key-Value Memory Feed-Forward
  - Year: 2020
  - Publication: [Transformer Feed-Forward Layers Are Key-Value Memories](https://arxiv.org/abs/2012.14913) (2020-12-29)

- Shortformer
  - Year: 2020
  - Publication: [Shortformer: Better Language Modeling using Shorter Inputs](https://arxiv.org/abs/2012.15832) (2020-12-31)
  - Code: [GitHub](https://github.com/ofirpress/shortformer)

- MiniLM v2
  - Year: 2020
  - Publication: [MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers](https://arxiv.org/abs/2012.15828) (2020-12-31)

- DeBERTaV2
  - Year: 2021
  - Publication: [Blog - Microsoft DeBERTa surpasses human performance on the SuperGLUE benchmark](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/) (2021-01-06)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-v2-xlarge)

- Switch Transformer
  - Year: 2021
  - Publication: [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) (2021-01-11)
  - Code: [GitHub](https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py)
  - Model weights: [HuggingFace models](https://huggingface.co/google/switch-c-2048)

- WuDao
  - Year: 2021
  - Publication: [Article: Five Key Facts Wu Dao 2.0: The Largest Transformer Model Ever Built](https://medium.com/dataseries/five-key-facts-wu-dao-2-0-the-largest-transformer-model-ever-built-19316159796b) (2021-01-11)

- UniT
  - Year: 2021
  - Publication: [UniT: Multimodal Multitask Learning with a Unified Transformer](https://arxiv.org/abs/2102.10772) (2021-02-22)
  - Code: [GitHub](https://github.com/facebookresearch/mmf)

- PLBART
  - Year: 2021
  - Publication: [Unified Pre-training for Program Understanding and Generation](https://arxiv.org/abs/2103.06333) (2021-03-10)
  - Code: [GitHub](https://github.com/wasiahmad/PLBART)
  - Model weights: [Direct link](https://github.com/wasiahmad/PLBART/tree/9bf5e12bb7374218c21546aa31020ef102a151e7#fine-tuning)

- GLM
  - Year: 2021
  - Publication: [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://arxiv.org/abs/2103.10360) (2021-03-18)
  - Code: [GitHub](https://github.com/THUDM/GLM)
  - Model weights: [HuggingFace models](https://huggingface.co/BAAI/glm-10b)

- Megatron-LM v2
  - Year: 2021
  - Publication: [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) (2021-04-09)
  - Code: [GitHub](https://github.com/NVIDIA/Megatron-LM)

- 24hBERT - Academic Budget BERT
  - Year: 2021
  - Publication: [How to Train BERT with an Academic Budget](https://arxiv.org/abs/2104.07705) (2021-04-15)
  - Code: [GitHub](https://github.com/IntelLabs/academic-budget-bert)

- RoFormer
  - Year: 2021
  - Publication: [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) (2021-04-20)
  - Code: [GitHub](https://github.com/ZhuiyiTechnology/roformer)
  - Model weights: [HuggingFace models](https://huggingface.co/junnyu/roformer_chinese_base)

- PanGu
  - Year: 2021
  - Publication: [PanGu: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation](https://arxiv.org/abs/2104.12369) (2021-04-26)

- FNet
  - Year: 2021
  - Publication: [FNet: Mixing Tokens with Fourier Transforms](https://arxiv.org/abs/2105.03824) (2021-05-09)
  - Video: [YouTube](https://www.youtube.com/watch?v=JJR3pBl78zw)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/f_net)

- EL-Attention
  - Year: 2021
  - Publication: [EL-Attention: Memory Efficient Lossless Attention for Generation](https://arxiv.org/abs/2105.04779) (2021-05-11)
  - Code: [GitHub](https://github.com/microsoft/fastseq)

- ByT5
  - Year: 2021
  - Publication: [ByT5: Towards a token-free future with pre-trained byte-to-byte models](https://arxiv.org/abs/2105.13626) (2021-05-28)
  - Code: [GitHub](https://github.com/google-research/byt5)

- Luna
  - Year: 2021
  - Publication: [Luna: Linear Unified Nested Attention](https://arxiv.org/abs/2106.01540) (2021-06-03)
  - Code: [GitHub](https://github.com/XuezheMax/fairseq-apollo)

- GPT-J
  - Year: 2021
  - Publication: [Blog - GPT-J](https://www.eleuther.ai/artifacts/gpt-j) (2021-06-04)
  - Code: [GitHub](https://github.com/kingoflolz/mesh-transformer-jax/)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/gpt-j-6b)

- LoRA
  - Year: 2021
  - Publication: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) (2021-06-17)
  - Code: [GitHub](https://github.com/microsoft/LoRA)

- BitFit
  - Year: 2021
  - Publication: [BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199) (2021-06-18)
  - Code: [GitHub](https://github.com/benzakenelad/BitFit)

- Charformer
  - Year: 2021
  - Publication: [Charformer: Fast Character Transformers via Gradient-based Subword Tokenization](https://arxiv.org/abs/2106.12672) (2021-06-23)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/charformer)

- ERNIE 3.0
  - Year: 2021
  - Publication: [ERNIE 3.0: Large-scale Knowledge Enhanced Pre-training for Language Understanding and Generation](https://arxiv.org/abs/2107.02137) (2021-07-05)

- ALiBi
  - Year: 2021
  - Publication: [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409) (2021-08-27)
  - Code: [GitHub](https://github.com/ofirpress/attention_with_linear_biases)
  - Model weights: [Private page](https://github.com/ofirpress/attention_with_linear_biases/tree/master/examples/language_model)

- CodeT5
  - Year: 2021
  - Publication: [CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation](https://arxiv.org/abs/2109.00859) (2021-09-02)
  - Code: [GitHub](https://github.com/salesforce/CodeT5)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codet5-base)

- HyperCLOVA
  - Year: 2021
  - Publication: [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://arxiv.org/abs/2109.04650) (2021-09-10)

- Jurassic-1
  - Year: 2021
  - Publication: [Jurassic-1: Technical Details And Evaluation](https://assets.website-files.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf) (2021-09-11)
  - Code: [GitHub](https://github.com/ai21labs/lm-evaluation)

- Primer
  - Year: 2021
  - Publication: [Primer: Searching for Efficient Transformers for Language Modeling](https://arxiv.org/abs/2109.08668) (2021-09-17)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/primer)

- PLATO-XL
  - Year: 2021
  - Publication: [PLATO-XL: Exploring the Large-scale Pre-training of Dialogue Generation](https://arxiv.org/abs/2109.09519) (2021-09-20)

- T0
  - Year: 2021
  - Publication: [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207) (2021-10-15)
  - Code: [GitHub](https://github.com/bigscience-workshop/t-zero)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/T0)

- NormFormer
  - Year: 2021
  - Publication: [NormFormer: Improved Transformer Pretraining with Extra Normalization](https://arxiv.org/abs/2110.09456) (2021-10-18)

- DeBERTaV3
  - Year: 2021
  - Publication: [DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing](https://arxiv.org/abs/2111.09543) (2021-11-18)
  - Code: [GitHub](https://github.com/microsoft/DeBERTa)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/deberta-v3-base)

- Gopher
  - Year: 2021
  - Publication: [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/abs/2112.11446) (2021-12-08)

- Memory efficient attention
  - Year: 2021
  - Publication: [Self-attention Does Not Need O(n2) Memory](https://arxiv.org/abs/2112.05682) (2021-12-10)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/memory_efficient_attention)

- XGLM
  - Year: 2021
  - Publication: [Few-shot Learning with Multilingual Language Models](https://arxiv.org/abs/2112.10668) (2021-12-20)
  - Code: [GitHub](https://github.com/facebookresearch/fairseq/tree/main/examples/xglm)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/xglm-7.5B)

- LaMDA
  - Year: 2022
  - Publication: [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239) (2022-01-20)

- Megatron-Turing NLG
  - Year: 2022
  - Publication: [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990) (2022-01-28)

- Memorizing Transformers, MemTRM
  - Year: 2022
  - Publication: [Memorizing Transformers](https://arxiv.org/abs/2203.08913) (2022-03-16)

- CodeGen
  - Year: 2022
  - Publication: [CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis](https://arxiv.org/abs/2203.13474) (2022-03-25)
  - Code: [GitHub](https://github.com/salesforce/CodeGen)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen-16B-multi)

- BaGuaLu
  - Year: 2022
  - Publication: [BaGuaLu: targeting brain scale pretrained models with over 37 million cores](https://dl.acm.org/doi/abs/10.1145/3503221.3508417) (2022-03-28)

- Chinchilla
  - Year: 2022
  - Publication: [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556) (2022-03-29)

- NoPos
  - Year: 2022
  - Publication: [Transformer Language Models without Positional Encodings Still Learn Positional Information](https://arxiv.org/abs/2203.16634) (2022-03-30)
  - Code: [GitHub](https://github.com/adihaviv/NoPos)

- PaLM
  - Year: 2022
  - Publication: [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311) (2022-04-05)

- GPT-NeoX
  - Year: 2022
  - Publication: [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) (2022-04-14)
  - Code: [GitHub](https://github.com/EleutherAI/gpt-neox)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/gpt-neox-20b)

- Jurassic-X
  - Year: 2022
  - Publication: [Blog - Jurassic-X: Crossing the neuro-symbolic chasm with the MRKL system](https://www.ai21.com/blog/jurassic-x-crossing-the-neuro-symbolic-chasm-with-the-mrkl-system) (2022-04-20)

- OPT
  - Year: 2022
  - Publication: [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068) (2022-05-02)
  - Code: [GitHub](https://github.com/facebookresearch/metaseq)

- UL2
  - Year: 2022
  - Publication: [UL2: Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131) (2022-05-10)
  - Code: [GitHub](https://github.com/google-research/google-research/tree/master/ul2)

- FlashAttention
  - Year: 2022
  - Publication: [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135) (2022-05-27)
  - Code: [GitHub](https://github.com/HazyResearch/flash-attention)

- YaLM
  - Year: 2022
  - Publication: [Blog - Yandex Publishes YaLM 100B. Itâ€™s the Largest GPT-Like Neural Network in Open Source](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6) (2022-06-23)

- Minerva
  - Year: 2022
  - Publication: [Solving Quantitative Reasoning Problems with Language Models](https://arxiv.org/abs/2206.14858) (2022-06-29)

- CodeRL
  - Year: 2022
  - Publication: [CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning](https://arxiv.org/abs/2207.01780) (2022-07-05)
  - Code: [GitHub](https://github.com/salesforce/CodeRL)

- CALM
  - Year: 2022
  - Publication: [Confident Adaptive Language Modeling](https://arxiv.org/abs/2207.07061) (2022-07-14)
  - Code: [GitHub](https://github.com/google-research/t5x)

- LLM.int8
  - Year: 2022
  - Publication: [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339) (2022-08-15)
  - Code: [GitHub](https://github.com/TimDettmers/bitsandbytes)

- Petals
  - Year: 2022
  - Publication: [Petals: Collaborative Inference and Fine-tuning of Large Models](https://arxiv.org/abs/2209.01188) (2022-09-02)
  - Code: [GitHub](https://github.com/bigscience-workshop/petals)

- NeMo Megatron
  - Year: 2022
  - Publication: [HuggingFace model card - NeMo Megatron](https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B) (2022-09-10)
  - Model weights: [HuggingFace models](https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B)

- GLM-130B
  - Year: 2022
  - Publication: [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/abs/2210.02414) (2022-10-05)
  - Code: [GitHub](https://github.com/THUDM/GLM-130B)
  - Model weights: [HuggingFace models](https://huggingface.co/spaces/THUDM/GLM-130B)

- Magneto
  - Year: 2022
  - Publication: [Foundation Transformers](https://arxiv.org/abs/2210.06423) (2022-10-12)
  - Code: [GitHub](https://github.com/sunyt32/torchscale)

- TransNormer
  - Year: 2022
  - Publication: [The Devil in Linear Transformer](https://arxiv.org/abs/2210.10340) (2022-10-19)
  - Code: [GitHub](https://github.com/OpenNLPLab/Transnormer)

- FlanT5
  - Year: 2022
  - Publication: [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416) (2022-10-20)
  - Code: [GitHub](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints)
  - Model weights: [HuggingFace models](https://huggingface.co/google/flan-t5-base)

- OLLA
  - Year: 2022
  - Publication: [OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks](https://arxiv.org/abs/2210.12924) (2022-10-24)

- BLOOMZ & mT0
  - Year: 2022
  - Publication: [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/abs/2211.01786) (2022-11-03)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloomz)

- BLOOM
  - Year: 2022
  - Publication: [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100) (2022-11-09)
  - Model weights: [HuggingFace models](https://huggingface.co/bigscience/bloom)

- Galactica
  - Year: 2022
  - Publication: [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085) (2022-11-16)
  - Code: [GitHub](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/galactica-6.7b)

- MegaBlocks
  - Year: 2022
  - Publication: [MegaBlocks: Efficient Sparse Training with Mixture-of-Experts](https://arxiv.org/abs/2211.15841) (2022-11-29)
  - Code: [GitHub](https://github.com/stanford-futuredata/megablocks)

- NPM
  - Year: 2022
  - Publication: [Nonparametric Masked Language Modeling](https://arxiv.org/abs/2212.01349) (2022-12-02)
  - Code: [GitHub](https://github.com/facebookresearch/NPM)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/npm)

- BioMedLM
  - Year: 2022
  - Publication: [Blog - BioMedLM: a Domain-Specific Large Language Model for Biomedical Text](https://www.mosaicml.com/blog/introducing-pubmed-gpt) (2022-12-15)
  - Code: [GitHub](https://github.com/stanford-crfm/BioMedLM)
  - Model weights: [HuggingFace models](https://huggingface.co/stanford-crfm/BioMedLM)

- LeX-Transformer
  - Year: 2022
  - Publication: [A Length-Extrapolatable Transformer](https://arxiv.org/abs/2212.10554) (2022-12-20)
  - Code: [GitHub](https://github.com/sunyt32/torchscale)

- OPT-IML
  - Year: 2022
  - Publication: [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/abs/2212.12017) (2022-12-22)
  - Model weights: [HuggingFace models](https://huggingface.co/facebook/opt-iml-30b)

- Cramming
  - Year: 2022
  - Publication: [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034) (2022-12-28)
  - Code: [GitHub](https://github.com/JonasGeiping/cramming)

- StitchNet
  - Year: 2023
  - Publication: [StitchNet: Composing Neural Networks from Pre-Trained Fragments](https://arxiv.org/abs/2301.01947) (2023-01-05)
  - Code: [GitHub](https://github.com/steerapi/stitchnet)

- Palmyra
  - Year: 2023
  - Publication: [Blog - Palmyra LLMs empower secure, enterprise-grade generative AI for business](https://writer.com/blog/palmyra/) (2023-01-06)
  - Model weights: [HuggingFace models](https://huggingface.co/Writer/palmyra-large)

- SantaCoder
  - Year: 2023
  - Publication: [SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) (2023-01-09)
  - Code: [GitHub](https://github.com/bigcode-project/Megatron-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/santacoder)

- NarrowBERT
  - Year: 2023
  - Publication: [NarrowBERT: Accelerating Masked Language Model Pretraining and Inference](https://arxiv.org/abs/2301.04761) (2023-01-11)

- Diff-Codegen
  - Year: 2023
  - Publication: [Blog - Diff Models - A New Way to Edit Code](https://carper.ai/diff-models-a-new-way-to-edit-code/) (2023-01-27)
  - Code: [GitHub](https://github.com/salesforce/CodeGen)
  - Model weights: [HuggingFace models](https://huggingface.co/CarperAI/diff-codegen-6b-v2)

- AltUp
  - Year: 2023
  - Publication: [Alternating Updates for Efficient Transformers](https://arxiv.org/abs/2301.13310) (2023-01-30)

- E-SPA (Exponential Signal Preserving Attention)
  - Year: 2023
  - Publication: [Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation](https://arxiv.org/abs/2302.10322) (2023-02-20)

- Hyena
  - Year: 2023
  - Publication: [Hyena Hierarchy: Towards Larger Convolutional Language Models](https://arxiv.org/abs/2302.10866) (2023-02-21)
  - Code: [GitHub](https://github.com/HazyResearch/safari)

- SpikeGPT
  - Year: 2023
  - Publication: [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks](https://arxiv.org/abs/2302.13939) (2023-02-27)
  - Code: [GitHub](https://github.com/ridgerchu/SpikeGPT)

- LLama
  - Year: 2023
  - Publication: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) (2023-02-27)
  - Code: [GitHub](https://github.com/facebookresearch/llama)
  - Model weights: [Private page - request required](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)

- KOSMOS-1
  - Year: 2023
  - Publication: [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045) (2023-02-27)

- ParaFormer
  - Year: 2023
  - Publication: [ParaFormer: Parallel Attention Transformer for Efficient Feature Matching](https://arxiv.org/abs/2303.00941) (2023-03-02)

- Palmyra
  - Year: 2023
  - Publication: [Blog - Palmyra LLMs empower secure, enterprise-grade generative AI for business](https://writer.com/blog/palmyra/) (2023-03-02)
  - Model weights: [HuggingFace models](https://huggingface.co/Writer/palmyra-large)

- PaLM-E
  - Year: 2023
  - Publication: [PaLM-E: An Embodied Multimodal Language Model](https://arxiv.org/abs/2303.03378) (2023-03-06)

- Jurassic-2
  - Year: 2023
  - Publication: [Blog - Announcing Jurassic-2 and Task-Specific APIs](https://www.ai21.com/blog/introducing-j2) (2023-03-09)

- Alpaca
  - Year: 2023
  - Publication: [Alpaca: A Strong Open-Source Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html) (2023-03-13)
  - Code: [GitHub](https://github.com/tatsu-lab/stanford_alpaca)

- TWM
  - Year: 2023
  - Publication: [Transformer-based World Models Are Happy With 100k Interactions](https://arxiv.org/abs/2303.07109) (2023-03-13)
  - Code: [GitHub](https://github.com/jrobine/twm)

- FlexGen
  - Year: 2023
  - Publication: [FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU](https://arxiv.org/abs/2303.06865) (2023-03-13)
  - Code: [GitHub](https://github.com/FMInference/FlexGen)

- GPT-4
  - Year: 2023
  - Publication: [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) (2023-03-15)

- alpaca-opt
  - Year: 2023
  - Publication: [README - alpaca-opt-6.7b repository](https://github.com/Manuel030/alpaca-opt/blob/0466819d3647568a125530488f79c5627dd292ed/README.md) (2023-03-16)
  - Code: [GitHub](https://github.com/Manuel030/alpaca-opt)
  - Model weights: [HuggingFace models](https://huggingface.co/Manuel030/alpaca-opt-6.7b)

- GALPACA
  - Year: 2023
  - Publication: [HuggingFace model card - GALPACA](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b) (2023-03-16)
  - Model weights: [HuggingFace models](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-6.7b)

- AlpacOOM
  - Year: 2023
  - Publication: [Tweet: Alpaca + BLOOM = Alpacoom](https://twitter.com/alfredplpl/status/1636858660218617857) (2023-03-17)
  - Model weights: [HuggingFace models](https://huggingface.co/mrm8488/Alpacoom)

- CoLT5
  - Year: 2023
  - Publication: [CoLT5: Faster Long-Range Transformers with Conditional Computation](https://arxiv.org/abs/2303.09752) (2023-03-17)

- GPT4All
  - Year: 2023
  - Publication: [Tweet: Today we're releasing GPT4All, an assistant-style chatbot distilled from 430k GPT-3.5-Turbo outputs that you can run on your laptop.](https://twitter.com/nomic_ai/status/1640834838578995202) (2023-03-28)
  - Code: [GitHub](https://github.com/nomic-ai/gpt4all)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/gpt4all-lora)

- OpenFlamingo
  - Year: 2023
  - Publication: [Blog - ANNOUNCING OPENFLAMINGO: AN OPEN-SOURCE FRAMEWORK FOR TRAINING VISION-LANGUAGE MODELS WITH IN-CONTEXT LEARNING](https://laion.ai/blog/open-flamingo/) (2023-03-28)
  - Code: [GitHub](https://github.com/mlfoundations/open_flamingo)
  - Model weights: [HuggingFace models](https://huggingface.co/openflamingo/OpenFlamingo-9B)

- LLaMA-Adapter
  - Year: 2023
  - Publication: [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199) (2023-03-28)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- GeoV
  - Year: 2023
  - Publication: [README - GeoV repository](https://github.com/geov-ai/geov/blob/4bd51d2d81168ca1d8917aeb7513a1d95feaf727/readme.md) (2023-03-30)
  - Code: [GitHub](https://github.com/geov-ai/geov)
  - Model weights: [HuggingFace models](https://huggingface.co/GeoV/GeoV-9b)

- CodeGeeX
  - Year: 2023
  - Publication: [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X](https://arxiv.org/abs/2303.17568) (2023-03-30)
  - Code: [GitHub](https://github.com/THUDM/CodeGeeX)
  - Model weights: [Private page - request required](https://models.aminer.cn/codegeex/download/request)

- Vicuna
  - Year: 2023
  - Publication: [Blog - Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality](https://lmsys.org/blog/2023-03-30-vicuna/) (2023-03-30)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/vicuna-7b-v1.5)

- BloombergGPT
  - Year: 2023
  - Publication: [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564) (2023-03-30)

- CodeGeeX2
  - Year: 2023
  - Publication: [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X](https://arxiv.org/abs/2303.17568) (2023-03-30)
  - Code: [GitHub](https://github.com/THUDM/CodeGeeX2)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/codegeex2-6b)

- GPTrillion
  - Year: 2023
  - Publication: [Tweet: GPTrillion: a 1.5T Parameter Open-Source Model](https://twitter.com/BananaDev_/status/1642211220072673286) (2023-04-01)

- Pythia
  - Year: 2023
  - Publication: [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373) (2023-04-03)
  - Code: [GitHub](https://github.com/EleutherAI/pythia)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/pythia-70m)

- Koala
  - Year: 2023
  - Publication: [Blog - Koala: A Dialogue Model for Academic Research](https://bair.berkeley.edu/blog/2023/04/03/koala/) (2023-04-03)
  - Code: [GitHub](https://github.com/young-geng/EasyLM)
  - Model weights: [HuggingFace models](https://huggingface.co/young-geng/koala)

- Cerebras-GPT
  - Year: 2023
  - Publication: [Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster](https://arxiv.org/abs/2304.03208) (2023-04-06)
  - Code: [GitHub](https://github.com/Cerebras/modelzoo)
  - Model weights: [HuggingFace models](https://huggingface.co/cerebras/Cerebras-GPT-13B)

- Dolly
  - Year: 2023
  - Publication: [Blog - Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) (2023-04-12)
  - Code: [GitHub](https://github.com/databrickslabs/dolly)
  - Model weights: [HuggingFace models](https://huggingface.co/databricks/dolly-v2-12b)

- GPT4All-J
  - Year: 2023
  - Publication: [Tweet: Announcing GPT4All-J: The First Apache-2 Licensed Chatbot That Runs Locally on Your Machine](https://twitter.com/andriy_mulyar/status/1646622168350875655) (2023-04-13)
  - Code: [GitHub](https://github.com/nomic-ai/gpt4all)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/gpt4all-j)

- OpenAssistant
  - Year: 2023
  - Publication: [OpenAssistant Conversations -- Democratizing Large Language Model Alignment](https://arxiv.org/abs/2304.07327) (2023-04-14)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)
  - Model weights: [HuggingFace models](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5)

- LLaVA
  - Year: 2023
  - Publication: [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) (2023-04-17)
  - Code: [GitHub](https://github.com/haotian-liu/LLaVA)
  - Model weights: [HuggingFace models](https://huggingface.co/liuhaotian/llava-v1.5-7b)

- umT5
  - Year: 2023
  - Publication: [UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining](https://arxiv.org/abs/2304.09151) (2023-04-18)
  - Code: [GitHub](https://github.com/google-research/t5x)
  - Model weights: [Direct link](https://github.com/google-research/t5x/blob/main/docs/models.md#umt5-checkpoints)

- StableLM-Alpha
  - Year: 2023
  - Publication: [Blog - Stability AI Launches the First of its StableLM Suite of Language Models](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) (2023-04-19)
  - Code: [GitHub](https://github.com/stability-AI/stableLM/)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)

- RMT
  - Year: 2023
  - Publication: [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/abs/2304.11062) (2023-04-19)
  - Code: [GitHub](https://github.com/booydar/t5-experiments/tree/scaling-report)

- MOSS
  - Year: 2023
  - Publication: [README - MOSS repository](https://github.com/OpenLMLab/MOSS/blob/cb43caf8f662f60c855c70e723b0cb9e943db4c0/README_en.md) (2023-04-19)
  - Code: [GitHub](https://github.com/OpenLMLab/MOSS)
  - Model weights: [HuggingFace models](https://huggingface.co/fnlp/moss-moon-003-base)

- MPT-1b-RedPajama-200b
  - Year: 2023
  - Publication: [HuggingFace model card - MPT-1b-RedPajama-200b](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b) (2023-04-20)
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-1b-redpajama-200b)

- MiniGPT-4
  - Year: 2023
  - Publication: [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592) (2023-04-20)
  - Code: [GitHub](https://github.com/Vision-CAIR/MiniGPT-4)

- WizardLM
  - Year: 2023
  - Publication: [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244) (2023-04-24)
  - Code: [GitHub](https://github.com/nlpxucan/abcd)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardLM-30B-V1.0)

- FastChat-T5
  - Year: 2023
  - Publication: [Tweet: We are excited to release FastChat-T5: our compact and commercial-friendly chatbot!](https://twitter.com/lmsysorg/status/1652037026705985537) (2023-04-28)
  - Code: [GitHub](https://github.com/lm-sys/FastChat)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0)

- Lamini
  - Year: 2023
  - Publication: [Blog - Introducing Lamini, the LLM Engine for Rapidly Customizing Models](https://lamini.ai/blog/introducing-lamini) (2023-04-28)
  - Code: [GitHub](https://github.com/lamini-ai/lamini)
  - Model weights: [HuggingFace models](https://huggingface.co/lamini/instruct-peft-tuned-12b)

- LLaMA-Adapter V2
  - Year: 2023
  - Publication: [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model](https://arxiv.org/abs/2304.15010) (2023-04-28)
  - Code: [GitHub](https://github.com/ZrrSkywalker/LLaMA-Adapter)

- OpenLLaMA
  - Year: 2023
  - Publication: [README - OpenLLaMA repository](https://github.com/openlm-research/open_llama/blob/6e7f73eab7e799e2464f38ed977e537bae02873e/README.md) (2023-04-29)
  - Code: [GitHub](https://github.com/openlm-research/open_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/openlm-research/open_llama_13b)

- Unlimiformer
  - Year: 2023
  - Publication: [Unlimiformer: Long-Range Transformers with Unlimited Length Input](https://arxiv.org/abs/2305.01625) (2023-05-02)
  - Code: [GitHub](https://github.com/abertsch72/unlimiformer)
  - Model weights: [HuggingFace models](https://huggingface.co/abertsch/unlimiformer-bart-booksum-retrieval)

- ReplitLM
  - Year: 2023
  - Publication: [Tweet: Last night, we released our new complete code model: replit-code-v1-3b.](https://twitter.com/Replit/status/1653802301331759104) (2023-05-03)
  - Code: [GitHub](https://github.com/replit/ReplitLM)
  - Model weights: [HuggingFace models](https://huggingface.co/replit/replit-code-v1-3b)

- CodeGen2
  - Year: 2023
  - Publication: [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309) (2023-05-03)
  - Code: [GitHub](https://github.com/salesforce/CodeGen2)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen2-16B)

- MPT-7B
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b) (2023-05-05)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b)

- MPT-7B-Instruct
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b) (2023-05-05)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-instruct)

- MPT-7B-Chat
  - Year: 2023
  - Publication: [Blog - Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b) (2023-05-05)
  - Code: [GitHub](https://github.com/mosaicml/llm-foundry)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-chat)

- Otter
  - Year: 2023
  - Publication: [Otter: A Multi-Modal Model with In-Context Instruction Tuning](https://arxiv.org/abs/2305.03726) (2023-05-05)
  - Code: [GitHub](https://github.com/Luodian/otter)
  - Model weights: [HuggingFace models](https://huggingface.co/luodian/otter-9b-hf)

- Open-source PaLM
  - Year: 2023
  - Publication: [README - Open-source PaLM repository](https://github.com/conceptofmind/PaLM/blob/dc2425df678d300b9d5b65d028fc70293c01a589/README.md) (2023-05-07)
  - Code: [GitHub](https://github.com/conceptofmind/PaLM)
  - Model weights: [HuggingFace models](https://huggingface.co/conceptofmind/palm-1b)

- Multimodal-GPT
  - Year: 2023
  - Publication: [MultiModal-GPT: A Vision and Language Model for Dialogue with Humans](https://arxiv.org/abs/2305.04790) (2023-05-08)
  - Code: [GitHub](https://github.com/open-mmlab/Multimodal-GPT)
  - Model weights: [Direct link](https://download.openmmlab.com/mmgpt/v0/mmgpt-lora-v0-release.pt)

- TNN
  - Year: 2023
  - Publication: [Toeplitz Neural Network for Sequence Modeling](https://arxiv.org/abs/2305.04749) (2023-05-08)
  - Code: [GitHub](https://github.com/OpenNLPLab/Tnn)

- StarCoder
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161) (2023-05-09)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoder)

- StarChat Alpha
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161) (2023-05-09)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceH4/starchat-alpha)

- ImageBind
  - Year: 2023
  - Publication: [ImageBind: One Embedding Space To Bind Them All](https://arxiv.org/abs/2305.05665) (2023-05-09)
  - Code: [GitHub](https://github.com/facebookresearch/ImageBind)
  - Model weights: [Direct link](https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth)

- StarCoderBase
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161) (2023-05-09)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoderbase)

- StarCoderPlus
  - Year: 2023
  - Publication: [StarCoder: may the source be with you!](https://arxiv.org/abs/2305.06161) (2023-05-09)
  - Code: [GitHub](https://github.com/bigcode-project/starcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoderplus)

- DLite
  - Year: 2023
  - Publication: [Blog - Announcing DLite V2: Lightweight, Open LLMs That Can Run Anywhere](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e) (2023-05-10)
  - Model weights: [HuggingFace models](https://huggingface.co/aisquared/dlite-v2-1_5b)

- PaLM 2
  - Year: 2023
  - Publication: [Blog - Introducing PaLM 2](https://blog.google/technology/ai/google-palm-2-ai-large-language-model/) (2023-05-10)

- MEGABYTE
  - Year: 2023
  - Publication: [MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers](https://arxiv.org/abs/2305.07185) (2023-05-12)

- CodeT5+
  - Year: 2023
  - Publication: [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922) (2023-05-13)
  - Code: [GitHub](https://github.com/salesforce/CodeT5)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codet5p-16b)

- DarkBERT
  - Year: 2023
  - Publication: [DarkBERT: A Language Model for the Dark Side of the Internet](https://arxiv.org/abs/2305.08596) (2023-05-15)

- mLongT5
  - Year: 2023
  - Publication: [mLongT5: A Multilingual and Efficient Text-To-Text Transformer for Longer Sequences](https://arxiv.org/abs/2305.11129) (2023-05-18)
  - Code: [GitHub](https://github.com/google-research/longt5)

- LIMA
  - Year: 2023
  - Publication: [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) (2023-05-18)

- BLOOMChat
  - Year: 2023
  - Publication: [Blog - BLOOMChat: a New Open Multilingual Chat LLM](https://sambanova.ai/blog/introducing-bloomchat-176b-the-multilingual-chat-based-llm/) (2023-05-19)
  - Model weights: [HuggingFace models](https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1)

- LLM-Pruner
  - Year: 2023
  - Publication: [LLM-Pruner: On the Structural Pruning of Large Language Models](https://arxiv.org/abs/2305.11627) (2023-05-19)
  - Code: [GitHub](https://github.com/horseee/LLM-Pruner)

- RWKV-LM
  - Year: 2023
  - Publication: [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048) (2023-05-22)
  - Code: [GitHub](https://github.com/BlinkDL/RWKV-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)

- Aurora genAI
  - Year: 2023
  - Publication: [Blog - Intel Announces Aurora genAI, Generative AI Model With 1 Trillion Parameters](https://wccftech.com/intel-aurora-genai-chatgpt-competitor-generative-ai-model-with-1-trillion-parameters) (2023-05-22)

- Goat
  - Year: 2023
  - Publication: [Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks](https://arxiv.org/abs/2305.14201) (2023-05-23)
  - Code: [GitHub](https://github.com/liutiedong/goat)
  - Model weights: [HuggingFace models](https://huggingface.co/tiedong/goat-lora-7b)

- UltraLM / UltraLLaMA
  - Year: 2023
  - Publication: [Enhancing Chat Language Models by Scaling High-quality Instructional Conversations](https://arxiv.org/abs/2305.14233) (2023-05-23)
  - Code: [GitHub](https://github.com/thunlp/UltraChat)
  - Model weights: [HuggingFace models](https://huggingface.co/openbmb/UltraLM-13b)

- QLoRA
  - Year: 2023
  - Publication: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) (2023-05-23)
  - Code: [GitHub](https://github.com/artidoro/qlora)

- Gorilla
  - Year: 2023
  - Publication: [Gorilla: Large Language Model Connected with Massive APIs](https://arxiv.org/abs/2305.15334) (2023-05-24)
  - Code: [GitHub](https://github.com/ShishirPatil/gorilla)

- BiomedGPT
  - Year: 2023
  - Publication: [BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks](https://arxiv.org/abs/2305.17100) (2023-05-26)
  - Code: [GitHub](https://github.com/taokz/BiomedGPT)

- Refact-1.6B
  - Year: 2023
  - Publication: [Applying All Recent Innovations To Train a Code Model](https://refact.ai/blog/2023/applying-recent-innovations-to-train-model/) (2023-05-26)
  - Code: [GitHub](https://github.com/smallcloudai/refact/)
  - Model weights: [HuggingFace models](https://huggingface.co/smallcloudai/Refact-1_6B-fim)

- Backpacks
  - Year: 2023
  - Publication: [Backpack Language Models](https://arxiv.org/abs/2305.16765) (2023-05-26)
  - Code: [GitHub](https://github.com/john-hewitt/backpacks-flash-attn)
  - Model weights: [HuggingFace models](https://huggingface.co/stanfordnlp/backpack-gpt2)

- MeZO
  - Year: 2023
  - Publication: [Fine-Tuning Language Models with Just Forward Passes](https://arxiv.org/abs/2305.17333) (2023-05-27)
  - Code: [GitHub](https://github.com/princeton-nlp/MeZO)

- NoPE
  - Year: 2023
  - Publication: [The Impact of Positional Encoding on Length Generalization in Transformers](https://arxiv.org/abs/2305.19466) (2023-05-31)

- MEFT
  - Year: 2023
  - Publication: [Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning](https://arxiv.org/abs/2306.00477) (2023-06-01)
  - Code: [GitHub](https://github.com/baohaoLiao/mefts)

- Falcon
  - Year: 2023
  - Publication: [Blog - The Falcon has landed in the Hugging Face ecosystem](https://huggingface.co/blog/falcon) (2023-06-05)
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/falcon-40b)

- SpQR
  - Year: 2023
  - Publication: [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/abs/2306.03078) (2023-06-05)
  - Code: [GitHub](https://github.com/Vahe1994/SpQR)

- RedPajama-INCITE
  - Year: 2023
  - Publication: [Blog - RedPajama 7B now available, instruct model outperforms all open 7B models on HELM benchmarks](https://www.together.xyz/blog/redpajama-7b) (2023-06-06)
  - Code: [GitHub](https://github.com/togethercomputer/redpajama.cpp)
  - Model weights: [HuggingFace models](https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base)

- Tulu
  - Year: 2023
  - Publication: [How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources](https://arxiv.org/abs/2306.04751) (2023-06-07)
  - Code: [GitHub](https://github.com/allenai/open-instruct)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/tulu-65b)

- INSTRUCTEVAL
  - Year: 2023
  - Publication: [INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2306.04757) (2023-06-07)
  - Code: [GitHub](https://github.com/declare-lab/instruct-eval)
  - Model weights: [HuggingFace models](https://huggingface.co/declare-lab/flan-alpaca-xxl)

- MoLM
  - Year: 2023
  - Publication: [ModuleFormer: Modularity Emerges from Mixture-of-Experts](https://arxiv.org/abs/2306.04640) (2023-06-07)
  - Code: [GitHub](https://github.com/IBM/ModuleFormer)
  - Model weights: [HuggingFace models](https://huggingface.co/ibm/MoLM-350M-4B)

- FinGPT
  - Year: 2023
  - Publication: [FinGPT: Open-Source Financial Large Language Models](https://arxiv.org/abs/2306.06031) (2023-06-09)
  - Code: [GitHub](https://github.com/ai4finance-foundation/fingpt)

- LongMem
  - Year: 2023
  - Publication: [Augmenting Language Models with Long-Term Memory](https://arxiv.org/abs/2306.07174) (2023-06-12)
  - Code: [GitHub](https://github.com/Victorwz/LongMem)

- Orca
  - Year: 2023
  - Publication: [Orca: Progressive Learning from Complex Explanation Traces of GPT-4](https://arxiv.org/abs/2306.07174) (2023-06-12)

- h2ogpt
  - Year: 2023
  - Publication: [h2oGPT: Democratizing Large Language Models](https://arxiv.org/abs/2306.08161) (2023-06-13)
  - Code: [GitHub](https://github.com/h2oai/h2ogpt)
  - Model weights: [HuggingFace models](https://huggingface.co/h2oai/h2ogpt-oasst1-512-12b)

- SqueezeLLM
  - Year: 2023
  - Publication: [SqueezeLLM: Dense-and-Sparse Quantization](https://export.arxiv.org/abs/2306.07629) (2023-06-13)
  - Code: [GitHub](https://github.com/SqueezeAILab/SqueezeLLM)

- WizardCoder
  - Year: 2023
  - Publication: [WizardCoder: Empowering Code Large Language Models with Evol-Instruct](https://arxiv.org/abs/2306.08568) (2023-06-14)
  - Code: [GitHub](https://github.com/nlpxucan/abcd)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0)

- Macaw-LLM
  - Year: 2023
  - Publication: [Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration](https://arxiv.org/abs/2306.09093) (2023-06-15)
  - Code: [GitHub](https://github.com/lyuchenyang/Macaw-LLM)

- LOMO
  - Year: 2023
  - Publication: [Full Parameter Fine-tuning for Large Language Models with Limited Resources](https://arxiv.org/abs/2306.09782) (2023-06-16)
  - Code: [GitHub](https://github.com/OpenLMLab/LOMO)

- phi-1
  - Year: 2023
  - Publication: [Textbooks Are All You Need](https://arxiv.org/abs/2306.11644) (2023-06-20)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-1)

- Inflection-1
  - Year: 2023
  - Publication: [Blog - Inflection-1: Piâ€™s Best-in-Class LLM](https://inflection.ai/inflection-1) (2023-06-22)

- ChatGLM-6B
  - Year: 2023
  - Publication: [README - ChatGLM-6B repository](https://github.com/THUDM/ChatGLM-6B/blob/d835c4b0017310d53b98bad98f68671fa861d158/README_en.md) (2023-06-25)
  - Code: [GitHub](https://github.com/THUDM/ChatGLM-6B)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/chatglm-6b)

- Kosmos-2
  - Year: 2023
  - Publication: [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824) (2023-06-26)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/kosmos-2)
  - Model weights: [direct link](https://conversationhub.blob.core.windows.net/beit-share-public/kosmos-2/kosmos-2.pt?sv=2021-10-04&st=2023-06-08T11%3A16%3A02Z&se=2033-06-09T11%3A16%3A00Z&sr=c&sp=r&sig=N4pfCVmSeq4L4tS8QbrFVsX6f6q844eft8xSuXdxU48%3D)

- Position Interpolation
  - Year: 2023
  - Publication: [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/abs/2306.15595) (2023-06-27)

- XGen
  - Year: 2023
  - Publication: [Blog - Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length](https://blog.salesforceairesearch.com/xgen/) (2023-06-28)
  - Code: [GitHub](https://github.com/salesforce/xGen)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/xgen-7b-8k-base)

- NTK
  - Year: 2023
  - Publication: [Blog - NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.](https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/) (2023-06-29)

- LongChat
  - Year: 2023
  - Publication: [Blog - How Long Can Open-Source LLMs Truly Promise on Context Length?](https://lmsys.org/blog/2023-06-29-longchat/) (2023-06-29)
  - Code: [GitHub](https://github.com/DachengLi1/LongChat)
  - Model weights: [HuggingFace models](https://huggingface.co/lmsys/longchat-7b-v1.5-32k)

- LongNet
  - Year: 2023
  - Publication: [LongNet: Scaling Transformers to 1,000,000,000 Tokens](https://arxiv.org/abs/2307.02486) (2023-07-05)
  - Code: [GitHub](https://github.com/microsoft/unilm)

- CodeGen2.5
  - Year: 2023
  - Publication: [Blog - CodeGen2.5: Small, but mighty](https://blog.salesforceairesearch.com/codegen25/) (2023-07-06)
  - Code: [GitHub](https://github.com/salesforce/CodeGen2)
  - Model weights: [HuggingFace models](https://huggingface.co/Salesforce/codegen25-7b-multi)

- InternLM
  - Year: 2023
  - Publication: [InternLM: A Multilingual Language Model with Progressively Enhanced Capabilities](https://github.com/InternLM/InternLM-techreport/blob/main/InternLM.pdf) (2023-07-06)
  - Code: [GitHub](https://github.com/InternLM/InternLM)
  - Model weights: [HuggingFace models](https://huggingface.co/internlm/internlm-7b)

- LongLLaMA
  - Year: 2023
  - Publication: [Focused Transformer: Contrastive Training for Context Scaling](https://arxiv.org/abs/2307.03170) (2023-07-06)
  - Code: [GitHub](https://github.com/CStanKonrad/long_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/syzymon/long_llama_3b)

- Redmond-Hermes-Coder
  - Year: 2023
  - Publication: [Tweet: Releasing Redmond-Hermes-Coder, a finetune of Wizardcoder on our dataset.](https://twitter.com/NousResearch/status/1674992144170340353) (2023-07-07)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Redmond-Hermes-Coder)

- PolyLM
  - Year: 2023
  - Publication: [PolyLM: An Open Source Polyglot Large Language Model](https://arxiv.org/abs/2307.06018) (2023-07-12)
  - Code: [GitHub](https://github.com/DAMO-NLP-MT/PolyLM)
  - Model weights: [HuggingFace models](https://huggingface.co/DAMO-NLP-MT/polylm-13b)

- RetNet
  - Year: 2023
  - Publication: [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/abs/2307.08621) (2023-07-17)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/retnet)

- MPT-7B-8k
  - Year: 2023
  - Publication: [Blog - Announcing MPT-7B-8K: 8K Context Length for Document Understanding](https://www.mosaicml.com/blog/long-context-mpt-7b-8k) (2023-07-18)
  - Code: [GitHub](https://github.com/CStanKonrad/long_llama)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mpt-7b-8k)

- Llama 2
  - Year: 2023
  - Publication: [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288) (2023-07-18)
  - Code: [GitHub](https://github.com/facebookresearch/llama)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-llama/Llama-2-7b)

- LRPE
  - Year: 2023
  - Publication: [Linearized Relative Positional Encoding](https://arxiv.org/abs/2307.09270) (2023-07-18)
  - Code: [GitHub](https://github.com/OpenNLPLab/Lrpe)

- PanGu-Coder2
  - Year: 2023
  - Publication: [PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback](https://arxiv.org/abs/2307.14936) (2023-07-27)

- Alfred-40B-0723
  - Year: 2023
  - Publication: [Blog - Introducing Alfred-40B-0723](https://www.lighton.ai/blog/lighton-s-blog-4/introducing-alfred-40b-0723-38) (2023-07-31)
  - Model weights: [HuggingFace models](https://huggingface.co/lightonai/alfred-40b-0723)

- Baby-CoThought
  - Year: 2023
  - Publication: [Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models](https://arxiv.org/abs/2308.01684) (2023-08-03)
  - Code: [GitHub](https://github.com/oooranz/Baby-CoThought)
  - Model weights: [HuggingFace models](https://huggingface.co/yaanhaan/Baby-CoThought)

- OpenHermes
  - Year: 2023
  - Publication: [HuggingFace model card - OpenHermes](https://huggingface.co/teknium/OpenHermes-13B) (2023-08-06)
  - Model weights: [HuggingFace models](https://huggingface.co/teknium/OpenHermes-13B)

- StableCode Complete Alpha
  - Year: 2023
  - Publication: [Blog - Announcing StableCode](https://stability.ai/blog/stablecode-llm-generative-ai-coding) (2023-08-08)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablecode-completion-alpha-3b)

- Claude Instant 1.2
  - Year: 2023
  - Publication: [Blog - Releasing Claude Instant 1.2](https://www.anthropic.com/index/releasing-claude-instant-1-2) (2023-08-09)

- OctoCoder
  - Year: 2023
  - Publication: [OctoPack: Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2308.07124) (2023-08-14)
  - Code: [GitHub](https://github.com/bigcode-project/octopack)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/octocoder)

- OctoGeeX
  - Year: 2023
  - Publication: [OctoPack: Instruction Tuning Code Large Language Models](https://arxiv.org/abs/2308.07124) (2023-08-14)
  - Code: [GitHub](https://github.com/bigcode-project/octopack)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/octogeex)

- Aquila
  - Year: 2023
  - Publication: [README - Aquila repository](https://github.com/FlagAI-Open/FlagAI/blob/e3062883ca5156d5c6f6b1992a5f1a08edbd437c/examples/Aquila/README_en.md) (2023-08-15)
  - Code: [GitHub](https://github.com/FlagAI-Open/FlagAI)
  - Model weights: [HuggingFace models](https://huggingface.co/BAAI/Aquila-7B)

- DeciCoder
  - Year: 2023
  - Publication: [Blog - Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation](https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/) (2023-08-15)
  - Model weights: [HuggingFace models](https://huggingface.co/Deci/DeciCoder-1b)

- WizardMath
  - Year: 2023
  - Publication: [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583) (2023-08-18)
  - Code: [GitHub](https://github.com/nlpxucan/abcd)
  - Model weights: [HuggingFace models](https://huggingface.co/WizardLM/WizardMath-7B-V1.0)

- SQLCoder
  - Year: 2023
  - Publication: [Blog - Open-sourcing SQLCoder: a state-of-the-art LLM for SQL generation](https://defog.ai/blog/open-sourcing-sqlcoder/) (2023-08-20)
  - Code: [GitHub](https://github.com/defog-ai/sqlcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/defog/sqlcoder)

- Giraffe
  - Year: 2023
  - Publication: [Giraffe: Adventures in Expanding Context Lengths in LLMs](https://arxiv.org/abs/2308.10882) (2023-08-21)
  - Code: [GitHub](https://github.com/abacusai/Long-Context)
  - Model weights: [HuggingFace models](https://huggingface.co/abacusai/Giraffe-v2-13b-32k)

- OpenMoE
  - Year: 2023
  - Publication: [Blog - OpenMoE v0.2 Release](https://xuefuzhao.notion.site/Aug-2023-OpenMoE-v0-2-Release-43808efc0f5845caa788f2db52021879) (2023-08-21)
  - Code: [GitHub](https://github.com/XueFuzhao/OpenMoE)
  - Model weights: [HuggingFace models](https://huggingface.co/fuzhao/OpenMoE_Base)

- Code Llama
  - Year: 2023
  - Publication: [Code Llama: Open Foundation Models for Code](https://arxiv.org/abs/2308.12950) (2023-08-24)
  - Code: [GitHub](https://github.com/facebookresearch/codellama)
  - Model weights: [Direct link](https://ai.meta.com/resources/models-and-libraries/llama-downloads/)

- Phind-CodeLlama
  - Year: 2023
  - Publication: [HuggingFace model card - Phind-CodeLlama](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2) (2023-08-28)
  - Model weights: [HuggingFace models](https://huggingface.co/Phind/Phind-CodeLlama-34B-v2)

- Jais
  - Year: 2023
  - Publication: [Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models](https://arxiv.org/abs/2308.16149) (2023-08-30)
  - Model weights: [HuggingFace models](https://huggingface.co/core42/jais-13b)

- Yarn
  - Year: 2023
  - Publication: [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071) (2023-08-31)
  - Code: [GitHub](https://github.com/jquesnelle/yarn)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k)

- One Wide FFN
  - Year: 2023
  - Publication: [One Wide Feedforward is All You Need](https://arxiv.org/abs/2309.01826) (2023-09-04)

- Falcon-180B
  - Year: 2023
  - Publication: [Blog - Spread Your Wings: Falcon 180B is here](https://huggingface.co/blog/falcon-180b) (2023-09-06)
  - Model weights: [HuggingFace models](https://huggingface.co/tiiuae/falcon-180B)

- MathGLM
  - Year: 2023
  - Publication: [GPT Can Solve Mathematical Problems Without a Calculator](https://arxiv.org/abs/2309.03241) (2023-09-06)
  - Code: [GitHub](https://github.com/THUDM/MathGLM)
  - Model weights: [Direct link](https://cloud.tsinghua.edu.cn/d/92127e3a1b4144db8d13/)

- Persimmon-8B
  - Year: 2023
  - Publication: [Releasing Persimmon-8B](https://www.adept.ai/blog/persimmon-8b) (2023-09-07)
  - Code: [GitHub](https://github.com/persimmon-ai-labs/adept-inference)
  - Model weights: [Direct link](https://axtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-oci.com/n/axtkn4xl5cip/b/adept-public-data/o/8b_base_model_release.tar)

- FLM-101B
  - Year: 2023
  - Publication: [FLM-101B: An Open LLM and How to Train It with $100K Budget](https://arxiv.org/abs/2309.03852) (2023-09-07)
  - Model weights: [HuggingFace models](https://huggingface.co/CofeAI/FLM-101B)

- DoLa
  - Year: 2023
  - Publication: [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883) (2023-09-07)
  - Code: [GitHub](https://github.com/voidism/DoLa)

- phi-1.5
  - Year: 2023
  - Publication: [Textbooks Are All You Need II: phi-1.5 technical report](https://arxiv.org/abs/2309.05463) (2023-09-11)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-1_5)

- MAmmoTH
  - Year: 2023
  - Publication: [MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning](https://arxiv.org/abs/2309.05653) (2023-09-11)
  - Code: [GitHub](https://github.com/TIGER-AI-Lab/MAmmoTH)
  - Model weights: [HuggingFace models](https://huggingface.co/TIGER-Lab/MAmmoTH-Coder-7B)

- MoV, MoLoRA
  - Year: 2023
  - Publication: [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://arxiv.org/abs/2309.05444) (2023-09-11)
  - Code: [GitHub](https://github.com/for-ai/parameter-efficient-moe)

- vLLM
  - Year: 2023
  - Publication: [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180) (2023-09-12)
  - Code: [GitHub](https://github.com/vllm-project/vllm)

- OpenHermes-2-Mistral
  - Year: 2023
  - Publication: [HuggingFace model card - OpenHermes-2-Mistral](https://huggingface.co/teknium/OpenHermes-2-Mistral-7B) (2023-09-12)
  - Model weights: [HuggingFace models](https://huggingface.co/teknium/OpenHermes-2-Mistral-7B)

- DeciLM
  - Year: 2023
  - Publication: [Blog - 15 times Faster than Llama 2: Introducing DeciLM â€“ NAS-Generated LLM with Variable GQA](https://deci.ai/blog/decilm-15-times-faster-than-llama2-nas-generated-llm-with-variable-gqa/) (2023-09-13)
  - Model weights: [HuggingFace models](https://huggingface.co/Deci/DeciLM-6b)

- RAIN
  - Year: 2023
  - Publication: [RAIN: Your Language Models Can Align Themselves without Finetuning](https://arxiv.org/abs/2309.07124) (2023-09-13)
  - Code: [GitHub](https://github.com/SafeAILab/RAIN)

- Xwin-LM
  - Year: 2023
  - Publication: [README - README -](https://github.com/Xwin-LM/Xwin-LM/blob/25862b325ee2a5b7c3c0007728eaeeb3daf5f05f/README.md) (2023-09-16)
  - Code: [GitHub](https://github.com/Xwin-LM/Xwin-LM)
  - Model weights: [HuggingFace models](https://huggingface.co/Xwin-LM/Xwin-LM-13B-V0.2)

- AdaptLLM
  - Year: 2023
  - Publication: [Adapting Large Language Models via Reading Comprehension](https://arxiv.org/abs/2309.09530) (2023-09-18)
  - Code: [GitHub](https://github.com/microsoft/LMOps)

- Baichuan 2
  - Year: 2023
  - Publication: [Baichuan 2: Open Large-scale Language Models](https://arxiv.org/abs/2309.10305) (2023-09-19)
  - Code: [GitHub](https://github.com/baichuan-inc/Baichuan2)
  - Model weights: [HuggingFace models](https://huggingface.co/baichuan-inc/Baichuan2-13B-Base)

- PoSE
  - Year: 2023
  - Publication: [PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training](https://arxiv.org/abs/2309.10400) (2023-09-19)
  - Code: [GitHub](https://github.com/dwzhu-pku/PoSE)
  - Model weights: [HuggingFace models](https://huggingface.co/dwzhu/LLaMA-7B-PoSE-Linear-16k)

- BTLM-3B-8k
  - Year: 2023
  - Publication: [BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model](https://arxiv.org/abs/2309.11568) (2023-09-20)
  - Model weights: [HuggingFace models](https://huggingface.co/cerebras/btlm-3b-8k-base)

- Kosmos-2.5
  - Year: 2023
  - Publication: [Kosmos-2.5: A Multimodal Literate Model](https://arxiv.org/abs/2309.11419) (2023-09-20)

- Nous-Capybara
  - Year: 2023
  - Publication: [HuggingFace model card - Nous-Capybara](https://huggingface.co/NousResearch/Nous-Capybara-7B-GGUF) (2023-09-20)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Nous-Capybara-7B-GGUF)

- OpenChat
  - Year: 2023
  - Publication: [OpenChat: Advancing Open-source Language Models with Mixed-Quality Data](https://arxiv.org/abs/2309.11235) (2023-09-20)
  - Code: [GitHub](https://github.com/imoneoi/openchat)
  - Model weights: [HuggingFace models](https://huggingface.co/openchat/openchat_3.5)

- Glaive-coder
  - Year: 2023
  - Publication: [Blog - Releasing glaive-coder-7B and the Code Models Arena](https://glaive.ai/blog/releasing-code-model-arena) (2023-09-21)
  - Model weights: [HuggingFace models](https://huggingface.co/glaiveai/glaive-coder-7b)

- LongLoRA
  - Year: 2023
  - Publication: [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307) (2023-09-21)
  - Code: [GitHub](https://github.com/dvlab-research/LongLoRA)
  - Model weights: [HuggingFace models](https://huggingface.co/Yukang/Llama-2-7b-longlora-100k-ft)

- MetaMath
  - Year: 2023
  - Publication: [MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models](https://arxiv.org/abs/2309.12284) (2023-09-21)
  - Code: [GitHub](https://github.com/meta-math/MetaMath)
  - Model weights: [HuggingFace models](https://huggingface.co/meta-math/MetaMath-7B-V1.0)

- DeepSpeed-Ulysses
  - Year: 2023
  - Publication: [DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://arxiv.org/abs/2309.14509) (2023-09-25)

- QA-LoRA
  - Year: 2023
  - Publication: [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717) (2023-09-26)

- Mistral
  - Year: 2023
  - Publication: [Blog - Mistral 7B - The best 7B model to date, Apache 2.0](https://mistral.ai/news/announcing-mistral-7b/) (2023-09-27)
  - Code: [GitHub](https://github.com/mistralai/mistral-src)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mistral-7B-v0.1)

- Qwen
  - Year: 2023
  - Publication: [Qwen Technical Report](https://arxiv.org/abs/2309.16609) (2023-09-28)
  - Code: [GitHub](https://github.com/QwenLM/Qwen)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen-7B)

- Nous-Hermes
  - Year: 2023
  - Publication: [Tweet: Nous-Hermes-13b fp16 weights have been released.](https://twitter.com/NousResearch/status/1664848823687028737) (2023-09-29)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Nous-Hermes-13b)

- StableLM-3B-4E1T
  - Year: 2023
  - Publication: [Blog - Technical report for StableLM-3B-4E1T](https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo) (2023-09-29)
  - Code: [GitHub](https://github.com/Stability-AI/StableLM)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-3b-4e1t)

- StreamingLLM
  - Year: 2023
  - Publication: [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453) (2023-09-29)
  - Code: [GitHub](https://github.com/mit-han-lab/streaming-llm)

- OpenHermes-2.5-Mistral
  - Year: 2023
  - Publication: [HuggingFace model card - OpenHermes-2.5-Mistral](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B) (2023-09-29)
  - Model weights: [HuggingFace models](https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B)

- MiniGPT-5
  - Year: 2023
  - Publication: [MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens](https://arxiv.org/abs/2310.02239) (2023-10-03)
  - Code: [GitHub](https://github.com/eric-ai-lab/MiniGPT-5)

- Kosmos-G
  - Year: 2023
  - Publication: [Kosmos-G: Generating Images in Context with Multimodal Large Language Models](https://arxiv.org/abs/2310.02992) (2023-10-04)

- LightSeq
  - Year: 2023
  - Publication: [LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers](https://arxiv.org/abs/2310.03294) (2023-10-05)
  - Code: [GitHub](https://github.com/RulinShao/LightSeq)

- AgentInstruct
  - Year: 2023
  - Publication: [Agent Instructs Large Language Models to be General Zero-Shot Reasoners](https://arxiv.org/abs/2310.03710) (2023-10-05)
  - Code: [GitHub](https://github.com/wang-research-lab/agentinstruct)
  - Model weights: [HuggingFace models](https://huggingface.co/datasets/WangResearchLab/AgentInstruct)

- FIRE
  - Year: 2023
  - Publication: [Functional Interpolation for Relative Positions Improves Long Context Transformers](https://arxiv.org/abs/2310.04418) (2023-10-06)

- HyperAttention
  - Year: 2023
  - Publication: [HyperAttention: Long-context Attention in Near-Linear Time](https://arxiv.org/abs/2310.05869) (2023-10-09)

- LLMLingua
  - Year: 2023
  - Publication: [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736) (2023-10-09)
  - Code: [GitHub](https://github.com/microsoft/LLMLingua)

- LongLLMLingua
  - Year: 2023
  - Publication: [LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression](https://arxiv.org/abs/2310.06839) (2023-10-10)
  - Code: [GitHub](https://github.com/microsoft/LLMLingua)

- CodeFuse
  - Year: 2023
  - Publication: [CodeFuse-13B: A Pretrained Multi-lingual Code Large Language Model](https://arxiv.org/abs/2310.06266) (2023-10-10)
  - Code: [GitHub](https://github.com/codefuse-ai/MFTCoder)
  - Model weights: [HuggingFace models](https://huggingface.co/codefuse-ai/CodeFuse-13B)

- MemGPT
  - Year: 2023
  - Publication: [MemGPT: Towards LLMs as Operating Systems](https://arxiv.org/abs/2310.08560) (2023-10-12)
  - Code: [GitHub](https://github.com/cpacker/MemGPT)

- LoftQ
  - Year: 2023
  - Publication: [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659) (2023-10-12)
  - Code: [GitHub](https://github.com/yxli2123/LoftQ)
  - Model weights: [HuggingFace models](https://huggingface.co/LoftQ/Meta-Llama-3-8B-4bit-64rank)

- MiniGPT-v2
  - Year: 2023
  - Publication: [MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning](https://arxiv.org/abs/2310.09478) (2023-10-14)
  - Code: [GitHub](https://github.com/Vision-CAIR/MiniGPT-4)

- Llemma
  - Year: 2023
  - Publication: [Llemma: An Open Language Model For Mathematics](https://arxiv.org/abs/2310.10631) (2023-10-16)
  - Code: [GitHub](https://github.com/EleutherAI/math-lm)
  - Model weights: [HuggingFace models](https://huggingface.co/EleutherAI/llemma_7b)

- MistralLite
  - Year: 2023
  - Publication: [HuggingFace model card - MistralLite](https://huggingface.co/amazon/MistralLite) (2023-10-16)
  - Model weights: [HuggingFace models](https://huggingface.co/amazon/MistralLite)

- MistralLite
  - Year: 2023
  - Publication: [HuggingFace model card - MistralLite](https://huggingface.co/amazon/MistralLite) (2023-10-16)
  - Model weights: [HuggingFace models](https://huggingface.co/amazon/MistralLite)

- Fuyu
  - Year: 2023
  - Publication: [Blog - Fuyu-8B: A Multimodal Architecture for AI Agents](https://www.adept.ai/blog/fuyu-8b) (2023-10-17)
  - Model weights: [HuggingFace models](https://huggingface.co/adept/fuyu-8b)

- VeRA
  - Year: 2023
  - Publication: [VeRA: Vector-based Random Matrix Adaptation](https://arxiv.org/abs/2310.11454) (2023-10-17)

- BitNet
  - Year: 2023
  - Publication: [BitNet: Scaling 1-bit Transformers for Large Language Models](https://arxiv.org/abs/2310.11453) (2023-10-17)

- Monarch Mixer:
  - Year: 2023
  - Publication: [Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture](https://arxiv.org/abs/2310.12109) (2023-10-18)

- Obsidian
  - Year: 2023
  - Publication: [HuggingFace model card - Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5) (2023-10-24)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)

- Zephyr
  - Year: 2023
  - Publication: [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944) (2023-10-25)
  - Code: [GitHub](https://github.com/huggingface/alignment-handbook)
  - Model weights: [HuggingFace models](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)

- QMoE
  - Year: 2023
  - Publication: [QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models](https://arxiv.org/abs/2310.16795) (2023-10-25)
  - Code: [GitHub](https://github.com/IST-DASLab/qmoe)

- CodeFusion
  - Year: 2023
  - Publication: [CodeFusion: A Pre-trained Diffusion Model for Code Generation](https://arxiv.org/abs/2310.17680) (2023-10-26)
  - Code: [GitHub](https://github.com/microsoft/prose-benchmarks)

- FP8-LM
  - Year: 2023
  - Publication: [FP8-LM: Training FP8 Large Language Models](https://arxiv.org/abs/2310.18313) (2023-10-27)
  - Code: [GitHub](https://github.com/Azure/MS-AMP)

- Deepseek Coder
  - Year: 2023
  - Publication: [Blog - DeepSeek Coder: Let the Code Write Itself](https://deepseekcoder.github.io/) (2023-10-28)
  - Code: [GitHub](https://github.com/deepseek-ai/deepseek-coder/)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base)

- Punica
  - Year: 2023
  - Publication: [Punica: Multi-Tenant LoRA Serving](https://arxiv.org/abs/2310.18547) (2023-10-28)
  - Code: [GitHub](https://github.com/punica-ai/punica)

- LeMa
  - Year: 2023
  - Publication: [Learning From Mistakes Makes LLM Better Reasoner](https://arxiv.org/abs/2310.20689) (2023-10-31)
  - Code: [GitHub](https://github.com/microsoft/CodeT)

- ChipNeMo
  - Year: 2023
  - Publication: [ChipNeMo: Domain-Adapted LLMs for Chip Design](https://arxiv.org/abs/2311.00176) (2023-10-31)

- TopicGPT
  - Year: 2023
  - Publication: [TopicGPT: A Prompt-based Topic Modeling Framework](https://arxiv.org/abs/2311.01449) (2023-11-02)
  - Code: [GitHub](https://github.com/chtmp223/topicGPT)

- Simplified Transformers
  - Year: 2023
  - Publication: [Simplifying Transformer Blocks](https://arxiv.org/abs/2311.01906) (2023-11-03)
  - Code: [GitHub](https://github.com/bobby-he/simplified_transformers)

- LSS Transformer
  - Year: 2023
  - Publication: [Ultra-Long Sequence Distributed Transformer](https://arxiv.org/abs/2311.02382) (2023-11-04)

- Octavius
  - Year: 2023
  - Publication: [Octavius: Mitigating Task Interference in MLLMs via MoE](https://arxiv.org/abs/2311.02684) (2023-11-05)

- CogVLM
  - Year: 2023
  - Publication: [CogVLM: Visual Expert for Pretrained Language Models](https://arxiv.org/abs/2311.03079) (2023-11-06)
  - Code: [GitHub](https://github.com/THUDM/CogVLM)
  - Model weights: [HuggingFace models](https://huggingface.co/THUDM/CogVLM)

- S-LoRA
  - Year: 2023
  - Publication: [S-LoRA: Serving Thousands of Concurrent LoRA Adapters](https://arxiv.org/abs/2311.03285) (2023-11-06)
  - Code: [GitHub](https://github.com/S-LoRA/S-LoRA)

- LongQLoRA
  - Year: 2023
  - Publication: [LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models](https://arxiv.org/abs/2311.04879) (2023-11-08)
  - Code: [GitHub](https://github.com/yangjianxin1/LongQLoRA)

- LLM Decontaminator
  - Year: 2023
  - Publication: [Rethinking Benchmark and Contamination for Language Models with Rephrased Samples](https://arxiv.org/abs/2311.04850) (2023-11-08)
  - Code: [GitHub](https://github.com/lm-sys/llm-decontaminator)

- Mirasol3B
  - Year: 2023
  - Publication: [Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities](https://arxiv.org/abs/2311.05698) (2023-11-09)

- FlashFFTConv
  - Year: 2023
  - Publication: [FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores](https://arxiv.org/abs/2311.05908) (2023-11-10)
  - Code: [GitHub](https://github.com/HazyResearch/flash-fft-conv)

- MiniMA
  - Year: 2023
  - Publication: [Towards the Law of Capacity Gap in Distilling Language Models](https://arxiv.org/abs/2311.07052) (2023-11-13)
  - Code: [GitHub](https://github.com/genezc/minima)
  - Model weights: [HuggingFace models](https://huggingface.co/GeneZC/MiniMA-3B)

- Neural-chat
  - Year: 2023
  - Publication: [HuggingFace model card - Neural-chat](https://huggingface.co/Intel/neural-chat-7b-v3-1) (2023-11-14)
  - Code: [GitHub](https://github.com/intel/intel-extension-for-transformers)
  - Model weights: [HuggingFace models](https://huggingface.co/Intel/neural-chat-7b-v3-1)

- UltraFastBERT
  - Year: 2023
  - Publication: [Exponentially Faster Language Modelling](https://arxiv.org/abs/2311.10770) (2023-11-15)
  - Code: [GitHub](https://github.com/pbelcak/UltraFastBERT)
  - Model weights: [HuggingFace models](https://huggingface.co/pbelcak/UltraFastBERT-1x11-long)

- Tulu 2
  - Year: 2023
  - Publication: [Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2](https://arxiv.org/abs/2311.10702) (2023-11-17)
  - Code: [GitHub](https://github.com/allenai/open-instruct)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/tulu-2-7b)

- Orca 2
  - Year: 2023
  - Publication: [Orca 2: Teaching Small Language Models How to Reason](https://arxiv.org/abs/2311.11045) (2023-11-18)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/Orca-2-13b)

- LQ-LoRA
  - Year: 2023
  - Publication: [LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning](https://arxiv.org/abs/2311.12023) (2023-11-20)
  - Code: [GitHub](https://github.com/HanGuo97/lq-lora)

- System 2 Attention
  - Year: 2023
  - Publication: [System 2 Attention (is something you might need too)](https://arxiv.org/abs/2311.11829) (2023-11-20)

- Lookahead Decoding
  - Year: 2023
  - Publication: [Blog - Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/) (2023-11-21)
  - Code: [GitHub](https://github.com/hao-ai-lab/LookaheadDecoding)

- Inflection-2
  - Year: 2023
  - Publication: [Blog - Inflection-2: The Next Step Up](https://inflection.ai/inflection-2) (2023-11-22)

- QuIP
  - Year: 2023
  - Publication: [Blog - QuIP with Lattice Codebooks](https://cornell-relaxml.github.io/quip-sharp/) (2023-11-23)
  - Code: [GitHub](https://github.com/Cornell-RelaxML/quip-sharp)

- Starling
  - Year: 2023
  - Publication: [Blog - Starling-7B: Increasing LLM Helpfulness & Harmlessness with RLAIF](https://starling.cs.berkeley.edu/) (2023-11-28)
  - Model weights: [HuggingFace models](https://huggingface.co/berkeley-nest/Starling-RM-7B-alpha)

- Notus
  - Year: 2023
  - Publication: [HuggingFace model card - Notus](https://huggingface.co/argilla/notus-7b-v1) (2023-12-01)
  - Code: [GitHub](https://github.com/argilla-io/notus)
  - Model weights: [HuggingFace models](https://huggingface.co/argilla/notus-7b-v1)

- Mamba
  - Year: 2023
  - Publication: [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/abs/2312.00752) (2023-12-01)
  - Code: [GitHub](https://github.com/havenhq/mamba-chat)
  - Model weights: [HuggingFace models](https://huggingface.co/havenhq/mamba-chat)

- Magicoder
  - Year: 2023
  - Publication: [Magicoder: Source Code Is All You Need](https://arxiv.org/abs/2312.02120) (2023-12-04)
  - Code: [GitHub](https://github.com/ise-uiuc/magicoder)
  - Model weights: [HuggingFace models](https://huggingface.co/ise-uiuc/Magicoder-CL-7B)

- RankZephyr
  - Year: 2023
  - Publication: [RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!](https://arxiv.org/abs/2312.02724) (2023-12-05)
  - Code: [GitHub](https://github.com/castorini/rank_llm)
  - Model weights: [HuggingFace models](https://huggingface.co/castorini/rank_vicuna_7b_v1)

- Xaberius
  - Year: 2023
  - Publication: [HuggingFace model card - Xaberius](https://huggingface.co/fblgit/una-xaberius-34b-v1beta) (2023-12-05)
  - Model weights: [HuggingFace models](https://huggingface.co/fblgit/una-xaberius-34b-v1beta)

- OneLLM
  - Year: 2023
  - Publication: [OneLLM: One Framework to Align All Modalities with Language](https://arxiv.org/abs/2312.03700) (2023-12-06)
  - Code: [GitHub](https://github.com/csuhan/OneLLM)
  - Model weights: [HuggingFace models](https://huggingface.co/csuhan/OneLLM-7B)

- StableLM Zephyr
  - Year: 2023
  - Publication: [Blog - Introducing Stable LM Zephyr 3B: A New Addition to Stable LM, Bringing Powerful LLM Assistants to Edge Devices](https://stability.ai/news/stablelm-zephyr-3b-stability-llm) (2023-12-07)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stablelm-zephyr-3b)

- EAGLE
  - Year: 2023
  - Publication: [Blog - EAGLE: Lossless Acceleration of LLM Decoding by Feature Extrapolation](https://sites.google.com/view/eagle-llm) (2023-12-08)

- StripedHyena
  - Year: 2023
  - Publication: [Blog - Paving the way to efficient architectures: StripedHyena-7B, open source models offering a glimpse into a world beyond Transformers](https://www.together.ai/blog/stripedhyena-7b) (2023-12-08)
  - Code: [GitHub](https://github.com/togethercomputer/stripedhyena)
  - Model weights: [HuggingFace models](https://huggingface.co/togethercomputer/StripedHyena-Hessian-7B)

- Amber
  - Year: 2023
  - Publication: [LLM360: Towards Fully Transparent Open-Source LLMs](https://arxiv.org/abs/2312.06550) (2023-12-11)
  - Model weights: [HuggingFace models](https://huggingface.co/LLM360/Amber)

- CrystalCoder
  - Year: 2023
  - Publication: [LLM360: Towards Fully Transparent Open-Source LLMs](https://arxiv.org/abs/2312.06550) (2023-12-11)
  - Model weights: [HuggingFace models](https://huggingface.co/LLM360/CrystalCoder)

- phi-2
  - Year: 2023
  - Publication: [Blog - Phi-2: The surprising power of small language models](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) (2023-12-12)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/phi-2)

- SGLang
  - Year: 2023
  - Publication: [Efficiently Programming Large Language Models using SGLang](https://arxiv.org/abs/2312.07104) (2023-12-12)
  - Code: [GitHub](https://github.com/sgl-project/sglang/)

- LLM in a flash
  - Year: 2023
  - Publication: [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/abs/2312.11514) (2023-12-12)

- SwitchHead
  - Year: 2023
  - Publication: [SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention](https://arxiv.org/abs/2312.07987) (2023-12-13)
  - Code: [GitHub](https://github.com/robertcsordas/moe_attention)

- Dolphin 2.5
  - Year: 2023
  - Publication: [HuggingFace model card - Dolphin 2.5](https://huggingface.co/cognitivecomputations/dolphin-2.5-mixtral-8x7b) (2023-12-14)
  - Model weights: [HuggingFace models](https://huggingface.co/cognitivecomputations/dolphin-2.5-mixtral-8x7b)

- ZeroQuant
  - Year: 2023
  - Publication: [ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks](https://arxiv.org/abs/2312.08583) (2023-12-14)
  - Code: [GitHub](https://github.com/microsoft/DeepSpeed)

- PowerInfer
  - Year: 2023
  - Publication: [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](https://arxiv.org/abs/2312.12456) (2023-12-16)
  - Code: [GitHub](https://github.com/SJTU-IPADS/PowerInfer)
  - Model weights: [HuggingFace models](https://huggingface.co/PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF)

- Gemini
  - Year: 2023
  - Publication: [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805) (2023-12-19)

- WhiteRabbitNeo
  - Year: 2023
  - Publication: [HuggingFace model card - WhiteRabbitNeo](https://huggingface.co/WhiteRabbitNeo/WhiteRabbitNeo-33B-v1) (2023-12-27)
  - Model weights: [HuggingFace models](https://huggingface.co/whiterabbitneo/WhiteRabbitNeo-33B-v1)

- MosaicBERT
  - Year: 2023
  - Publication: [MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining](https://arxiv.org/abs/2312.17482) (2023-12-29)
  - Code: [GitHub](https://github.com/mosaicml/examples)
  - Model weights: [HuggingFace models](https://huggingface.co/mosaicml/mosaic-bert-base)

- LLM Maybe LongLM
  - Year: 2024
  - Publication: [LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning](https://arxiv.org/abs/2401.01325) (2024-01-02)

- SPIN
  - Year: 2024
  - Publication: [Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/abs/2401.01335) (2024-01-02)
  - Code: [GitHub](https://github.com/uclaml/SPIN)
  - Model weights: [HuggingFace models](https://huggingface.co/UCLA-AGI/zephyr-7b-sft-full-SPIN-iter0)

- TinyLlama
  - Year: 2023
  - Publication: [TinyLlama: An Open-Source Small Language Model](https://arxiv.org/abs/2401.02385) (2024-01-04)
  - Code: [GitHub](https://github.com/jzhang38/TinyLlama)
  - Model weights: [HuggingFace models](https://huggingface.co/PY007/TinyLlama-1.1B-step-50K-105b)

- LLaMA Pro
  - Year: 2024
  - Publication: [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/abs/2401.02415) (2024-01-04)
  - Code: [GitHub](https://github.com/TencentARC/LLaMA-Pro)
  - Model weights: [HuggingFace models](https://huggingface.co/TencentARC/LLaMA-Pro-8B)

- LLaMA Pro
  - Year: 2024
  - Publication: [LLaMA Pro: Progressive LLaMA with Block Expansion](https://arxiv.org/abs/2401.02415) (2024-01-04)
  - Code: [GitHub](https://github.com/TencentARC/LLaMA-Pro)
  - Model weights: [HuggingFace models](https://huggingface.co/TencentARC/LLaMA-Pro-8B)

- Deepseek LLM
  - Year: 2023
  - Publication: [DeepSeek LLM: Scaling Open-Source Language Models with Longtermism](https://arxiv.org/abs/2401.02954) (2024-01-05)
  - Code: [GitHub](https://github.com/deepseek-ai/deepseek-LLM)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-llm-7b-base)

- Infinite-LLM
  - Year: 2024
  - Publication: [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](https://arxiv.org/abs/2401.02669) (2024-01-05)

- Genstruct
  - Year: 2024
  - Publication: [HuggingFace model card - Genstruct](https://huggingface.co/NousResearch/Genstruct-7B) (2024-01-05)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Genstruct-7B)

- Activation Beacon
  - Year: 2024
  - Publication: [Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon](https://arxiv.org/abs/2401.03462) (2024-01-07)
  - Code: [GitHub](https://github.com/FlagOpen/FlagEmbedding)

- MoE-Mamba
  - Year: 2024
  - Publication: [MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://arxiv.org/abs/2401.04081) (2024-01-08)
  - Code: [GitHub](https://github.com/llm-random/llm-random)

- Mixtral of Experts
  - Year: 2024
  - Publication: [Mixtral of Experts](https://arxiv.org/abs/2401.04088) (2024-01-08)
  - Code: [GitHub](https://github.com/mistralai/mistral-src)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)

- TeleChat
  - Year: 2024
  - Publication: [TeleChat Technical Report](https://arxiv.org/abs/2401.03804) (2024-01-08)

- Lightning Attention-2
  - Year: 2024
  - Publication: [Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models](https://arxiv.org/abs/2401.04658) (2024-01-09)
  - Code: [GitHub](https://github.com/OpenNLPLab/lightning-attention)

- RoSA
  - Year: 2024
  - Publication: [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation](https://arxiv.org/abs/2401.04679) (2024-01-09)
  - Code: [GitHub](https://github.com/IST-DASLab/RoSA)

- MegaDolphin
  - Year: 2024
  - Publication: [HuggingFace model card - MegaDolphin](https://huggingface.co/cognitivecomputations/MegaDolphin-120b) (2024-01-10)
  - Model weights: [HuggingFace models](https://huggingface.co/cognitivecomputations/MegaDolphin-120b)

- DeepSeekMoE
  - Year: 2024
  - Publication: [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066) (2024-01-11)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-MoE)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-moe-16b-base)

- Patchscopes
  - Year: 2024
  - Publication: [Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models](https://arxiv.org/abs/2401.06102) (2024-01-11)

- DeepSeekMoE
  - Year: 2024
  - Publication: [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066) (2024-01-11)
  - Code: [GitHub](https://github.com/deepseek-ai/DeepSeek-MoE)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-moe-16b-base)

- Mixtral Nous-Hermes
  - Year: 2024
  - Publication: [Tweet: Introducing our new flagship LLM, Nous-Hermes 2 on Mixtral 8x7B.](https://twitter.com/NousResearch/status/1746988416779309143) (2024-01-15)
  - Model weights: [HuggingFace models](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO)

- Stable Code
  - Year: 2024
  - Publication: [Blog - Stable Code 3B: Coding on the Edge](https://stability.ai/news/stable-code-2024-llm-code-completion-release) (2024-01-16)
  - Model weights: [HuggingFace models](https://huggingface.co/stabilityai/stable-code-3b)

- AlphaCodium
  - Year: 2024
  - Publication: [Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering](https://arxiv.org/abs/2401.08500) (2024-01-16)
  - Code: [GitHub](https://github.com/Codium-ai/AlphaCodium)

- ReFT
  - Year: 2024
  - Publication: [ReFT: Reasoning with Reinforced Fine-Tuning](https://arxiv.org/abs/2401.08967) (2024-01-17)
  - Code: [GitHub](https://github.com/lqtrung1998/mwp_ReFT)
  - Model weights: [HuggingFace models](https://huggingface.co/lqtrung1998/Codellama-7b-hf-ReFT-GSM8k)

- Stable LM 2
  - Year: 2024
  - Publication: [Blog - Introducing Stable LM 2 1.6B](https://stability.ai/news/introducing-stable-lm-2) (2024-01-19)
  - Code: [GitHub](https://github.com/Stability-AI/StableLM)
  - Model weights: [Direct link](https://stability.ai/news/introducing-stable-lm-2)

- Imp
  - Year: 2024
  - Publication: [HuggingFace model card - Imp](https://huggingface.co/MILVLG/imp-v1-3b) (2024-01-24)
  - Code: [GitHub](https://github.com/MILVLG/imp)
  - Model weights: [HuggingFace models](https://huggingface.co/MILVLG/imp-v1-3b)

- Snorkel AI
  - Year: 2024
  - Publication: [Blog - New benchmark results demonstrate value of Snorkel AI approach to LLM alignment](https://snorkel.ai/new-benchmark-results-demonstrate-value-of-snorkel-ai-approach-to-llm-alignment/) (2024-01-24)
  - Model weights: [HuggingFace models](https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO)

- MaLA-500
  - Year: 2024
  - Publication: [MaLA-500: Massive Language Adaptation of Large Language Models](https://arxiv.org/abs/2401.13303) (2024-01-24)
  - Model weights: [HuggingFace models](https://huggingface.co/MaLA-LM/mala-500)

- MambaByte
  - Year: 2024
  - Publication: [MambaByte: Token-free Selective State Space Model](https://arxiv.org/abs/2401.13660) (2024-01-24)

- MM-LLMs
  - Year: 2024
  - Publication: [MM-LLMs: Recent Advances in MultiModal Large Language Models](https://arxiv.org/abs/2401.13601) (2024-01-24)

- SpacTor-T5
  - Year: 2024
  - Publication: [SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection](https://arxiv.org/abs/2401.13160) (2024-01-24)

- FP6-LLM
  - Year: 2024
  - Publication: [FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design](https://arxiv.org/abs/2401.14112) (2024-01-25)
  - Code: [GitHub](https://github.com/usyd-fsalab/fp6_llm)

- SliceGPT
  - Year: 2024
  - Publication: [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://arxiv.org/abs/2401.15024) (2024-01-26)
  - Code: [GitHub](https://github.com/microsoft/TransformerCompression)

- SliceGPT
  - Year: 2024
  - Publication: [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](https://arxiv.org/abs/2401.15024) (2024-01-26)
  - Code: [GitHub](https://github.com/microsoft/TransformerCompression)

- Eagle 7B
  - Year: 2024
  - Publication: [Blog - Eagle 7B: Soaring past Transformers with 1 Trillion Tokens Across 100+ Languages](https://blog.rwkv.com/p/eagle-7b-soaring-past-transformers) (2024-01-29)
  - Model weights: [HuggingFace models](https://huggingface.co/RWKV/v5-Eagle-7B)

- H2O-Danube
  - Year: 2024
  - Publication: [H2O-Danube-1.8B Technical Report](https://arxiv.org/abs/2401.16818) (2024-01-30)
  - Model weights: [HuggingFace models](https://huggingface.co/h2oai/h2o-danube-1.8b-base)

- KVQuant
  - Year: 2024
  - Publication: [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079) (2024-01-31)
  - Code: [GitHub](https://github.com/SqueezeAILab/KVQuant/)

- KVQuant
  - Year: 2024
  - Publication: [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](https://arxiv.org/abs/2401.18079) (2024-01-31)
  - Code: [GitHub](https://github.com/SqueezeAILab/KVQuant/)

- OLMo
  - Year: 2024
  - Publication: [OLMo: Accelerating the Science of Language Models](https://arxiv.org/abs/2402.00838) (2024-02-01)
  - Code: [GitHub](https://github.com/allenai/OLMo)
  - Model weights: [HuggingFace models](https://huggingface.co/allenai/OLMo-1B)

- BlackMamba
  - Year: 2024
  - Publication: [Blog - BlackMamba: Mixture of Experts for State-Space Models](https://static1.squarespace.com/static/658ded386c43c219ee47caba/t/65bd73200920d050ccbac40c/1706914594353/blackMamba.pdf) (2024-02-01)
  - Code: [GitHub](https://github.com/Zyphra/BlackMamba)
  - Model weights: [HuggingFace models](https://huggingface.co/Zyphra/BlackMamba-1.5B)

- Tiny Titans
  - Year: 2024
  - Publication: [Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?](https://arxiv.org/abs/2402.00841) (2024-02-01)

- Smaug
  - Year: 2024
  - Publication: [HuggingFace model card - Smaug](https://huggingface.co/abacusai/Smaug-72B-v0.1) (2024-02-02)
  - Model weights: [HuggingFace models](https://huggingface.co/abacusai/Smaug-72B-v0.1)

- Nomic Embed
  - Year: 2024
  - Publication: [Nomic Embed: Training a Reproducible Long Context Text Embedder](https://arxiv.org/abs/2402.01613) (2024-02-02)
  - Code: [GitHub](https://github.com/nomic-ai/contrastors)
  - Model weights: [HuggingFace models](https://huggingface.co/nomic-ai/nomic-embed-text-v1)

- Qwen 1.5
  - Year: 2024
  - Publication: [Blog - Introducing Qwen 1.5](https://qwenlm.github.io/blog/qwen1.5/) (2024-02-04)
  - Code: [GitHub](https://github.com/QwenLM/Qwen1.5)
  - Model weights: [HuggingFace models](https://huggingface.co/Qwen/Qwen1.5-0.5B)

- DeepSeekMath
  - Year: 2024
  - Publication: [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) (2024-02-05)
  - Code: [GitHub](https://github.com/deepseek-ai/deepseek-math)
  - Model weights: [HuggingFace models](https://huggingface.co/deepseek-ai/deepseek-math-7b-base)

- SQLCoder 2
  - Year: 2024
  - Publication: [HuggingFace model card - SQLCoder 2](https://huggingface.co/defog/sqlcoder-7b-2) (2024-02-05)
  - Code: [GitHub](https://github.com/defog-ai/sqlcoder)
  - Model weights: [HuggingFace models](https://huggingface.co/defog/sqlcoder-7b-2)

- RethinkTinyLM
  - Year: 2024
  - Publication: [Rethinking Optimization and Architecture for Tiny Language Models](https://arxiv.org/abs/2402.02791) (2024-02-05)
  - Code: [GitHub](https://github.com/YuchuanTian/RethinkTinyLM)

- Self-Discover
  - Year: 2024
  - Publication: [Self-Discover: Large Language Models Self-Compose Reasoning Structures](https://arxiv.org/abs/2402.03620) (2024-02-06)

- e5
  - Year: 2024
  - Publication: [Multilingual E5 Text Embeddings: A Technical Report](https://arxiv.org/abs/2402.05672) (2024-02-08)
  - Code: [GitHub](https://github.com/microsoft/unilm/tree/master/e5)
  - Model weights: [HuggingFace models](https://huggingface.co/intfloat/e5-small-v2)

- InternLM-Math
  - Year: 2024
  - Publication: [InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning](https://arxiv.org/abs/2402.06332) (2024-02-09)
  - Code: [GitHub](https://github.com/InternLM/InternLM-Math)
  - Model weights: [HuggingFace models](https://huggingface.co/internlm/internlm2-math-7b)

- DoRA
  - Year: 2024
  - Publication: [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) (2024-02-14)

- GritLM
  - Year: 2024
  - Publication: [Generative Representational Instruction Tuning](https://arxiv.org/abs/2402.09906) (2024-02-15)
  - Code: [GitHub](https://github.com/ContextualAI/gritlm)
  - Model weights: [HuggingFace models](https://huggingface.co/GritLM/GritLM-7B)

- LoRA+
  - Year: 2024
  - Publication: [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) (2024-02-19)
  - Code: [GitHub](https://github.com/nikhil-ghosh-berkeley/loraplus)

- LongRoPE
  - Year: 2024
  - Publication: [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://arxiv.org/abs/2402.13753) (2024-02-21)
  - Code: [GitHub](https://github.com/microsoft/LongRoPE)

- Gemma
  - Year: 2024
  - Publication: [Blog - Gemma: Introducing new state-of-the-art open models](https://blog.google/technology/developers/gemma-open-models/) (2024-02-21)
  - Model weights: [HuggingFace models](https://huggingface.co/google/gemma-7b)

- MobileLLM
  - Year: 2024
  - Publication: [MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases](https://arxiv.org/abs/2402.14905) (2024-02-22)

- MegaScale
  - Year: 2024
  - Publication: [MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs](https://arxiv.org/abs/2402.15627) (2024-02-23)

- Nemotron-4
  - Year: 2024
  - Publication: [Nemotron-4 15B Technical Report](https://arxiv.org/abs/2402.16819) (2024-02-26)

- Mistral Pro
  - Year: 2024
  - Publication: [HuggingFace model card - Mistral Pro](https://huggingface.co/TencentARC/Mistral_Pro_8B_v0.1) (2024-02-27)
  - Model weights: [HuggingFace models](https://huggingface.co/TencentARC/Mistral_Pro_8B_v0.1)

- BitNet
  - Year: 2024
  - Publication: [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) (2024-02-27)

- CLLMs (Consistency Large Language Models)
  - Year: 2024
  - Publication: [CLLMs: Consistency Large Language Models](https://arxiv.org/abs/2403.00835) (2024-02-28)
  - Code: [GitHub](https://github.com/hao-ai-lab/Consistency_LLM)
  - Model weights: [HuggingFace models](https://huggingface.co/cllm/consistency-llm-7b-sharegpt48k)

- Griffin
  - Year: 2024
  - Publication: [Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models](https://arxiv.org/abs/2402.19427) (2024-02-29)

- StarCoder-2
  - Year: 2024
  - Publication: [StarCoder 2 and The Stack v2: The Next Generation](https://arxiv.org/abs/2402.19173) (2024-02-29)
  - Model weights: [HuggingFace models](https://huggingface.co/bigcode/starcoder2-15b)

- Resonance RoPE
  - Year: 2024
  - Publication: [Resonance RoPE: Improving Context Length Generalization of Large Language Models](https://arxiv.org/abs/2403.00071) (2024-02-29)
  - Code: [GitHub](https://github.com/sheryc/resonance_rope)

- Claude 3
  - Year: 2024
  - Publication: [Blog - Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) (2024-03-04)

- Design2Code
  - Year: 2024
  - Publication: [Design2Code: How Far Are We From Automating Front-End Engineering?](https://arxiv.org/abs/2403.03163) (2024-03-05)
  - Code: [GitHub](https://github.com/NoviScl/Design2Code)

- SaulLM
  - Year: 2024
  - Publication: [SaulLM-7B: A pioneering Large Language Model for Law](https://arxiv.org/abs/2403.03883) (2024-03-06)
  - Model weights: [HuggingFace models](https://huggingface.co/Equall/Saul-Base)

- GaLore
  - Year: 2024
  - Publication: [GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507) (2024-03-06)

- Yi
  - Year: 2024
  - Publication: [Yi: Open Foundation Models by 01.AI](https://arxiv.org/abs/2403.04652) (2024-03-07)
  - Code: [GitHub](https://github.com/01-ai/Yi)
  - Model weights: [HuggingFace models](https://huggingface.co/01-ai/Yi-34B)

- inflection-2.5
  - Year: 2024
  - Publication: [Blog - Inflection-2.5: meet the world's best personal AI](https://inflection.ai/inflection-2-5) (2024-03-07)

- DeepSpeed-FP6
  - Year: 2024
  - Publication: [README - DeepSpeed-FP6: The Power of FP6-Centric Serving for Large Language Models](https://github.com/microsoft/DeepSpeed/blob/d86a68c3d407b4aebb2116cec6f351ea5e799c19/blogs/deepspeed-fp6/03-05-2024/README.md) (2024-03-08)
  - Code: [GitHub](https://github.com/microsoft/DeepSpeed)

- GEAR
  - Year: 2024
  - Publication: [GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM](https://arxiv.org/abs/2403.05527) (2024-03-08)
  - Code: [GitHub](https://github.com/opengear-project/GEAR)

- Command R
  - Year: 2024
  - Publication: [Blog - Introducing Command R Fine-Tuning: Industry-Leading Performance at a Fraction of the Cost](https://cohere.com/blog/commandr-fine-tuning) (2024-03-09)
  - Code: [GitHub](https://huggingface.co/CohereForAI/c4ai-command-r-v01)

- ORPO
  - Year: 2024
  - Publication: [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691) (2024-03-12)
  - Code: [GitHub](https://github.com/xfactlab/orpo)
  - Model weights: [HuggingFace models](https://huggingface.co/kaist-ai/mistral-orpo-alpha)

- BTX (Branch-Train-MiX)
  - Year: 2024
  - Publication: [Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM](https://arxiv.org/abs/2403.07816) (2024-03-12)

- ScatterMoE
  - Year: 2024
  - Publication: [Scattered Mixture-of-Experts Implementation](https://arxiv.org/abs/2403.08245) (2024-03-13)
  - Code: [GitHub](https://github.com/shawntan/scattermoe)

- Quiet-STaR
  - Year: 2024
  - Publication: [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) (2024-03-14)

- MM1
  - Year: 2024
  - Publication: [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) (2024-03-14)

- BurstAttention
  - Year: 2024
  - Publication: [BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences](https://arxiv.org/abs/2403.09347) (2024-03-14)

- DMC (Dynamic Memory Compression)
  - Year: 2024
  - Publication: [Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference](https://arxiv.org/abs/2403.09636) (2024-03-14)

- GemMoE
  - Year: 2024
  - Publication: [HuggingFace model card - GemMoE](https://huggingface.co/Crystalcareai/GemMoE-Base-Random) (2024-03-15)

- Agent-FLAN
  - Year: 2024
  - Publication: [Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models](https://arxiv.org/abs/2403.12881) (2024-03-19)
  - Code: [GitHub](https://github.com/InternLM/Agent-FLAN)
  - Model weights: [HuggingFace models](https://huggingface.co/internlm/Agent-FLAN-7b)

- LLM2LLM
  - Year: 2024
  - Publication: [LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement](https://arxiv.org/abs/2403.15042) (2024-03-22)
  - Code: [GitHub](https://github.com/SqueezeAILab/LLM2LLM)

- FollowIR
  - Year: 2024
  - Publication: [FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions](https://arxiv.org/abs/2403.15246) (2024-03-22)
  - Code: [GitHub](https://github.com/orionw/FollowIR)
  - Model weights: [HuggingFace models](https://huggingface.co/jhu-clsp/FollowIR-7B)

- InternLM2
  - Year: 2024
  - Publication: [InternLM2 Technical Report](https://arxiv.org/abs/2403.17297) (2024-03-26)

- Jamba
  - Year: 2024
  - Publication: [DiJiang: Efficient Large Language Models through Compact Kernelization](https://arxiv.org/abs/2403.19928) (2024-03-29)
  - Model weights: [HuggingFace models](https://huggingface.co/ai21labs/Jamba-v0.1)

- Transformer-Lite
  - Year: 2024
  - Publication: [Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs](https://arxiv.org/abs/2403.20041) (2024-03-29)

- Gecko
  - Year: 2024
  - Publication: [Gecko: Versatile Text Embeddings Distilled from Large Language Models](https://arxiv.org/abs/2403.20327) (2024-03-29)

- Aurora-M
  - Year: 2024
  - Publication: [Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order](https://arxiv.org/abs/2404.00399) (2024-03-30)
  - Model weights: [HuggingFace models](https://huggingface.co/aurora-m/aurora-m-base)

- Poro
  - Year: 2024
  - Publication: [Poro 34B and the Blessing of Multilinguality](https://arxiv.org/abs/2404.01856) (2024-04-02)
  - Model weights: [HuggingFace models](https://huggingface.co/LumiOpen/Poro-34B)

- Mixture-of-Depths
  - Year: 2024
  - Publication: [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258) (2024-04-02)

- LASP (Linear Attention Sequence Parallelism)
  - Year: 2024
  - Publication: [Linear Attention Sequence Parallelism](https://arxiv.org/abs/2404.02882) (2024-04-03)
  - Code: [GitHub](https://github.com/OpenNLPLab/LASP)

- ChatGLM-Math
  - Year: 2024
  - Publication: [ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline](https://arxiv.org/abs/2404.02893) (2024-04-03)
  - Code: [GitHub](https://github.com/THUDM/ChatGLM-Math)

- PiSSA
  - Year: 2024
  - Publication: [PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](https://arxiv.org/abs/2404.02948) (2024-04-03)
  - Code: [GitHub](https://github.com/GraphPKU/PiSSA)
  - Model weights: [HuggingFace models](https://huggingface.co/fxmeng/PiSSA-Llama-3-8B-Instruct-r16)

- Command R+
  - Year: 2024
  - Publication: [Blog - Introducing Command R+: A Scalable LLM Built for Business](https://cohere.com/blog/command-r-plus-microsoft-azure) (2024-04-04)
  - Model weights: [HuggingFace models](https://huggingface.co/CohereForAI/c4ai-command-r-plus)

- ReFT (Representation Finetuning)
  - Year: 2024
  - Publication: [ReFT: Representation Finetuning for Language Models](https://arxiv.org/abs/2404.03592) (2024-04-04)
  - Code: [GitHub](https://github.com/stanfordnlp/pyreft)

- Megalodon
  - Year: 2024
  - Publication: [Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length](https://arxiv.org/abs/2404.08801) (2024-04-12)
  - Code: [GitHub](https://github.com/XuezheMax/megalodon)

- TransformerFAM
  - Year: 2024
  - Publication: [TransformerFAM: Feedback attention is working memory](https://arxiv.org/abs/2404.09173) (2024-04-14)

- Prepacking
  - Year: 2024
  - Publication: [Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models](https://arxiv.org/abs/2404.09529) (2024-04-15)
  - Code: [GitHub](https://github.com/siyan-zhao/prepacking)

- decoupleQ
  - Year: 2024
  - Publication: [decoupleQ: Towards 2-bit Post-Training Uniform Quantization via decoupling Parameters into Integer and Floating Points](https://arxiv.org/abs/2404.12759) (2024-04-19)
  - Code: [GitHub](https://github.com/bytedance/decoupleQ)

- MoVA
  - Year: 2024
  - Publication: [MoVA: Adapting Mixture of Vision Experts to Multimodal Context](https://arxiv.org/abs/2404.13046) (2024-04-19)
  - Code: [GitHub](https://github.com/TempleX98/MoVA)

- phi-3
  - Year: 2024
  - Publication: [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219) (2024-04-22)
  - Model weights: [HuggingFace models](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)

- Arctic-Embed
  - Year: 2024
  - Publication: [Arctic-Embed: Scalable, Efficient, and Accurate Text Embedding Models](https://arxiv.org/abs/2405.05374) (2024-05-08)
  - Code: [GitHub](https://github.com/Snowflake-Labs/arctic-embed)
  - Model weights: [HuggingFace models](https://huggingface.co/Snowflake/snowflake-arctic-embed-xs)

- Codestral
  - Year: 2024
  - Publication: [Blog - Codestral: Hello, World!](https://mistral.ai/news/codestral) (2024-05-29)
  - Model weights: [HuggingFace models](https://huggingface.co/mistralai/Codestral-22B-v0.1)
